(torch-gpu) yuantang@Yuans-MBP code % python train_facegen_alt.py --seed 557 --dataset_root /Volumes/Yuan-T7/Datasets/face_gen --resume /Volumes/Yuan-T7/FAU_models/checkpoint_epoch_init.pth
PyTorch Version:  2.0.0
Torchvision Version:  0.15.1
Initializing Datasets and Dataloaders...
Train Sets: ['aw' 'e']
Test Sets: ['a' 'eb']
Total Number of Train Sets: 92
Total Number of Test Sets: 92

train_loss at epoch0: 0.03956284730330757
train_mses at epoch0: [4.14192254 1.14966851 1.1679435  1.14142848 1.17416413 0.98594665
 1.1486575  1.16242593 1.14653736 0.02560116]
train_maes at epoch0: [1.58717841 0.37929923 0.57006508 0.4076571  0.36040651 0.52405629
 0.36073308 0.38164837 0.38045128 0.07733254]
test_loss at epoch0: 0.0383926448614701
test_mses at epoch0: [3.6995286  1.10757697 1.12638898 1.08723833 1.16192449 0.95666231
 1.11980594 1.11899446 1.12348925 0.02486212]
test_maes at epoch0: [1.54291575 0.4204724  0.62979571 0.47292578 0.375353   0.55143316
 0.38659133 0.41307457 0.40607665 0.06626587]

train_loss at epoch1: 0.03807581507641336
train_mses at epoch1: [3.53008479 1.09853106 1.13562104 1.08342703 1.15196163 0.93438419
 1.11810407 1.10951754 1.10564175 0.02580985]
train_maes at epoch1: [1.55265867 0.45130858 0.6717802  0.52018066 0.38220359 0.56633238
 0.40012257 0.42852137 0.43086166 0.08007506]
test_loss at epoch1: 0.03683816479599994
test_mses at epoch1: [3.24601068 1.05156718 1.12212071 1.04811453 1.13825693 0.88735375
 1.07135077 1.06956371 1.07909787 0.02516845]
test_maes at epoch1: [1.5647753  0.49684735 0.71929157 0.59259526 0.41137962 0.58297007
 0.44438933 0.47976415 0.46939676 0.05984774]

train_loss at epoch2: 0.03624278566111689
train_mses at epoch2: [3.1508164  1.02260587 1.15484977 1.03193863 1.128032   0.81339732
 1.05584942 1.06899542 1.05591125 0.02695657]
train_maes at epoch2: [1.56611389 0.5157038  0.74274438 0.61439672 0.42358536 0.54954765
 0.45177394 0.48609689 0.49846306 0.08129767]
test_loss at epoch2: 0.03522404769192571
test_mses at epoch2: [3.08916013 0.98229038 1.09929041 1.01691312 1.10675347 0.78167368
 1.02422692 1.03330067 1.04576518 0.02618604]
test_maes at epoch2: [1.56980855 0.54431705 0.72760249 0.65336039 0.47170942 0.51216511
 0.50448578 0.54131454 0.5340148  0.05666275]

train_loss at epoch3: 0.0348369854947795
train_mses at epoch3: [3.15887209 0.96180002 1.13308222 1.05629917 1.11415519 0.74982924
 1.00572745 1.03517553 1.02525647 0.03126843]
train_maes at epoch3: [1.58475612 0.53284284 0.72293932 0.6777362  0.48593496 0.47746084
 0.49814619 0.53594121 0.5575002  0.07671833]
test_loss at epoch3: 0.03338656995607459
test_mses at epoch3: [2.95429402 0.89444    1.05740927 0.95061518 1.09016461 0.65850824
 0.98262726 1.00389896 1.02351036 0.02816942]
test_maes at epoch3: [1.5516396  0.53444629 0.68613642 0.61322851 0.51559528 0.42441154
 0.53837021 0.56822343 0.56704563 0.06890434]

train_loss at epoch4: 0.03313079346781191
train_mses at epoch4: [3.01872025 0.88743133 1.09952096 0.9356817  1.09338592 0.54907405
 0.99625791 1.02148872 1.02064055 0.03025059]
train_maes at epoch4: [1.55325168 0.49226164 0.6688907  0.58752009 0.5029523  0.38103871
 0.51668015 0.56029171 0.58674025 0.08151542]
test_loss at epoch4: 0.031721264123916626
test_mses at epoch4: [2.73098877 0.80949462 1.01473867 0.88403501 1.08153115 0.5664523
 0.94937319 0.9764893  1.00427387 0.02969778]
test_maes at epoch4: [1.47748552 0.47857409 0.62344146 0.51372583 0.53371011 0.3496909
 0.55573454 0.55637441 0.55979575 0.07658754]

train_loss at epoch5: 0.031311780862186264
train_mses at epoch5: [2.78414227 0.84952241 1.02929411 0.83147038 1.09845299 0.50657654
 0.92703353 0.98415702 0.97150155 0.03261171]
train_maes at epoch5: [1.4916512  0.47660504 0.63540293 0.50067864 0.52883461 0.35525835
 0.53822432 0.54420125 0.56457271 0.09050053]
test_loss at epoch5: 0.029441462910693626
test_mses at epoch5: [2.58000688 0.6889792  0.98227875 0.80430689 1.05759587 0.42763071
 0.89687105 0.92472141 0.96512295 0.03555275]
test_maes at epoch5: [1.44932699 0.45882992 0.6335796  0.45968164 0.5537856  0.32384709
 0.58187241 0.53993606 0.55029869 0.09407592]

train_loss at epoch6: 0.02912590425947438
train_mses at epoch6: [2.81092394 0.67749148 1.04060082 0.72810709 1.05112679 0.43923995
 0.89085386 0.91780378 0.9431102  0.04157996]
train_maes at epoch6: [1.52733179 0.43990391 0.67675208 0.45748209 0.53429107 0.37815423
 0.56796402 0.5237737  0.5682347  0.12266947]
test_loss at epoch6: 0.026545639271321503
test_mses at epoch6: [2.59586911 0.53981779 0.94430295 0.67892021 1.01592447 0.2902496
 0.82766654 0.84251276 0.89790909 0.04670545]
test_maes at epoch6: [1.45890825 0.42649034 0.62808919 0.45763957 0.56022435 0.30395425
 0.58563334 0.51773766 0.53939712 0.12334311]

train_loss at epoch7: 0.02631134896174721
train_mses at epoch7: [2.49655865 0.57289622 1.00311092 0.61821259 1.00186525 0.31182576
 0.79856696 0.83514107 0.81560322 0.05553577]
train_maes at epoch7: [1.3873276  0.41600991 0.64893688 0.49896494 0.5347198  0.358924
 0.5593827  0.49865123 0.54292683 0.15907524]
test_loss at epoch7: 0.02378669575504635
test_mses at epoch7: [2.2015341  0.39624015 0.86804033 0.5582253  0.97439813 0.2520135
 0.74553661 0.77680436 0.83975852 0.04649104]
test_maes at epoch7: [1.3067876  0.32773623 0.47039576 0.46824861 0.54142488 0.35511987
 0.55305378 0.47604789 0.50755744 0.13436041]

train_loss at epoch8: 0.023282314124314682
train_mses at epoch8: [2.2148098  0.48106115 0.93855263 0.45236646 0.96612431 0.23973973
 0.72917786 0.73283779 0.80373469 0.04322069]
train_maes at epoch8: [1.28999512 0.34917626 0.50598454 0.45821739 0.52078016 0.37441237
 0.50720799 0.45483549 0.54552101 0.13665707]
test_loss at epoch8: 0.020898590269296066
test_mses at epoch8: [2.02785525 0.28414816 0.78659733 0.4339017  0.92016343 0.24765065
 0.61746806 0.68012356 0.75970073 0.05357337]
test_maes at epoch8: [1.17223278 0.31988876 0.45676444 0.34149888 0.49417785 0.36916555
 0.42722574 0.44902936 0.43891479 0.14054439]

train_loss at epoch9: 0.02036082679810731
train_mses at epoch9: [2.03890464 0.28999042 0.78606411 0.36844697 0.91763299 0.25638951
 0.6070098  0.67802103 0.72928746 0.06240708]
train_maes at epoch9: [1.11468225 0.32461185 0.48333937 0.35008088 0.48321999 0.38453212
 0.40922595 0.45795067 0.47080958 0.17283006]
test_loss at epoch9: 0.01841926185981087
test_mses at epoch9: [1.98982956 0.21667389 0.70009182 0.33790528 0.84786227 0.31055501
 0.48720427 0.58693657 0.649245   0.05828511]
test_maes at epoch9: [1.07764005 0.33827396 0.5175543  0.31255924 0.42689104 0.37824056
 0.37003076 0.47621695 0.41624884 0.15020916]

train_loss at epoch10: 0.018311687137769615
train_mses at epoch10: [2.07178908 0.25905473 0.71131548 0.32930957 0.84057206 0.30295165
 0.48333078 0.61551839 0.58493047 0.07275836]
train_maes at epoch10: [1.05384104 0.3584024  0.49940445 0.34310156 0.43653797 0.4003231
 0.38569009 0.48120764 0.45546141 0.18275085]
test_loss at epoch10: 0.015632797194563824
test_mses at epoch10: [1.95644036 0.23597433 0.564624   0.25004339 0.7692116  0.2503269
 0.36733769 0.5080106  0.53586429 0.05221121]
test_maes at epoch10: [0.98719344 0.37619924 0.43431418 0.31821605 0.3979695  0.42328895
 0.35436273 0.457883   0.42518583 0.14833969]

train_loss at epoch11: 0.015460478870765022
train_mses at epoch11: [1.94940459 0.24327636 0.52112766 0.27761902 0.75283167 0.21835994
 0.41040081 0.42404993 0.59992268 0.07447576]
train_maes at epoch11: [0.95964611 0.36367596 0.41325551 0.35147482 0.42163355 0.38289322
 0.37997013 0.44484413 0.44828508 0.19295057]
test_loss at epoch11: 0.013189875885196354
test_mses at epoch11: [1.84869568 0.15713164 0.46078694 0.22871057 0.71678753 0.17978132
 0.30093517 0.42282718 0.45218084 0.04067918]
test_maes at epoch11: [0.93218052 0.30694101 0.36716705 0.30310608 0.39187425 0.32368917
 0.31665619 0.42098105 0.40750866 0.11970418]

train_loss at epoch12: 0.013287796598413715
train_mses at epoch12: [1.82669565 0.20941472 0.41670797 0.29621602 0.7144925  0.24062316
 0.31736788 0.37183992 0.39278835 0.05074498]
train_maes at epoch12: [0.94795709 0.32472091 0.39040355 0.37121853 0.4339145  0.36699521
 0.36027981 0.38896888 0.38268829 0.15371653]
test_loss at epoch12: 0.011091995174470156
test_mses at epoch12: [1.53660561 0.13393352 0.36114655 0.1892415  0.62577851 0.15065278
 0.24047406 0.37731743 0.37779161 0.03416908]
test_maes at epoch12: [0.85032797 0.28559434 0.33276233 0.25646974 0.39162563 0.29558132
 0.30023979 0.42026643 0.40728553 0.11037611]

train_loss at epoch13: 0.012127652440382086
train_mses at epoch13: [1.58789951 0.16538409 0.34688287 0.30380716 0.64925451 0.22902799
 0.32980014 0.35986356 0.32960902 0.05620279]
train_maes at epoch13: [0.95598399 0.30679128 0.35558415 0.34855787 0.46449821 0.35689881
 0.36090054 0.39758951 0.415488   0.16990181]
test_loss at epoch13: 0.009667405939620474
test_mses at epoch13: [1.25378261 0.10104068 0.31240983 0.18221266 0.56586482 0.13390771
 0.2119885  0.32110743 0.31896254 0.03374036]
test_maes at epoch13: [0.77661029 0.23096601 0.29662846 0.2609058  0.4315119  0.2520745
 0.27126975 0.37876396 0.37971936 0.12533733]

train_loss at epoch14: 0.011174622113290041
train_mses at epoch14: [1.3590299  0.12765376 0.32100746 0.25148483 0.61018808 0.16627678
 0.30538256 0.37647915 0.32736417 0.05767861]
train_maes at epoch14: [0.91037311 0.25575573 0.33245023 0.31346012 0.4697089  0.27600104
 0.34718436 0.40374621 0.3636498  0.17220035]
test_loss at epoch14: 0.00826163719529691
test_mses at epoch14: [0.97135504 0.08129875 0.26129077 0.1638587  0.48991807 0.10008587
 0.20209078 0.28831494 0.26207718 0.02398538]
test_maes at epoch14: [0.69533694 0.18961247 0.27238706 0.21534016 0.34650994 0.23399538
 0.25660418 0.3411521  0.33767071 0.0949574 ]

train_loss at epoch15: 0.009988991138727768
train_mses at epoch15: [1.21664087 0.13067793 0.2815216  0.27932419 0.55051814 0.129169
 0.23758891 0.29491953 0.31143465 0.0447936 ]
train_maes at epoch15: [0.81364102 0.26652436 0.30721831 0.31203768 0.40977488 0.27244396
 0.30554756 0.33962487 0.36806643 0.14884135]
test_loss at epoch15: 0.007574529427549113
test_mses at epoch15: [0.96882592 0.07554502 0.2367416  0.17243768 0.46004001 0.08366707
 0.18723097 0.23828107 0.22985719 0.02731095]
test_maes at epoch15: [0.64428743 0.16417081 0.24879148 0.21537666 0.32491895 0.19669364
 0.24406352 0.27740212 0.29943144 0.09701545]

train_loss at epoch16: 0.00895455657787945
train_mses at epoch16: [0.84327259 0.08519046 0.28155498 0.20127987 0.45619475 0.15133522
 0.2666203  0.34182558 0.22267279 0.0448734 ]
train_maes at epoch16: [0.67184708 0.19516234 0.32130841 0.26875011 0.35184303 0.26874077
 0.29148801 0.33693031 0.31106033 0.1433257 ]
test_loss at epoch16: 0.006456109816613404
test_mses at epoch16: [0.72911151 0.05702734 0.2160211  0.14737274 0.37908811 0.06280987
 0.17271051 0.21733934 0.19064561 0.02639488]
test_maes at epoch16: [0.61959512 0.14602076 0.27070823 0.21849649 0.31866463 0.17658087
 0.2248244  0.25271823 0.25532088 0.09403431]

train_loss at epoch17: 0.008079420775175095
train_mses at epoch17: [1.06945897 0.11762414 0.26235333 0.17721624 0.38384495 0.100797
 0.23029008 0.26108375 0.25819273 0.041112  ]
train_maes at epoch17: [0.75058144 0.21639309 0.30273444 0.27746902 0.35780924 0.22278195
 0.27982419 0.314134   0.30527151 0.15175037]
test_loss at epoch17: 0.005849474312170692
test_mses at epoch17: [0.66205738 0.05527624 0.1903726  0.13325559 0.3417864  0.05247348
 0.16037191 0.20350608 0.17323532 0.02392724]
test_maes at epoch17: [0.57266298 0.13089765 0.22545818 0.16084549 0.36456341 0.16633978
 0.21825273 0.23638826 0.23094886 0.09120118]

train_loss at epoch18: 0.006962098667155142
train_mses at epoch18: [0.76956685 0.05191044 0.22480857 0.17512878 0.38387895 0.07482459
 0.17977401 0.21551228 0.24144036 0.03622137]
train_maes at epoch18: [0.60718281 0.17338612 0.29780406 0.23327712 0.38021507 0.19771429
 0.24193792 0.28676621 0.27779316 0.13969092]
test_loss at epoch18: 0.005415024964705757
test_mses at epoch18: [0.65491151 0.04790718 0.18623    0.12586433 0.27881914 0.05103821
 0.1602815  0.19390904 0.16637628 0.02153372]
test_maes at epoch18: [0.55391046 0.16052522 0.22187079 0.16502725 0.31708271 0.146965
 0.22893458 0.2314028  0.24301955 0.08970665]

train_loss at epoch19: 0.00680407104284867
train_mses at epoch19: [0.8623778  0.10687002 0.24546384 0.13101929 0.29045153 0.1150979
 0.21227203 0.2403116  0.16970051 0.04388247]
train_maes at epoch19: [0.67044631 0.22789181 0.28884657 0.2194623  0.3445491  0.2333329
 0.29253144 0.29120885 0.25957049 0.15473987]
test_loss at epoch19: 0.004836484789848328
test_mses at epoch19: [0.61883227 0.02905748 0.17938236 0.13045069 0.23599133 0.03908119
 0.13667847 0.17777429 0.14802447 0.01930051]
test_maes at epoch19: [0.53200329 0.11529054 0.22511372 0.21085132 0.28745027 0.14280309
 0.2011652  0.20166762 0.20248715 0.08364085]

train_loss at epoch20: 0.006215560047522835
train_mses at epoch20: [0.68050205 0.06841052 0.18636417 0.13763821 0.25512815 0.07885072
 0.21997576 0.23322418 0.20627241 0.03356471]
train_maes at epoch20: [0.61512552 0.19013644 0.285003   0.25757451 0.31010497 0.20642453
 0.2828419  0.26432464 0.26898038 0.12983576]
test_loss at epoch20: 0.004749866850350214
test_mses at epoch20: [0.5986431  0.03489111 0.17374499 0.10535252 0.22888805 0.04316269
 0.15366735 0.17431166 0.14302082 0.01776536]
test_maes at epoch20: [0.5171802  0.1188945  0.20392313 0.16081155 0.2776382  0.13398523
 0.19848222 0.20246161 0.19471388 0.07363874]

train_loss at epoch21: 0.006149698534737463
train_mses at epoch21: [0.67851624 0.04996423 0.22768857 0.1149058  0.2660455  0.08524928
 0.1895691  0.25656247 0.18156807 0.03142103]
train_maes at epoch21: [0.61442659 0.1632176  0.27459343 0.24137528 0.34448302 0.20598437
 0.2442121  0.27710697 0.25550901 0.12637165]
test_loss at epoch21: 0.004293704405426979
test_mses at epoch21: [0.57657531 0.0184221  0.16103947 0.11707071 0.19658119 0.03360187
 0.12034226 0.17037783 0.13274673 0.0186764 ]
test_maes at epoch21: [0.51568183 0.09682467 0.20260101 0.17728407 0.2647991  0.13047205
 0.16104845 0.20379967 0.19207465 0.07730725]

train_loss at epoch22: 0.005479326390701792
train_mses at epoch22: [0.70586902 0.04019073 0.16688459 0.11345472 0.23466311 0.07755462
 0.15034936 0.29715213 0.13709349 0.03813491]
train_maes at epoch22: [0.62109203 0.14540966 0.26276612 0.20947094 0.30209137 0.19679806
 0.22932145 0.30588001 0.2464873  0.1364648 ]
test_loss at epoch22: 0.0041866778679516
test_mses at epoch22: [0.55801862 0.01902277 0.1572315  0.10278184 0.17034099 0.06377482
 0.12213754 0.16506096 0.12604528 0.01825739]
test_maes at epoch22: [0.50852194 0.0981503  0.21641117 0.15152982 0.24480896 0.13635042
 0.19555352 0.20154916 0.18534354 0.07483187]

train_loss at epoch23: 0.005150013481793197
train_mses at epoch23: [0.58774748 0.06282826 0.17396322 0.07810207 0.15256448 0.08332254
 0.17575797 0.23894775 0.17407647 0.02901446]
train_maes at epoch23: [0.57128139 0.17056983 0.23958027 0.18248069 0.25891127 0.19156518
 0.24122063 0.25112364 0.26288031 0.12401468]
test_loss at epoch23: 0.003871047302432682
test_mses at epoch23: [0.55884313 0.02693414 0.14481882 0.08633298 0.16267976 0.04168141
 0.11251546 0.15062547 0.12715503 0.01671201]
test_maes at epoch23: [0.49808798 0.09446474 0.18765933 0.14460145 0.23721787 0.1251317
 0.14906398 0.18644604 0.18002258 0.06365811]

train_loss at epoch24: 0.0044561188829981765
train_mses at epoch24: [0.56769089 0.04669097 0.14441124 0.08603432 0.12607698 0.05539522
 0.1636763  0.22817949 0.1383887  0.02922728]
train_maes at epoch24: [0.57327892 0.15084044 0.22695064 0.18857533 0.25454055 0.1817277
 0.23354652 0.26902322 0.23969663 0.12846367]
test_loss at epoch24: 0.003704317885896434
test_mses at epoch24: [0.53915062 0.0193394  0.19757961 0.10931095 0.12912907 0.01429132
 0.09924411 0.14950198 0.10199368 0.01725483]
test_maes at epoch24: [0.48579657 0.09030291 0.27486812 0.15118739 0.2100833  0.09315003
 0.16229563 0.20750712 0.17306899 0.06661487]

train_loss at epoch25: 0.004788169070430424
train_mses at epoch25: [0.78284031 0.04539952 0.17483218 0.15899138 0.17719481 0.05084026
 0.11213458 0.17215587 0.12917579 0.04162868]
train_maes at epoch25: [0.64849953 0.1563433  0.26127158 0.21967647 0.26228239 0.17214231
 0.19338619 0.2367203  0.23875973 0.14846485]
test_loss at epoch25: 0.0033746756937192836
test_mses at epoch25: [0.63525091 0.06357168 0.1320188  0.07343715 0.12426518 0.02336098
 0.09535413 0.12707553 0.09199686 0.01702776]
test_maes at epoch25: [0.54831124 0.12664449 0.21366737 0.14460353 0.19521287 0.08843752
 0.12590839 0.16284472 0.15992448 0.06620863]

train_loss at epoch26: 0.0045469144926123
train_mses at epoch26: [0.8400317  0.10579043 0.17105179 0.13351681 0.14116396 0.04272536
 0.12036105 0.14367031 0.11143519 0.02931186]
train_maes at epoch26: [0.65296085 0.17973807 0.27377573 0.23351476 0.22569182 0.15401202
 0.21283181 0.21625052 0.22997412 0.11643758]
test_loss at epoch26: 0.002747723994695622
test_mses at epoch26: [0.46167672 0.02048196 0.12718604 0.07616423 0.09647578 0.01166388
 0.07259999 0.1187071  0.07413954 0.01638102]
test_maes at epoch26: [0.45101965 0.09445163 0.1941732  0.14219345 0.16902372 0.07695534
 0.12751257 0.16168099 0.14199944 0.06589164]

train_loss at epoch27: 0.003798023028218228
train_mses at epoch27: [0.69836793 0.05720587 0.15163383 0.08980387 0.15995747 0.05488493
 0.07721627 0.12914048 0.09343613 0.02792885]
train_maes at epoch27: [0.6412027  0.16718371 0.26295797 0.20504596 0.25371271 0.18083627
 0.17722274 0.22382857 0.21347992 0.10977994]
test_loss at epoch27: 0.0026303405023139458
test_mses at epoch27: [0.41183416 0.02354262 0.11341516 0.06710101 0.10378875 0.00796727
 0.08190798 0.11242042 0.06335088 0.01499802]
test_maes at epoch27: [0.44757592 0.11254595 0.17869782 0.18539925 0.22610073 0.07134869
 0.15888257 0.15968873 0.1401446  0.06094676]

train_loss at epoch28: 0.0036279087805229683
train_mses at epoch28: [0.5605914  0.05524274 0.12664141 0.10324306 0.11344508 0.05002577
 0.10951841 0.13991718 0.08572946 0.03632197]
train_maes at epoch28: [0.55472503 0.15739801 0.23418005 0.22235108 0.22537966 0.14563248
 0.20708786 0.2329226  0.17935934 0.1349755 ]
test_loss at epoch28: 0.0029252710873666015
test_mses at epoch28: [0.53817641 0.01304256 0.17437288 0.10741965 0.07827922 0.01860334
 0.06577782 0.12280495 0.05707143 0.01522222]
test_maes at epoch28: [0.4599897  0.07814894 0.24064229 0.2032123  0.17125584 0.08420172
 0.11686755 0.158486   0.13285537 0.05598365]

train_loss at epoch29: 0.0041758157312870026
train_mses at epoch29: [0.63534755 0.06671164 0.18929584 0.12535385 0.0834469  0.06296928
 0.12048412 0.13475676 0.12177004 0.03374293]
train_maes at epoch29: [0.53741928 0.18467095 0.25920099 0.21958778 0.19504747 0.15149446
 0.20225278 0.21801179 0.23768217 0.12393993]
test_loss at epoch29: 0.00226272322723399
test_mses at epoch29: [0.37729803 0.01943171 0.12948568 0.08971141 0.04997129 0.00569316
 0.04502894 0.09319785 0.06252947 0.0135377 ]
test_maes at epoch29: [0.42590738 0.0909708  0.19824096 0.17318554 0.14859773 0.06090863
 0.11257568 0.15389044 0.15506824 0.05396679]

train_loss at epoch30: 0.0034004716283601265
train_mses at epoch30: [0.4364372  0.03578879 0.13456411 0.11060904 0.0729278  0.04710775
 0.11489913 0.14670027 0.06917282 0.030338  ]
train_maes at epoch30: [0.49261665 0.13411751 0.21801894 0.21565763 0.19829963 0.16563796
 0.21565083 0.2278021  0.18409992 0.11113864]
test_loss at epoch30: 0.00272074066426443
test_mses at epoch30: [0.49244473 0.02443804 0.16771155 0.07084324 0.05141143 0.05837433
 0.06789879 0.10612202 0.04662247 0.01503268]
test_maes at epoch30: [0.43519041 0.10476981 0.18405185 0.12686641 0.12150367 0.12765728
 0.11943053 0.14621187 0.12877952 0.05613352]

train_loss at epoch31: 0.0032932646572589874
train_mses at epoch31: [0.57157736 0.03463673 0.12905985 0.09077926 0.082026   0.07250188
 0.07827086 0.16555438 0.05539742 0.02795362]
train_maes at epoch31: [0.53405968 0.13553028 0.19610366 0.18610331 0.16907434 0.16345664
 0.17626278 0.22077232 0.17026194 0.12020883]
test_loss at epoch31: 0.0018867861236567082
test_mses at epoch31: [0.32639439 0.01691946 0.1351455  0.04017016 0.04638421 0.00647858
 0.04956695 0.07807947 0.03394051 0.01372929]
test_maes at epoch31: [0.40445961 0.09135182 0.24587333 0.11231767 0.1277968  0.06542138
 0.12598316 0.14489902 0.12239027 0.05640859]

train_loss at epoch32: 0.003128039043234742
train_mses at epoch32: [0.39304543 0.04978568 0.18717667 0.0572002  0.08046077 0.06151986
 0.07158319 0.11572815 0.06046479 0.02472714]
train_maes at epoch32: [0.4837558  0.15957417 0.27620525 0.17354714 0.19157013 0.18361584
 0.17776872 0.21172536 0.17828963 0.10761899]
test_loss at epoch32: 0.0016370808462733808
test_mses at epoch32: [0.29142969 0.01266527 0.07491576 0.04584596 0.03080815 0.01517481
 0.04117053 0.08491037 0.04553834 0.0145257 ]
test_maes at epoch32: [0.35531507 0.07851641 0.14626214 0.11347313 0.10915494 0.10499026
 0.11851384 0.16426952 0.14385797 0.05885958]

train_loss at epoch33: 0.0025749485939741135
train_mses at epoch33: [0.33993062 0.04029554 0.1204593  0.08185017 0.03752825 0.04884222
 0.07654651 0.10552255 0.046959   0.0263998 ]
train_maes at epoch33: [0.44407347 0.14131415 0.2103718  0.19112095 0.15231468 0.17324748
 0.1885676  0.20740198 0.15650536 0.1011445 ]
test_loss at epoch33: 0.0015358314242051995
test_mses at epoch33: [0.33978784 0.01719645 0.08401648 0.04062967 0.02993538 0.01063268
 0.0276397  0.07795739 0.03389259 0.01408235]
test_maes at epoch33: [0.39855617 0.08188956 0.17289066 0.10520567 0.10826561 0.07813468
 0.09121595 0.13736718 0.1113095  0.05795072]

train_loss at epoch34: 0.0029387237585109215
train_mses at epoch34: [0.47103466 0.04818253 0.12447906 0.11545846 0.05137645 0.04826575
 0.04858171 0.09608843 0.10317901 0.02312492]
train_maes at epoch34: [0.49670608 0.14788312 0.21090806 0.22364761 0.15945475 0.16893828
 0.16314187 0.20017435 0.21504955 0.10616237]
test_loss at epoch34: 0.0015059796202441921
test_mses at epoch34: [0.29513111 0.0412968  0.06269216 0.03838834 0.02691434 0.01653934
 0.03152821 0.0686955  0.03255912 0.01487853]
test_maes at epoch34: [0.37739543 0.11097918 0.12497253 0.09998755 0.09780794 0.09235753
 0.10472688 0.12828181 0.11292675 0.06633207]

train_loss at epoch35: 0.002798823234827622
train_mses at epoch35: [0.40818483 0.06666964 0.09986984 0.062677   0.05477724 0.06633725
 0.0646163  0.11593791 0.06127615 0.03238277]
train_maes at epoch35: [0.47818106 0.16849659 0.18487963 0.18060253 0.16629242 0.15462323
 0.16071311 0.2102988  0.18357401 0.12272837]
test_loss at epoch35: 0.0013138644154305043
test_mses at epoch35: [0.26667153 0.02839994 0.05583999 0.05700326 0.02210057 0.01111438
 0.02015652 0.05788935 0.02379149 0.01475064]
test_maes at epoch35: [0.35197701 0.08796927 0.1144872  0.1426828  0.08468929 0.07868272
 0.07355289 0.12193509 0.09965711 0.06374238]

train_loss at epoch36: 0.0018220558033689208
train_mses at epoch36: [0.23760697 0.03745821 0.04194045 0.06003664 0.0338835  0.05018321
 0.04281241 0.06705471 0.04834926 0.03252363]
train_maes at epoch36: [0.38773258 0.13310868 0.15289071 0.17335103 0.13835214 0.14700875
 0.13867754 0.17441087 0.16279963 0.12245589]
test_loss at epoch36: 0.0011962011213535848
test_mses at epoch36: [0.20877548 0.01194532 0.05905959 0.03473586 0.01431376 0.00884517
 0.02591835 0.07889913 0.02209723 0.01365049]
test_maes at epoch36: [0.33835748 0.07887614 0.14786033 0.1042892  0.0817236  0.07303459
 0.08250157 0.15740688 0.09368138 0.06137812]

train_loss at epoch37: 0.002287119181583757
train_mses at epoch37: [0.37027812 0.06059475 0.09312349 0.05276741 0.03353739 0.03836933
 0.04978546 0.11326193 0.04557245 0.03304668]
train_maes at epoch37: [0.45171032 0.15734145 0.19196972 0.16446079 0.1422807  0.1416645
 0.1576525  0.20703964 0.15359665 0.12616932]
test_loss at epoch37: 0.0013635428217442138
test_mses at epoch37: [0.22203559 0.01564949 0.08740557 0.02808452 0.02242225 0.05006052
 0.02227979 0.04694973 0.01700358 0.01510052]
test_maes at epoch37: [0.33807651 0.07904556 0.13174093 0.09631011 0.08305317 0.12012544
 0.07799922 0.10869191 0.08811325 0.06205491]

train_loss at epoch38: 0.0030238509340130763
train_mses at epoch38: [0.42748445 0.0415141  0.19498709 0.06155752 0.06713276 0.06617699
 0.04852765 0.11088273 0.04324464 0.03111285]
train_maes at epoch38: [0.45006287 0.13305759 0.21954255 0.15395773 0.15002074 0.16862879
 0.13416661 0.18477613 0.14634065 0.1166022 ]
test_loss at epoch38: 0.001386035841120326
test_mses at epoch38: [0.25185185 0.03083066 0.10299868 0.02650405 0.02056922 0.03724704
 0.02300351 0.04254093 0.01011892 0.01505237]
test_maes at epoch38: [0.35225915 0.09713984 0.14907837 0.09226533 0.08618959 0.11095313
 0.07613353 0.10834799 0.06816559 0.06211312]

train_loss at epoch39: 0.002433314271595167
train_mses at epoch39: [0.44078267 0.03348014 0.10879576 0.06795649 0.05400184 0.04471082
 0.06209269 0.0871284  0.05405513 0.02936785]
train_maes at epoch39: [0.52302276 0.13208593 0.19966758 0.1879853  0.15993348 0.16596815
 0.16525973 0.19506762 0.16723139 0.11953151]
test_loss at epoch39: 0.0012642803561428318
test_mses at epoch39: [0.24506282 0.00828047 0.07342895 0.06576547 0.01208757 0.01347634
 0.02203595 0.05092987 0.0225162  0.0142113 ]
test_maes at epoch39: [0.37308103 0.06579732 0.17485692 0.14534723 0.0738168  0.08845709
 0.09561444 0.13591498 0.0968896  0.06973615]

train_loss at epoch40: 0.0023506626729731975
train_mses at epoch40: [0.32540138 0.05155326 0.11428601 0.07303208 0.03563397 0.05071443
 0.08019109 0.06476541 0.03597814 0.0268728 ]
train_maes at epoch40: [0.4361793  0.14451423 0.20918822 0.1752993  0.12834402 0.159589
 0.17473884 0.17108428 0.13602283 0.11419333]
test_loss at epoch40: 0.0015712242330545964
test_mses at epoch40: [0.2836782  0.03079194 0.05334662 0.06687061 0.03011218 0.02839635
 0.03821086 0.06519574 0.0205777  0.0157768 ]
test_maes at epoch40: [0.3753522  0.08948585 0.09321247 0.12130302 0.08542939 0.10783537
 0.08911393 0.12554865 0.08269096 0.06715484]

train_loss at epoch41: 0.0022083554497879486
train_mses at epoch41: [0.35136248 0.05795667 0.08058738 0.08851791 0.04457139 0.04669715
 0.0408225  0.05888514 0.05003772 0.02791733]
train_maes at epoch41: [0.47487183 0.1586048  0.15603424 0.17328438 0.13298504 0.14890398
 0.13524816 0.14643791 0.14544248 0.11479066]
test_loss at epoch41: 0.0007964538855721121
test_mses at epoch41: [0.15370888 0.01205339 0.03790822 0.02844224 0.00881876 0.01386489
 0.01339841 0.03937227 0.00932909 0.01341271]
test_maes at epoch41: [0.28378438 0.08370964 0.12163894 0.08520774 0.0691967  0.0756961
 0.0753534  0.11561956 0.063758   0.05878663]

train_loss at epoch42: 0.001817131617470928
train_mses at epoch42: [0.24173566 0.03866628 0.05689227 0.05303893 0.03270442 0.03533495
 0.05515362 0.07710764 0.04054028 0.02797089]
train_maes at epoch42: [0.36583791 0.14921869 0.16680973 0.15470173 0.13359034 0.13274945
 0.15206734 0.18731847 0.14705154 0.12091993]
test_loss at epoch42: 0.0007544630612044231
test_mses at epoch42: [0.16460867 0.00819366 0.05535185 0.0183789  0.00604634 0.012596
 0.00962906 0.02626418 0.01535771 0.01289264]
test_maes at epoch42: [0.29874871 0.06109778 0.18185236 0.07825607 0.05735119 0.07406897
 0.06713407 0.10413044 0.08168135 0.05502462]

train_loss at epoch43: 0.001919912660251493
train_mses at epoch43: [0.30693916 0.02520958 0.10051337 0.04804156 0.04238444 0.03365847
 0.03699543 0.06685914 0.05652128 0.02237746]
train_maes at epoch43: [0.38013519 0.10585024 0.21902746 0.14342009 0.13934792 0.12940483
 0.13108411 0.16963412 0.15327871 0.09328951]
test_loss at epoch43: 0.000903509516754876
test_mses at epoch43: [0.20573883 0.00976284 0.04898168 0.02053492 0.01349093 0.01193687
 0.01995528 0.04052992 0.01936419 0.01289433]
test_maes at epoch43: [0.32777546 0.06283344 0.13061711 0.08035098 0.07347159 0.08012074
 0.07436132 0.11330896 0.08737351 0.04993754]

train_loss at epoch44: 0.0016283585244546766
train_mses at epoch44: [0.24190341 0.02939775 0.07879349 0.04903971 0.04479753 0.03397999
 0.04614121 0.02775505 0.04134954 0.01952267]
train_maes at epoch44: [0.39949954 0.13553237 0.17231737 0.14027572 0.15393922 0.1297944
 0.14234845 0.12355951 0.1531762  0.09416119]
test_loss at epoch44: 0.0004859946000025324
test_mses at epoch44: [0.11950095 0.00809042 0.0182152  0.01980943 0.00543611 0.00530323
 0.01070899 0.01721741 0.00929224 0.01184899]
test_maes at epoch44: [0.25854132 0.06233403 0.07723705 0.0795999  0.05469252 0.05751629
 0.07492496 0.0938407  0.07083323 0.04800575]

train_loss at epoch45: 0.0017819657678837361
train_mses at epoch45: [0.32521419 0.04774741 0.04723411 0.04022464 0.03865911 0.05790573
 0.04449838 0.05698577 0.0338898  0.02331158]
train_maes at epoch45: [0.44653435 0.14345249 0.15489324 0.15241835 0.14564528 0.16188662
 0.15112279 0.17464921 0.1321353  0.09772535]
test_loss at epoch45: 0.0006242676374866911
test_mses at epoch45: [0.15800288 0.01995403 0.02965408 0.02332632 0.00788703 0.00788758
 0.00956202 0.01450245 0.01104461 0.0123916 ]
test_maes at epoch45: [0.27448498 0.09034842 0.08337045 0.09640215 0.06781125 0.0655782
 0.05403358 0.07891629 0.06909379 0.04871318]

train_loss at epoch46: 0.0014651042368748913
train_mses at epoch46: [0.25872379 0.04410908 0.05312891 0.03609716 0.03681398 0.04243596
 0.02970196 0.03255803 0.02939773 0.02260752]
train_maes at epoch46: [0.40094603 0.12866373 0.15096938 0.13586644 0.14325653 0.13939366
 0.12596406 0.12695467 0.12640179 0.09789865]
test_loss at epoch46: 0.0008001949881081996
test_mses at epoch46: [0.23830316 0.01721212 0.05320939 0.03175107 0.00559353 0.00882718
 0.01706447 0.01263585 0.01277223 0.01272864]
test_maes at epoch46: [0.33453561 0.07442179 0.11127232 0.09585243 0.05017786 0.07956635
 0.06347171 0.07178851 0.06766286 0.04921878]

train_loss at epoch47: 0.0016183123883345852
train_mses at epoch47: [0.2676119  0.02860888 0.06102532 0.04887018 0.03526367 0.02935474
 0.02681162 0.07438209 0.03615765 0.0216384 ]
train_maes at epoch47: [0.40519778 0.11321281 0.16678609 0.15495504 0.1310497  0.12721312
 0.11546812 0.15806059 0.14281294 0.09293136]
test_loss at epoch47: 0.0005404781246476847
test_mses at epoch47: [0.12663704 0.01140299 0.01474366 0.02410194 0.00812666 0.00738543
 0.01415598 0.0171733  0.00904418 0.01159018]
test_maes at epoch47: [0.27638444 0.07206574 0.07278014 0.09481227 0.0597377  0.06241301
 0.07739534 0.08619788 0.06513456 0.04838713]

train_loss at epoch48: 0.0015603584925765576
train_mses at epoch48: [0.3016468  0.03663013 0.04039204 0.05298762 0.03492829 0.03043527
 0.03781508 0.04034394 0.0489218  0.02033088]
train_maes at epoch48: [0.42248561 0.1349181  0.13825573 0.15992954 0.12300729 0.13147556
 0.14475383 0.1368949  0.14558825 0.09663011]
test_loss at epoch48: 0.0007381956902859004
test_mses at epoch48: [0.11301397 0.00615417 0.02346075 0.01827175 0.00835422 0.01234914
 0.02667944 0.03332304 0.0258613  0.01267673]
test_maes at epoch48: [0.24794981 0.06084649 0.07774361 0.07711224 0.0603254  0.06624638
 0.08206343 0.1037184  0.09060044 0.05202578]

train_loss at epoch49: 0.0016890802704121756
train_mses at epoch49: [0.18732174 0.03070619 0.07020753 0.03700681 0.03045147 0.03388069
 0.03307111 0.06510789 0.06054212 0.02296593]
train_maes at epoch49: [0.32618755 0.11640912 0.14630565 0.13482026 0.12490392 0.13336505
 0.12959723 0.14862355 0.13660022 0.10107139]
test_loss at epoch49: 0.00044120561695941115
test_mses at epoch49: [0.09599465 0.01598346 0.0182783  0.01492082 0.0050263  0.0075583
 0.00812001 0.0082092  0.00798266 0.01153923]
test_maes at epoch49: [0.23652686 0.06716369 0.09296873 0.08225817 0.05422844 0.05598687
 0.06572829 0.0690532  0.06588503 0.04958314]

