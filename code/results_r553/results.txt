(torch-gpu) yuantang@Yuans-MBP code % python train_facegen.py --seed 553 --dataset_root /Volumes/Yuan-T7/Datasets/face_gen --resume /Volumes/Yuan-T7/FAU_models/checkpoint_epoch_init.pth
PyTorch Version:  2.0.0
Torchvision Version:  0.15.1
Initializing Datasets and Dataloaders...
Train Sets: ['aw' 'e']
Test Sets: ['a' 'eb']
Total Number of Train Sets: 92
Total Number of Test Sets: 92

train_loss at epoch0: 0.039644685776337334
train_mses at epoch0: [3.98115886 1.05710854 1.20217862 1.17258606 1.17982963 0.98671461
 1.14125064 1.17866585 1.1474097  0.02521567]
train_maes at epoch0: [1.5307192  0.38552825 0.53736361 0.43143025 0.35107917 0.4918226
 0.34437488 0.39144337 0.40363125 0.0520733 ]
test_loss at epoch0: 0.038633695115213806
test_mses at epoch0: [3.63220858 1.02196393 1.19098765 1.18085565 1.17764679 0.9341043
 1.10498413 1.17471678 1.08473904 0.02604873]
test_maes at epoch0: [1.48932428 0.40444443 0.58318802 0.4444107  0.36920105 0.50286279
 0.35542026 0.39810563 0.45348131 0.05748662]

train_loss at epoch1: 0.03763901928196783
train_mses at epoch1: [3.20676186 0.91764057 1.19094064 1.12737716 1.17392814 0.86089794
 1.09266707 1.17369295 1.08930147 0.02859243]
train_maes at epoch1: [1.49260615 0.45457459 0.64759488 0.55109014 0.37524047 0.52939056
 0.36122524 0.42028926 0.45122339 0.07056749]
test_loss at epoch1: 0.03697670153949572
test_mses at epoch1: [3.03058321 0.91322123 1.21258892 1.13749429 1.16233365 0.84624374
 1.04452948 1.15373321 1.04863935 0.03070396]
test_maes at epoch1: [1.46809324 0.46162528 0.67671729 0.56445712 0.40218541 0.50629942
 0.3771565  0.42715865 0.51713713 0.07517611]

train_loss at epoch2: 0.03515061347380928
train_mses at epoch2: [2.85444811 0.78021909 1.24182057 1.02788365 1.15694344 0.73352808
 1.01129495 1.14223255 1.03967237 0.03580442]
train_maes at epoch2: [1.50058936 0.50502755 0.72860508 0.64868155 0.405276   0.51027492
 0.37993806 0.44356276 0.52963539 0.09311363]
test_loss at epoch2: 0.0352281435676243
test_mses at epoch2: [2.77785011 0.81185179 1.19884276 1.08284798 1.13711274 0.75704337
 0.97941231 1.1308724  1.02050481 0.03399452]
test_maes at epoch2: [1.45518264 0.45268459 0.68051309 0.61444427 0.44625289 0.43276357
 0.42143449 0.45536009 0.59828029 0.08440373]

train_loss at epoch3: 0.03268979878529259
train_mses at epoch3: [2.59063692 0.65609378 1.220048   0.98368333 1.11742896 0.57858127
 0.95396261 1.09227566 1.00732286 0.03437276]
train_maes at epoch3: [1.4319573  0.43821156 0.68000584 0.66513843 0.44937635 0.36852698
 0.42198848 0.47339296 0.60801177 0.09353936]
test_loss at epoch3: 0.03338033867918927
test_mses at epoch3: [2.61320845 0.70361906 1.17469294 1.00409532 1.11093831 0.67355917
 0.91865898 1.10473342 1.0021141  0.03764587]
test_maes at epoch3: [1.40018829 0.38105876 0.65147376 0.55086189 0.49028538 0.34909024
 0.48602653 0.49875699 0.63828408 0.09034304]

train_loss at epoch4: 0.03052542650181314
train_mses at epoch4: [2.51027672 0.54204556 1.17743039 0.89252172 1.07329875 0.39624088
 0.87046777 1.07735224 1.00889279 0.03731096]
train_maes at epoch4: [1.40221021 0.36180672 0.64561278 0.57161905 0.49638235 0.26414746
 0.48327794 0.53429371 0.64844718 0.1031939 ]
test_loss at epoch4: 0.03133596026379129
test_mses at epoch4: [2.45985491 0.56575047 1.17960353 0.91716238 1.08544093 0.54782654
 0.86678021 1.07570368 0.97050791 0.04875642]
test_maes at epoch4: [1.36563156 0.33468973 0.69145065 0.46999571 0.53652255 0.33057806
 0.55771071 0.5430325  0.63305059 0.10882289]

train_loss at epoch5: 0.027917230258817257
train_mses at epoch5: [2.46833063 0.42868239 1.2343674  0.66611971 1.07695547 0.26007224
 0.83402977 1.03384084 0.86800867 0.05593413]
train_maes at epoch5: [1.38289434 0.34344388 0.71339319 0.47205725 0.55421014 0.26889853
 0.57517025 0.57789928 0.60643068 0.1225703 ]
test_loss at epoch5: 0.029010459780693054
test_mses at epoch5: [2.37486515 0.41902696 1.19344459 0.77607617 1.06120878 0.41497397
 0.81390347 1.04185761 0.92307193 0.06235885]
test_maes at epoch5: [1.3513839  0.30501415 0.73253163 0.43137026 0.57101561 0.32582026
 0.59739771 0.55984156 0.58877713 0.12828497]

train_loss at epoch6: 0.025284811854362488
train_mses at epoch6: [2.38076411 0.20756011 1.16306008 0.55951983 0.98440901 0.18797704
 0.77093952 1.03125706 0.83760028 0.06142432]
train_maes at epoch6: [1.36007948 0.28714101 0.67686538 0.43567486 0.55201722 0.25307483
 0.5919948  0.60273872 0.55750534 0.12332469]
test_loss at epoch6: 0.026625813997310142
test_mses at epoch6: [2.20683845 0.32361008 1.12532401 0.6330878  1.03385486 0.35161139
 0.74064584 1.00647208 0.87252428 0.06040742]
test_maes at epoch6: [1.24063194 0.28317286 0.65547112 0.36979031 0.55893075 0.31517704
 0.53081921 0.53973825 0.51573952 0.12972069]

train_loss at epoch7: 0.022284022491911182
train_mses at epoch7: [2.37784947 0.18742056 1.07725323 0.32882496 0.99437411 0.15667764
 0.64299176 0.94077931 0.77941599 0.06048028]
train_maes at epoch7: [1.27880143 0.27211747 0.58724922 0.34277844 0.5327018  0.26436271
 0.51462752 0.56167661 0.51369474 0.13767224]
test_loss at epoch7: 0.02446300568787948
test_mses at epoch7: [2.09939746 0.27004859 1.04910615 0.48075495 1.00120477 0.33141655
 0.67071243 0.96383231 0.82335146 0.04632406]
test_maes at epoch7: [1.11688304 0.28419778 0.57602602 0.3287947  0.52281499 0.32872288
 0.44975439 0.51104648 0.47728853 0.11491655]

train_loss at epoch8: 0.020199770512788193
train_mses at epoch8: [2.07521432 0.14051886 0.95590887 0.29148537 0.8961943  0.16559282
 0.57884583 0.91603317 0.63168374 0.04484928]
train_maes at epoch8: [1.10455353 0.27318441 0.51809002 0.32913    0.49400694 0.29336822
 0.4672198  0.54525147 0.44197183 0.11811379]
test_loss at epoch8: 0.02207471041575722
test_mses at epoch8: [1.98191091 0.22817267 0.97140607 0.33305357 0.95894615 0.30351379
 0.58265467 0.9032243  0.76200213 0.03280891]
test_maes at epoch8: [1.06433228 0.280827   0.53663532 0.31690046 0.50848647 0.34876758
 0.39510318 0.49379556 0.44557058 0.10193749]

train_loss at epoch9: 0.018743837333243828
train_mses at epoch9: [2.00075468 0.14380899 0.88982689 0.24761524 0.88336355 0.18786272
 0.4908208  0.83519149 0.55396202 0.03460651]
train_maes at epoch9: [1.00529205 0.2794665  0.49578643 0.32687418 0.52644877 0.3269721
 0.43422556 0.51432816 0.42687881 0.12004113]
test_loss at epoch9: 0.02066244444121485
test_mses at epoch9: [1.94075703 0.20568267 0.89374855 0.30798736 0.92318493 0.31928631
 0.53600486 0.83897634 0.69721437 0.0237733 ]
test_maes at epoch9: [1.02337522 0.25813162 0.49978198 0.3031919  0.5325127  0.35308202
 0.35672182 0.477747   0.43161305 0.09342078]

train_loss at epoch10: 0.016551918309667835
train_mses at epoch10: [2.4746547  0.11966619 0.79883736 0.20912034 0.82538199 0.18078151
 0.44979534 0.70919918 0.42386809 0.03398994]
train_maes at epoch10: [1.07865579 0.25600701 0.45568151 0.32096908 0.53000274 0.32021832
 0.43799158 0.46820848 0.41938377 0.13383238]
test_loss at epoch10: 0.01897917558317599
test_mses at epoch10: [1.6483147  0.185595   0.80487054 0.30179508 0.88464149 0.32405309
 0.46001636 0.77156595 0.62163717 0.02048917]
test_maes at epoch10: [0.99459797 0.2472349  0.51378154 0.31570094 0.5658974  0.36162782
 0.32258599 0.44468781 0.41945442 0.09342872]

train_loss at epoch11: 0.014433295506498089
train_mses at epoch11: [1.71446462 0.13947685 0.69833238 0.18560708 0.75549119 0.14553172
 0.35390343 0.67116792 0.32315773 0.03118049]
train_maes at epoch11: [0.97089975 0.26356628 0.47109976 0.30529427 0.55349118 0.28635436
 0.3934521  0.48826142 0.36909623 0.12888192]
test_loss at epoch11: 0.017424729855164238
test_mses at epoch11: [1.56232613 0.20313456 0.72296605 0.29002743 0.82879252 0.32920842
 0.39314956 0.68263803 0.54741563 0.0180168 ]
test_maes at epoch11: [0.96433392 0.24492904 0.506269   0.28862919 0.51026914 0.35549303
 0.30629769 0.41007226 0.40331229 0.08751904]

train_loss at epoch12: 0.011857176280539969
train_mses at epoch12: [1.74252703 0.10813804 0.6121081  0.14081254 0.64630639 0.15281159
 0.22858534 0.52115762 0.25989111 0.03388547]
train_maes at epoch12: [0.9793333  0.22226012 0.43530761 0.2585127  0.44861529 0.30676185
 0.34063445 0.42817715 0.32726064 0.14116366]
test_loss at epoch12: 0.01662548145522242
test_mses at epoch12: [1.49560112 0.22268961 0.65238528 0.32821142 0.78194361 0.32796753
 0.39769577 0.6021193  0.50218071 0.01796578]
test_maes at epoch12: [0.96022456 0.2477395  0.50093087 0.30423373 0.47109884 0.34126631
 0.28937856 0.37962277 0.39072783 0.0839614 ]

train_loss at epoch13: 0.011743329303420109
train_mses at epoch13: [1.38466685 0.09112656 0.56648914 0.18069939 0.56869727 0.13269267
 0.26757232 0.55224346 0.2568487  0.04048396]
train_maes at epoch13: [0.91328179 0.21510807 0.42750892 0.28872004 0.40786902 0.28495123
 0.34309169 0.46025724 0.32964843 0.14972296]
test_loss at epoch13: 0.015154783816441246
test_mses at epoch13: [1.39063615 0.18836394 0.57815621 0.35968979 0.71829975 0.31216746
 0.33816158 0.54389219 0.43958824 0.02137645]
test_maes at epoch13: [0.96240777 0.24355306 0.49265925 0.30038119 0.47227657 0.37068948
 0.28714538 0.35304972 0.389554   0.09643805]

train_loss at epoch14: 0.009970696076102879
train_mses at epoch14: [1.4168455  0.08141887 0.37895686 0.19625063 0.52898498 0.12879596
 0.23780536 0.3932624  0.25760172 0.03715195]
train_maes at epoch14: [0.93533319 0.20953761 0.39014559 0.3049911  0.42098528 0.27592782
 0.3319226  0.36783731 0.37511363 0.13294588]
test_loss at epoch14: 0.014398113540981127
test_mses at epoch14: [1.32925552 0.1657488  0.49568478 0.40669621 0.67781156 0.29169222
 0.32675737 0.52293321 0.41265694 0.02519375]
test_maes at epoch14: [0.93830226 0.23356453 0.41709981 0.30163792 0.47891882 0.31783467
 0.27610655 0.35806117 0.38532169 0.09895896]

train_loss at epoch15: 0.008249831912310227
train_mses at epoch15: [1.06074265 0.06754645 0.32538185 0.12779477 0.47859494 0.08501175
 0.18189316 0.39117019 0.1511969  0.02907673]
train_maes at epoch15: [0.76880527 0.18638987 0.352145   0.27277032 0.379381   0.2334055
 0.29116467 0.3749995  0.28617245 0.11982267]
test_loss at epoch15: 0.01394560544387154
test_mses at epoch15: [1.2732717  0.15332868 0.47460757 0.42775689 0.64111075 0.28456442
 0.32371248 0.4431994  0.44819265 0.03013801]
test_maes at epoch15: [0.89966177 0.22608886 0.42504448 0.29828928 0.47419195 0.32687671
 0.2620202  0.31347689 0.35776256 0.09978783]

train_loss at epoch16: 0.007005948573350906
train_mses at epoch16: [1.02276437 0.07511783 0.2715457  0.20023714 0.32931538 0.08133362
 0.13885798 0.31107657 0.1556516  0.03476277]
train_maes at epoch16: [0.79781896 0.19243703 0.32200112 0.28031465 0.33367787 0.23265674
 0.2517206  0.33853723 0.29976814 0.13674346]
test_loss at epoch16: 0.013022342777770498
test_mses at epoch16: [1.19097213 0.14285305 0.48731207 0.35473305 0.61893635 0.26173847
 0.32791847 0.38180014 0.41297742 0.032767  ]
test_maes at epoch16: [0.89560849 0.22068719 0.4808064  0.29354789 0.53622395 0.33123372
 0.24476442 0.29489143 0.39574231 0.10378948]

train_loss at epoch17: 0.006266037569097851
train_mses at epoch17: [0.79737054 0.07705975 0.29144265 0.12026572 0.25812923 0.07303285
 0.14912004 0.27383056 0.14166998 0.02790083]
train_maes at epoch17: [0.71060156 0.18988437 0.3356671  0.26807158 0.329331   0.2137377
 0.25793743 0.35590911 0.27941711 0.12222178]
test_loss at epoch17: 0.01300738654706789
test_mses at epoch17: [1.17281289 0.18852359 0.4205354  0.40853382 0.57796745 0.25304994
 0.32071879 0.42313259 0.40676539 0.02521742]
test_maes at epoch17: [0.8940046  0.23284514 0.41395758 0.30729053 0.51852902 0.32179008
 0.25879682 0.35455174 0.41471545 0.09147063]

train_loss at epoch18: 0.005373759600131408
train_mses at epoch18: [0.67230026 0.05953853 0.23765481 0.1135255  0.17353266 0.05891092
 0.17456599 0.28820468 0.08102083 0.02460642]
train_maes at epoch18: [0.61642935 0.18294635 0.31498396 0.25875668 0.24975078 0.191838
 0.25883726 0.32269344 0.20839166 0.1112941 ]
test_loss at epoch18: 0.012457691456960596
test_mses at epoch18: [1.11139758 0.15718815 0.4016978  0.37128719 0.53151343 0.24972963
 0.28797365 0.4534613  0.41815383 0.02334351]
test_maes at epoch18: [0.88580664 0.20783983 0.42087449 0.31215171 0.51091988 0.32583061
 0.28861696 0.42031096 0.35579312 0.08917231]

train_loss at epoch19: 0.005079657811185588
train_mses at epoch19: [0.64251485 0.08669109 0.25453959 0.09502827 0.14064484 0.0700793
 0.14403167 0.2243818  0.08599731 0.04373313]
train_maes at epoch19: [0.60442482 0.19602404 0.3176764  0.22464386 0.23917347 0.20466019
 0.24859677 0.28159846 0.21799191 0.14454633]
test_loss at epoch19: 0.01236552550740864
test_mses at epoch19: [1.11352691 0.14455261 0.41904178 0.42116907 0.53747483 0.24507962
 0.33006193 0.35437837 0.39712742 0.02474492]
test_maes at epoch19: [0.8768411  0.20685953 0.44247086 0.32614922 0.5574377  0.30021812
 0.23954813 0.31075144 0.36404667 0.09163867]

train_loss at epoch20: 0.004701619562895402
train_mses at epoch20: [0.67423753 0.06063775 0.20992865 0.11865912 0.11850627 0.07052039
 0.11896189 0.26755026 0.06734177 0.02473723]
train_maes at epoch20: [0.62074112 0.17231955 0.29002214 0.22455857 0.23023551 0.20171257
 0.24527848 0.32091453 0.20300854 0.11308806]
test_loss at epoch20: 0.011815847262092258
test_mses at epoch20: [1.09337781 0.14528524 0.41549053 0.34081926 0.51434958 0.25076258
 0.30234606 0.39049427 0.36748481 0.02050941]
test_maes at epoch20: [0.86223215 0.19886561 0.43602818 0.31136641 0.55823968 0.29738295
 0.23294239 0.3497548  0.40235175 0.08640353]

train_loss at epoch21: 0.004506096729765768
train_mses at epoch21: [0.61243577 0.03738495 0.20523455 0.09476979 0.1062735  0.03904607
 0.13508182 0.24215264 0.11477954 0.02545195]
train_maes at epoch21: [0.59349897 0.14637747 0.28831401 0.22605505 0.22544024 0.15299145
 0.21943528 0.27904752 0.21192372 0.11967865]
test_loss at epoch21: 0.011447933057080145
test_mses at epoch21: [1.04791531 0.14876781 0.39646118 0.35153343 0.44160883 0.26234301
 0.26052061 0.41973838 0.36417313 0.01831303]
test_maes at epoch21: [0.84144773 0.19334422 0.41140804 0.30133246 0.49695225 0.33274532
 0.31072084 0.36267939 0.34623089 0.08290492]

train_loss at epoch22: 0.0042911916971206665
train_mses at epoch22: [0.65562924 0.04174976 0.21204201 0.06937907 0.08019391 0.05296673
 0.11379454 0.31277798 0.06474483 0.02605395]
train_maes at epoch22: [0.5998933  0.15275144 0.28384261 0.19291802 0.20113541 0.1783403
 0.24163048 0.30359387 0.19182095 0.11500295]
test_loss at epoch22: 0.011299811627553858
test_mses at epoch22: [1.06108644 0.12502538 0.39886975 0.38477741 0.54043718 0.25164294
 0.23572816 0.33294451 0.34245997 0.01971963]
test_maes at epoch22: [0.83688602 0.18839221 0.43027321 0.30125274 0.61102221 0.28901118
 0.28752925 0.29084851 0.33828924 0.08400836]

train_loss at epoch23: 0.003664365281229434
train_mses at epoch23: [0.51958768 0.05153419 0.16705959 0.07532306 0.05800436 0.07694315
 0.09672243 0.21380524 0.06122733 0.02764328]
train_maes at epoch23: [0.52623429 0.15647097 0.24627672 0.18222845 0.16931456 0.19355651
 0.20228515 0.27925149 0.17998869 0.13015228]
test_loss at epoch23: 0.01128310677797898
test_mses at epoch23: [1.02687504 0.09061942 0.39534124 0.3413607  0.55817944 0.27120847
 0.26524435 0.35652303 0.32058454 0.02573181]
test_maes at epoch23: [0.81779348 0.1656037  0.41833817 0.27747209 0.62787419 0.27065421
 0.22540437 0.29718417 0.34812763 0.09836757]

train_loss at epoch24: 0.003335961748076522
train_mses at epoch24: [0.56312732 0.03386638 0.15164185 0.051762   0.07168277 0.06599609
 0.08773779 0.20971084 0.05528987 0.02688603]
train_maes at epoch24: [0.60411279 0.1347827  0.22189622 0.16802591 0.18878284 0.18635403
 0.19678948 0.26001726 0.17506401 0.11907148]
test_loss at epoch24: 0.01068546817354534
test_mses at epoch24: [0.93259346 0.08633606 0.36853503 0.31900784 0.5065517  0.22020661
 0.23443813 0.41674259 0.3133323  0.02593285]
test_maes at epoch24: [0.78491755 0.1577876  0.39807719 0.26331883 0.58402738 0.27481704
 0.21625448 0.3618483  0.32862777 0.09893933]

train_loss at epoch25: 0.0038334929748721743
train_mses at epoch25: [0.56296663 0.04876348 0.20105952 0.06782601 0.04448235 0.06482075
 0.08297686 0.25214938 0.07324327 0.0259082 ]
train_maes at epoch25: [0.60987553 0.15978605 0.28316629 0.18139672 0.16766804 0.17540834
 0.19030461 0.26930781 0.19165077 0.11785227]
test_loss at epoch25: 0.010284233028474062
test_mses at epoch25: [0.89905454 0.09218778 0.35154349 0.32585936 0.43771573 0.20799337
 0.20915673 0.43619805 0.31363382 0.0240054 ]
test_maes at epoch25: [0.77447156 0.15906519 0.38625203 0.26325564 0.51665821 0.28919496
 0.22884958 0.37883232 0.31083127 0.09705976]

train_loss at epoch26: 0.0034438890769429827
train_mses at epoch26: [0.4449796  0.03138731 0.18091054 0.07152905 0.0553161  0.12073249
 0.07503318 0.17721458 0.05267512 0.0249913 ]
train_maes at epoch26: [0.49915168 0.12377079 0.23913045 0.17538471 0.16114419 0.21040922
 0.16845926 0.24393851 0.18141478 0.10893757]
test_loss at epoch26: 0.01093323528766632
test_mses at epoch26: [0.93177543 0.09557214 0.36059527 0.34301092 0.45922033 0.31702835
 0.23515511 0.35714013 0.3490039  0.02229051]
test_maes at epoch26: [0.77964756 0.16245182 0.36713796 0.26019431 0.53561766 0.2589064
 0.21486183 0.2854027  0.31266902 0.09202674]

train_loss at epoch27: 0.0028870011801305022
train_mses at epoch27: [0.37687036 0.05492561 0.1327263  0.05509517 0.05706844 0.0678442
 0.03811754 0.18430627 0.0331598  0.02830587]
train_maes at epoch27: [0.46766967 0.16051278 0.20376932 0.17394884 0.14903926 0.1939913
 0.13275274 0.25014637 0.13956044 0.12316297]
test_loss at epoch27: 0.010212994464065718
test_mses at epoch27: [0.89626182 0.10157009 0.36612114 0.21466766 0.56868334 0.25371881
 0.20061315 0.32511972 0.32662506 0.0216187 ]
test_maes at epoch27: [0.76638787 0.18652624 0.394793   0.25637629 0.64129015 0.25460104
 0.21614122 0.28706696 0.33374297 0.09362305]

train_loss at epoch28: 0.002742587710204332
train_mses at epoch28: [0.35298813 0.02865714 0.13086146 0.06524642 0.0635711  0.05587821
 0.05391933 0.15210797 0.04838347 0.02253895]
train_maes at epoch28: [0.44839798 0.13610479 0.224649   0.18183049 0.17289135 0.16651818
 0.17213121 0.25068854 0.16945112 0.11146687]
test_loss at epoch28: 0.011250026200128637
test_mses at epoch28: [0.98256184 0.14644031 0.38432646 0.32123273 0.61164566 0.23768409
 0.233358   0.33860933 0.33588587 0.02159796]
test_maes at epoch28: [0.78485438 0.19151801 0.37083787 0.25879057 0.67443268 0.30863603
 0.22670592 0.28828434 0.32567069 0.09187729]

train_loss at epoch29: 0.0028080485113289046
train_mses at epoch29: [0.41352543 0.06303525 0.16226032 0.05658925 0.0544233  0.04634019
 0.03893871 0.13770469 0.05587881 0.02312649]
train_maes at epoch29: [0.47032538 0.14993247 0.24246766 0.17176896 0.17861241 0.16863329
 0.14395709 0.2328763  0.17215316 0.10296823]
test_loss at epoch29: 0.011282717080219933
test_mses at epoch29: [0.99641071 0.13613    0.4027771  0.32045789 0.4833087  0.25721015
 0.25821305 0.37692012 0.36759304 0.02471622]
test_maes at epoch29: [0.78137076 0.16462687 0.36541091 0.25023426 0.56693197 0.31549656
 0.22946434 0.30399374 0.30171184 0.09784327]

train_loss at epoch30: 0.0026065445626559463
train_mses at epoch30: [0.24398023 0.03285966 0.1049412  0.06673441 0.03182385 0.0527358
 0.06344181 0.12793951 0.09721628 0.02754729]
train_maes at epoch30: [0.40173622 0.12049985 0.21331216 0.17578233 0.12529397 0.1534611
 0.14823905 0.23009032 0.19608753 0.11010466]
test_loss at epoch30: 0.009425648204658342
test_mses at epoch30: [0.7904489  0.08348237 0.35693411 0.16687936 0.40920714 0.22342569
 0.19681782 0.42066223 0.30998465 0.02876175]
test_maes at epoch30: [0.70658344 0.16291655 0.38216233 0.22213092 0.4996677  0.29040631
 0.24946207 0.37435539 0.30373678 0.10880474]

train_loss at epoch31: 0.002802188629689424
train_mses at epoch31: [0.33199483 0.07587083 0.11277599 0.08217009 0.04190303 0.03903567
 0.06385119 0.14245826 0.06773355 0.01961137]
train_maes at epoch31: [0.44237618 0.17115277 0.20362897 0.17916116 0.15325781 0.1431279
 0.16722545 0.24130968 0.18749324 0.10261637]
test_loss at epoch31: 0.010107661718907564
test_mses at epoch31: [0.91147049 0.11388966 0.36530076 0.27997788 0.42410863 0.28537692
 0.21501285 0.35669863 0.28710353 0.02334861]
test_maes at epoch31: [0.74987821 0.15352033 0.35971897 0.23878519 0.51595829 0.25809918
 0.22944265 0.29047122 0.32211295 0.0947019 ]

train_loss at epoch32: 0.0024633823448549147
train_mses at epoch32: [0.34690394 0.05597482 0.11545239 0.04492878 0.03188985 0.06072849
 0.0320507  0.13469545 0.06343015 0.02299051]
train_maes at epoch32: [0.45467379 0.13650624 0.19888024 0.16278063 0.13545363 0.17404162
 0.12793819 0.24406743 0.17943653 0.11449649]
test_loss at epoch32: 0.011433945077916851
test_mses at epoch32: [1.08167514 0.14074235 0.37341321 0.44546676 0.4564728  0.29779667
 0.2399987  0.36229454 0.31939349 0.02119223]
test_maes at epoch32: [0.79704144 0.16210245 0.3520911  0.27820077 0.55051431 0.25561547
 0.22028273 0.28478474 0.28937687 0.08817377]

train_loss at epoch33: 0.0024260664358735085
train_mses at epoch33: [0.43185566 0.07563303 0.08131204 0.07277019 0.04223622 0.03460039
 0.04227351 0.11898867 0.04934034 0.0246533 ]
train_maes at epoch33: [0.52214093 0.17438047 0.17553664 0.16841643 0.14269598 0.13055579
 0.14513169 0.22594802 0.17101379 0.11926279]
test_loss at epoch33: 0.010351969496063564
test_mses at epoch33: [0.92211297 0.08889972 0.34760887 0.27348534 0.46251871 0.2439804
 0.19850902 0.41361258 0.35185543 0.02480757]
test_maes at epoch33: [0.76382338 0.15918085 0.38045029 0.25689583 0.55997663 0.2828194
 0.2134886  0.31926549 0.28879662 0.09817141]

train_loss at epoch34: 0.0020728785868572154
train_mses at epoch34: [0.29760651 0.0340542  0.09018338 0.07890963 0.03069097 0.03378435
 0.03312225 0.10917088 0.03990387 0.02408221]
train_maes at epoch34: [0.4360689  0.13339062 0.21811699 0.19846867 0.12678231 0.1354382
 0.13206024 0.19641465 0.15584473 0.1090821 ]
test_loss at epoch34: 0.011294816175232763
test_mses at epoch34: [0.96576015 0.09783161 0.37321961 0.36396307 0.42631162 0.26864364
 0.26067358 0.44159852 0.36496172 0.02706584]
test_maes at epoch34: [0.76290932 0.15344454 0.36361847 0.25493405 0.5056805  0.29825891
 0.22020361 0.30553372 0.28389165 0.09646187]

train_loss at epoch35: 0.0020075670807905817
train_mses at epoch35: [0.33372817 0.03096861 0.10202544 0.07194364 0.04863099 0.03384576
 0.03445426 0.07367289 0.03741775 0.01677455]
train_maes at epoch35: [0.45945448 0.12035394 0.20702069 0.16274264 0.14355619 0.14091817
 0.12643741 0.17449722 0.13882271 0.08680978]
test_loss at epoch35: 0.01137176091256349
test_mses at epoch35: [1.03925708 0.1115949  0.39391891 0.40070886 0.44528444 0.26263349
 0.25399796 0.41360012 0.33446607 0.02641478]
test_maes at epoch35: [0.77613505 0.14711201 0.35880801 0.26319806 0.52423517 0.27643527
 0.21797854 0.28773086 0.30027876 0.09271965]

train_loss at epoch36: 0.0018617433455327282
train_mses at epoch36: [0.26113517 0.04086899 0.08146211 0.05636192 0.02911723 0.05001011
 0.0310554  0.05645273 0.05336579 0.02466256]
train_maes at epoch36: [0.41265975 0.1356732  0.18381474 0.15922944 0.12945445 0.15074848
 0.11722923 0.16228212 0.16031159 0.11254802]
test_loss at epoch36: 0.010280801550201748
test_mses at epoch36: [0.91658538 0.11053903 0.35268347 0.31097505 0.4732128  0.22761076
 0.20104203 0.39307111 0.30181891 0.02568617]
test_maes at epoch36: [0.73976192 0.15178727 0.35759032 0.25446636 0.55817611 0.25713134
 0.22511751 0.29037419 0.32092181 0.09323611]

train_loss at epoch37: 0.0021045273979720864
train_mses at epoch37: [0.24047576 0.03889448 0.10101574 0.05202065 0.02953381 0.06601644
 0.0447728  0.06059181 0.06417391 0.02322516]
train_maes at epoch37: [0.37413313 0.13646059 0.21397413 0.1699668  0.12227883 0.17877511
 0.14287443 0.1658723  0.18041519 0.10582073]
test_loss at epoch37: 0.011473215792490088
test_mses at epoch37: [1.02735771 0.11638604 0.41577296 0.38098195 0.44691751 0.26310786
 0.2432012  0.4212508  0.35318067 0.0255362 ]
test_maes at epoch37: [0.76787891 0.14717586 0.3545544  0.26872322 0.52288589 0.24974453
 0.22422176 0.28932576 0.30074333 0.09122445]

train_loss at epoch38: 0.0018625673635498338
train_mses at epoch38: [0.27167432 0.03643742 0.07750671 0.04577242 0.0355368  0.03666245
 0.02820264 0.08732867 0.05800909 0.01676271]
train_maes at epoch38: [0.41295204 0.13074303 0.17060736 0.15941546 0.11809073 0.13513996
 0.11318171 0.1745466  0.16862193 0.091735  ]
test_loss at epoch38: 0.011306332829205887
test_mses at epoch38: [0.90020962 0.09728853 0.39898953 0.31871135 0.43875997 0.27634318
 0.25101832 0.43474147 0.38810649 0.02729669]
test_maes at epoch38: [0.73675361 0.14430813 0.35254442 0.24821083 0.51353727 0.24592124
 0.22177732 0.29108204 0.28497372 0.09478349]

train_loss at epoch39: 0.0019208330094166424
train_mses at epoch39: [0.31760691 0.02372757 0.10092963 0.04852691 0.03915457 0.04402147
 0.02931533 0.07339638 0.05114271 0.01708807]
train_maes at epoch39: [0.43096934 0.10886829 0.22272385 0.14593028 0.1295754  0.16517707
 0.11907299 0.16744052 0.15090701 0.09615461]
test_loss at epoch39: 0.01082468648319659
test_mses at epoch39: [0.86722435 0.0968631  0.37458978 0.31043162 0.45671568 0.24187747
 0.226191   0.4201529  0.37344621 0.0246932 ]
test_maes at epoch39: [0.72981215 0.14265038 0.35793945 0.24901512 0.53932762 0.25941176
 0.23068199 0.2939409  0.28010535 0.09300187]

train_loss at epoch40: 0.0016425504632618117
train_mses at epoch40: [0.2365427  0.02007114 0.07572084 0.04723722 0.03234601 0.0383149
 0.04371313 0.0574139  0.03614906 0.02328297]
train_maes at epoch40: [0.37684961 0.10247587 0.19005996 0.15329925 0.12094838 0.15263079
 0.1398778  0.1584588  0.13307173 0.10176099]
test_loss at epoch40: 0.011569708261800848
test_mses at epoch40: [1.01446    0.12941551 0.42192014 0.38738525 0.44397476 0.25076666
 0.23710565 0.44348503 0.36267438 0.01879619]
test_maes at epoch40: [0.77018975 0.15411845 0.35313408 0.26315439 0.5225645  0.29391618
 0.25698562 0.29068344 0.27372241 0.08694937]

train_loss at epoch41: 0.001697370499048544
train_mses at epoch41: [0.19997206 0.04061692 0.04061875 0.04313047 0.04333217 0.03148009
 0.08094341 0.04447711 0.04574297 0.02232737]
train_maes at epoch41: [0.35730731 0.11997261 0.15396048 0.1417143  0.13357344 0.13634191
 0.16175875 0.13970278 0.13756119 0.10576576]
test_loss at epoch41: 0.010855550999226778
test_mses at epoch41: [0.91452221 0.10732924 0.39321912 0.318426   0.42738865 0.27914104
 0.21526954 0.44423866 0.32474153 0.01878901]
test_maes at epoch41: [0.74437682 0.1520845  0.35241087 0.24038841 0.50799846 0.29041635
 0.27026681 0.29125247 0.28078701 0.09602426]

train_loss at epoch42: 0.0016610430472570918
train_mses at epoch42: [0.2854209  0.04748003 0.05750576 0.04820307 0.02530876 0.03117156
 0.06200598 0.03941505 0.03440118 0.02053899]
train_maes at epoch42: [0.41291503 0.14190326 0.1652143  0.15269828 0.11406233 0.13477693
 0.16534363 0.13292326 0.12971581 0.10592887]
test_loss at epoch42: 0.011073641802953638
test_mses at epoch42: [0.90684619 0.10741054 0.39993593 0.29255646 0.42678229 0.31203555
 0.2140816  0.46719349 0.33763184 0.01884253]
test_maes at epoch42: [0.74579539 0.15175847 0.35195388 0.23000979 0.51083564 0.27703251
 0.25426953 0.29386282 0.28566    0.09411364]

train_loss at epoch43: 0.0014864174730103948
train_mses at epoch43: [0.19967657 0.02427287 0.04948962 0.05060823 0.02687903 0.03891575
 0.03907102 0.05948555 0.03362689 0.01767619]
train_maes at epoch43: [0.37303943 0.10666162 0.15928269 0.15628084 0.10693832 0.14319337
 0.12315398 0.16255879 0.13314402 0.09657753]
test_loss at epoch43: 0.011330076209876848
test_mses at epoch43: [0.95793138 0.10677872 0.43147892 0.31374434 0.45775668 0.27830027
 0.21642608 0.45848691 0.35411526 0.02150675]
test_maes at epoch43: [0.76070496 0.15274246 0.35671205 0.22615314 0.54843747 0.27752704
 0.23063872 0.2972214  0.28408999 0.09351986]

train_loss at epoch44: 0.00159336241852978
train_mses at epoch44: [0.22385618 0.01989353 0.05992423 0.04843566 0.02938968 0.04036318
 0.04718242 0.05360635 0.04446066 0.01499128]
train_maes at epoch44: [0.36545335 0.10302099 0.15907674 0.14453257 0.12043701 0.14515641
 0.13833407 0.16330669 0.14085766 0.08549972]
test_loss at epoch44: 0.01130856120068094
test_mses at epoch44: [0.96029567 0.11196141 0.41680342 0.32501102 0.44457937 0.2625094
 0.21727828 0.46675436 0.36415299 0.0248803 ]
test_maes at epoch44: [0.75970949 0.15344733 0.35265718 0.22512079 0.53876538 0.27593936
 0.22189212 0.2980207  0.28501429 0.09706547]

train_loss at epoch45: 0.0012879072000151095
train_mses at epoch45: [0.17430934 0.02613678 0.02760799 0.04600208 0.02006199 0.04248145
 0.02881056 0.05273805 0.03264063 0.01409716]
train_maes at epoch45: [0.32897779 0.10998666 0.1264006  0.14427627 0.10289157 0.13584507
 0.11468617 0.15298234 0.12719395 0.08989634]
test_loss at epoch45: 0.011003247097782467
test_mses at epoch45: [0.92292207 0.11184418 0.38462222 0.31115964 0.42308979 0.24415996
 0.19985466 0.48962607 0.37299075 0.02487997]
test_maes at epoch45: [0.74484592 0.15504274 0.34724262 0.22065301 0.51882601 0.27240501
 0.22471766 0.30143482 0.28721149 0.09581264]

train_loss at epoch46: 0.0015202043578028679
train_mses at epoch46: [0.17448862 0.03692776 0.06075664 0.03876927 0.02533966 0.02527922
 0.03392643 0.04460208 0.06321225 0.01955744]
train_maes at epoch46: [0.31163092 0.13709185 0.1702092  0.13321021 0.11954003 0.12162438
 0.13181268 0.1448155  0.15681494 0.10022661]
test_loss at epoch46: 0.011351697470830835
test_mses at epoch46: [0.98326913 0.11197303 0.43488715 0.32110361 0.43581145 0.24562176
 0.22362588 0.48786366 0.35983378 0.02139216]
test_maes at epoch46: [0.7566041  0.15984335 0.35450232 0.21990376 0.53133864 0.26806892
 0.22464158 0.30019947 0.29154441 0.08991564]

train_loss at epoch47: 0.0011880911238815474
train_mses at epoch47: [0.18415961 0.03054812 0.03242452 0.02939566 0.02203548 0.02544301
 0.02777969 0.04384752 0.04317208 0.01079033]
train_maes at epoch47: [0.32808168 0.11725875 0.13065607 0.12755887 0.10255893 0.11328939
 0.11587135 0.14518742 0.13502876 0.07221784]
test_loss at epoch47: 0.011234679623790409
test_mses at epoch47: [0.99699462 0.10615209 0.44664569 0.29882454 0.48528576 0.23204102
 0.2038611  0.50637644 0.32126451 0.01770698]
test_maes at epoch47: [0.7627379  0.15951582 0.36042134 0.2201247  0.57940085 0.26671461
 0.24832797 0.31511845 0.29922181 0.08735685]

train_loss at epoch48: 0.001222703565397988
train_mses at epoch48: [0.15057435 0.01478301 0.03958232 0.04554124 0.02619776 0.02741624
 0.02318147 0.04727135 0.03968287 0.01496089]
train_maes at epoch48: [0.30783443 0.08973065 0.1388723  0.1336025  0.10945278 0.12661509
 0.10590941 0.15362243 0.14482262 0.09603432]
test_loss at epoch48: 0.011483094614485035
test_mses at epoch48: [1.00322824 0.10748611 0.4272395  0.33114126 0.44989007 0.2696888
 0.20405143 0.54724555 0.31930369 0.0159447 ]
test_maes at epoch48: [0.7725011  0.15793706 0.35154161 0.23012822 0.54438373 0.25941567
 0.26839482 0.32095835 0.29406162 0.08270911]

train_loss at epoch49: 0.001258524559924136
train_mses at epoch49: [0.21116099 0.0391067  0.03026713 0.03130096 0.02286191 0.02887344
 0.02915476 0.05202996 0.03301193 0.01797286]
train_maes at epoch49: [0.33891996 0.12447649 0.11999673 0.12599614 0.1031492  0.11876602
 0.12938244 0.14269931 0.13052365 0.09928146]
test_loss at epoch49: 0.011006742391897284
test_mses at epoch49: [0.97457378 0.08679872 0.42216496 0.32038253 0.41142783 0.27807865
 0.20629025 0.47289724 0.3399031  0.01796155]
test_maes at epoch49: [0.75591335 0.15931881 0.35445422 0.22557837 0.49710993 0.26113505
 0.24967339 0.29370471 0.28021628 0.08386924]

