train_loss at epoch0: 0.03379420980494073
train_mses at epoch0: [3.31024131 1.06170944 1.14000979 0.9941875  1.04246395 0.45990876
 1.08781364 1.09519909 1.06928266 0.02090514]
train_maes at epoch0: [1.51991996 0.52103024 0.62479039 0.58095534 0.40283521 0.39268226
 0.36780147 0.51224563 0.5652616  0.05407868]
test_loss at epoch0: 0.031612602700578404
test_mses at epoch0: [2.98589168 0.98467582 1.14748935 0.98756469 0.98473174 0.28961688
 1.01573619 1.03066687 0.98558223 0.038615  ]
test_maes at epoch0: [1.52557591 0.48596882 0.71281473 0.36673235 0.52570963 0.35416056
 0.48466473 0.53588143 0.48230233 0.142662  ]

train_loss at epoch1: 0.026173255545027713
train_mses at epoch1: [2.00821921 0.73859248 1.13104522 0.72968923 0.81398947 0.09854207
 0.90865954 0.96540977 0.80059211 0.02513774]
train_maes at epoch1: [1.234641   0.48238448 0.59560972 0.49531289 0.54062501 0.21656289
 0.56327344 0.53918307 0.47179238 0.08526135]
test_loss at epoch1: 0.026649054821501386
test_mses at epoch1: [1.55534026 0.72283132 1.05864427 0.71338944 0.70864715 0.50664249
 0.80807088 0.96120012 0.73236346 0.15388681]
test_maes at epoch1: [1.0869079  0.44528447 0.7374288  0.38847249 0.39626053 0.60570812
 0.4092641  0.41721572 0.39259917 0.33728185]

train_loss at epoch2: 0.015569715645719082
train_mses at epoch2: [0.81919135 0.40345109 0.83360693 0.41993292 0.45409213 0.13120639
 0.40675398 0.74276211 0.3183082  0.03772787]
train_maes at epoch2: [0.73417446 0.4278031  0.55274478 0.424245   0.42330655 0.27288519
 0.37517868 0.48662566 0.3611451  0.13332017]
test_loss at epoch2: 0.021548856889947933
test_mses at epoch2: [1.28697847 0.42042393 0.78289109 1.16980772 0.42655173 0.49880042
 0.41668265 0.72076657 0.32020837 0.35683365]
test_maes at epoch2: [0.91436876 0.36963039 0.49009079 0.8816566  0.39095382 0.60506428
 0.36977076 0.47914894 0.31858545 0.53106215]

train_loss at epoch3: 0.008121449230833256
train_mses at epoch3: [0.74305232 0.13496855 0.52760693 0.26834865 0.19570043 0.08526333
 0.12285983 0.44287837 0.11328455 0.04212263]
train_maes at epoch3: [0.69454138 0.25843034 0.45003493 0.36536863 0.33046397 0.21484644
 0.24765083 0.44694793 0.24834246 0.15599615]
test_loss at epoch3: 0.017531491879453052
test_mses at epoch3: [0.92872304 0.31468593 0.6913984  1.12949947 0.30147331 0.39896517
 0.26481064 0.50577633 0.25372148 0.28431257]
test_maes at epoch3: [0.78998696 0.37134479 0.63652222 0.78439404 0.29450872 0.5476372
 0.31307529 0.4236581  0.25995136 0.45637706]

train_loss at epoch4: 0.004572051256260973
train_mses at epoch4: [0.53619047 0.09489164 0.27400511 0.12984044 0.12495715 0.07307775
 0.06708389 0.20360096 0.08302483 0.02682518]
train_maes at epoch4: [0.54154466 0.23389064 0.32820609 0.25961148 0.2505825  0.19907845
 0.19744366 0.32479702 0.2077234  0.12263123]
test_loss at epoch4: 0.015259932552246338
test_mses at epoch4: [0.92000604 0.30450035 0.59121011 1.05299444 0.27095089 0.23740735
 0.33191332 0.32892373 0.24420602 0.22179519]
test_maes at epoch4: [0.76845868 0.34074675 0.63033216 0.77408449 0.2617474  0.41046383
 0.45136907 0.35228449 0.29840576 0.39237887]

train_loss at epoch5: 0.0026485034442962486
train_mses at epoch5: [0.38205551 0.07156957 0.09594704 0.09217931 0.05919354 0.05193757
 0.06154307 0.10556185 0.05096027 0.02415229]
train_maes at epoch5: [0.45821655 0.18940078 0.22753093 0.22344185 0.18187872 0.16117438
 0.18049098 0.25098787 0.16675518 0.11393152]
test_loss at epoch5: 0.01406146229264584
test_mses at epoch5: [0.90623088 0.28592992 0.70187487 1.00304256 0.24787588 0.10104964
 0.23829229 0.25294945 0.22087505 0.24653049]
test_maes at epoch5: [0.76775635 0.36025199 0.72300586 0.71851076 0.21655319 0.20929899
 0.31837809 0.26428694 0.2126342  0.42110836]

train_loss at epoch6: 0.001956326023061225
train_mses at epoch6: [0.30138081 0.05487249 0.06077702 0.07617813 0.0499644  0.04004497
 0.04342659 0.06554318 0.03915807 0.02214657]
train_maes at epoch6: [0.40960546 0.16343946 0.18464926 0.19294798 0.15853679 0.13894126
 0.14414817 0.1924853  0.14386336 0.10518034]
test_loss at epoch6: 0.015816441368549428
test_mses at epoch6: [0.84159558 0.278892   0.96502962 1.12373949 0.25351314 0.11815488
 0.2383329  0.25367065 0.24937225 0.25106437]
test_maes at epoch6: [0.74688624 0.3687474  0.8753614  0.77612491 0.22989904 0.23916701
 0.30877835 0.25791412 0.21774068 0.42948506]

train_loss at epoch7: 0.0016815784367475103
train_mses at epoch7: [0.27699991 0.03769977 0.08577483 0.04745037 0.03431556 0.03206245
 0.04020067 0.05226468 0.04077273 0.01640958]
train_maes at epoch7: [0.38175689 0.13982823 0.20727172 0.15653742 0.13059575 0.1313299
 0.1383124  0.16920074 0.13685081 0.09590216]
test_loss at epoch7: 0.01379930075812847
test_mses at epoch7: [0.82218836 0.26872823 0.66904406 0.92110422 0.34428584 0.10285378
 0.2491445  0.23630875 0.26924698 0.18639937]
test_maes at epoch7: [0.74150581 0.34137177 0.69600843 0.66000994 0.28156363 0.19838194
 0.34121277 0.23433475 0.20400299 0.3548695 ]

train_loss at epoch8: 0.0015482338819097965
train_mses at epoch8: [0.2413091  0.04004167 0.07023388 0.04787355 0.03936592 0.03483281
 0.03104818 0.03933729 0.03552581 0.01636559]
train_maes at epoch8: [0.37554045 0.14251301 0.19799232 0.15760468 0.1368716  0.13407207
 0.12980807 0.1483666  0.13755243 0.09407863]
test_loss at epoch8: 0.01219890869836858
test_mses at epoch8: [0.69347767 0.22888122 0.54410329 0.79631437 0.24718093 0.10747438
 0.23298419 0.23740763 0.2576529  0.21889012]
test_maes at epoch8: [0.65995728 0.32045018 0.61633135 0.58959089 0.20636719 0.19809538
 0.31430129 0.23570923 0.19932262 0.38735778]

train_loss at epoch9: 0.0012376244952704044
train_mses at epoch9: [0.17581986 0.0268634  0.04585425 0.03804443 0.02802997 0.03001898
 0.02975411 0.03892077 0.03530885 0.01204508]
train_maes at epoch9: [0.32382518 0.11728797 0.15461936 0.13971489 0.11891    0.11331798
 0.11975465 0.14150424 0.12872387 0.07911468]
test_loss at epoch9: 0.012643500124203398
test_mses at epoch9: [0.75935706 0.26152009 0.57045668 0.84013655 0.25667061 0.11603279
 0.22078628 0.26331544 0.22067162 0.22174889]
test_maes at epoch9: [0.70008296 0.32399718 0.63925844 0.61707768 0.20298626 0.24786443
 0.29767694 0.26460926 0.18728623 0.39405459]

train_loss at epoch10: 0.001301683576658685
train_mses at epoch10: [0.20984365 0.03428155 0.04398073 0.04699898 0.03044354 0.02766224
 0.02868214 0.04295572 0.03299893 0.01124176]
train_maes at epoch10: [0.34039217 0.12482876 0.14710726 0.14600755 0.11285705 0.11503277
 0.11315078 0.14535618 0.12348759 0.07827   ]
test_loss at epoch10: 0.01273206005546641
test_mses at epoch10: [0.83826394 0.21855977 0.72070909 0.73842426 0.26427701 0.11665067
 0.22424168 0.28412122 0.2481533  0.17500294]
test_maes at epoch10: [0.73541338 0.30014016 0.73924641 0.56996546 0.21457112 0.25634394
 0.28202495 0.29671285 0.19104711 0.33964349]

train_loss at epoch11: 0.0010997474827664964
train_mses at epoch11: [0.19240271 0.0332481  0.03590063 0.04404636 0.02182678 0.02846198
 0.02139559 0.03348581 0.02419792 0.00959541]
train_maes at epoch11: [0.3473291  0.12347267 0.13597235 0.15118451 0.10053172 0.10843604
 0.10581101 0.13388224 0.11809988 0.07435005]
test_loss at epoch11: 0.011601906000299657
test_mses at epoch11: [0.75099291 0.27933857 0.41379277 0.73253968 0.26094532 0.11620344
 0.21767148 0.2681152  0.25305543 0.17924902]
test_maes at epoch11: [0.69460112 0.29008558 0.51509254 0.53929724 0.21603335 0.22996627
 0.29321345 0.25290319 0.19251196 0.34729335]

train_loss at epoch12: 0.0010948332462539065
train_mses at epoch12: [0.18899766 0.03191175 0.04102664 0.02965489 0.02184386 0.02308494
 0.02760957 0.03707349 0.02797085 0.0101576 ]
train_maes at epoch12: [0.32157048 0.11874318 0.13836571 0.12649966 0.09744117 0.10549624
 0.11276323 0.13501496 0.12114554 0.07462144]
test_loss at epoch12: 0.012593589081092083
test_mses at epoch12: [0.7820668  0.19831009 0.63988373 0.79176501 0.27391755 0.12200941
 0.2089703  0.2475636  0.23183496 0.24518949]
test_maes at epoch12: [0.70039996 0.25017854 0.68512648 0.58940242 0.21094014 0.25908344
 0.25392711 0.25488244 0.18480416 0.41631946]

train_loss at epoch13: 0.0011101766073323312
train_mses at epoch13: [0.19659223 0.03419117 0.03566489 0.02943633 0.03053261 0.02967282
 0.02661052 0.03178453 0.02728678 0.00804277]
train_maes at epoch13: [0.32858304 0.11988866 0.13657499 0.11999042 0.11265701 0.1192238
 0.10839889 0.12487116 0.11180818 0.06644653]
test_loss at epoch13: 0.011462155729532242
test_mses at epoch13: [0.75489776 0.25096844 0.54202053 0.69087873 0.22542092 0.12996854
 0.19934185 0.22394035 0.2176228  0.2053583 ]
test_maes at epoch13: [0.68699325 0.33908591 0.61940864 0.54592154 0.21471746 0.2831802
 0.28648403 0.22588472 0.17789511 0.37025018]

train_loss at epoch14: 0.0009569423709143984
train_mses at epoch14: [0.1324427  0.02838926 0.02882653 0.03478582 0.02104158 0.02712524
 0.01897348 0.02554902 0.0265331  0.00849465]
train_maes at epoch14: [0.27049033 0.11365419 0.12193812 0.12249981 0.09739627 0.11561338
 0.09948794 0.11759412 0.1137082  0.06881802]
test_loss at epoch14: 0.012229020941130658
test_mses at epoch14: [0.83630089 0.20591293 0.62179527 0.76461017 0.22341622 0.12306224
 0.19621173 0.24236267 0.25054851 0.23511876]
test_maes at epoch14: [0.72515113 0.24975837 0.66978271 0.56929905 0.1883832  0.26752845
 0.27002298 0.24885356 0.18299418 0.39793626]

train_loss at epoch15: 0.0009138988529114013
train_mses at epoch15: [0.14117512 0.0268353  0.02522388 0.03289802 0.01895843 0.02442586
 0.02269207 0.02864952 0.02287619 0.00801025]
train_maes at epoch15: [0.28678523 0.11101047 0.11151369 0.12746656 0.09052548 0.1041212
 0.1007879  0.1233173  0.10559421 0.065577  ]
test_loss at epoch15: 0.01067630560632716
test_mses at epoch15: [0.73288376 0.24322923 0.44816822 0.58158738 0.23308676 0.0933895
 0.21204589 0.25485597 0.24743912 0.18477903]
test_maes at epoch15: [0.68220547 0.23550102 0.54885676 0.46122468 0.20529636 0.21155887
 0.308752   0.26801665 0.17713608 0.34686267]

train_loss at epoch16: 0.0009313975956211698
train_mses at epoch16: [0.13502455 0.03150215 0.02996455 0.02447204 0.02312633 0.0229087
 0.01861942 0.03361296 0.02174038 0.00790463]
train_maes at epoch16: [0.27102238 0.11104637 0.12501608 0.11123547 0.09994234 0.09959543
 0.095333   0.1310793  0.10169103 0.06516112]
test_loss at epoch16: 0.01173907882197106
test_mses at epoch16: [0.93169632 0.21751894 0.63469951 0.5892221  0.25441359 0.09330739
 0.20793524 0.29225743 0.23391079 0.21627989]
test_maes at epoch16: [0.76453953 0.23747536 0.67773757 0.46426256 0.21811065 0.21382184
 0.28768721 0.30240031 0.17436742 0.38211541]

train_loss at epoch17: 0.0007870149342937672
train_mses at epoch17: [0.13578183 0.02049259 0.02803624 0.02360699 0.02151683 0.01948459
 0.01823019 0.02472962 0.0182049  0.00701775]
train_maes at epoch17: [0.27692121 0.09500416 0.11693634 0.11078572 0.09063061 0.09229054
 0.08981746 0.1169189  0.09482505 0.06249834]
test_loss at epoch17: 0.010863753043590708
test_mses at epoch17: [0.80452092 0.21305451 0.55228207 0.6175008  0.22682277 0.07959795
 0.19542014 0.24911882 0.2264573  0.1767968 ]
test_maes at epoch17: [0.70693698 0.23481207 0.62424339 0.48732632 0.19970224 0.1867794
 0.28862605 0.25426074 0.18585173 0.34068608]

train_loss at epoch18: 0.00078368318128459
train_mses at epoch18: [0.12464218 0.02043255 0.02289519 0.02306488 0.02247165 0.02040101
 0.01648091 0.02539806 0.02171046 0.00669149]
train_maes at epoch18: [0.26689844 0.09915039 0.10892047 0.10694746 0.09070697 0.09392723
 0.09205032 0.11111222 0.1025684  0.06011401]
test_loss at epoch18: 0.010989117630301639
test_mses at epoch18: [0.85645135 0.2088597  0.58768828 0.63771649 0.20866585 0.10553062
 0.18560921 0.22875702 0.22776413 0.17174063]
test_maes at epoch18: [0.71725514 0.24318841 0.64791507 0.50142098 0.1846935  0.20973769
 0.25846578 0.24754814 0.16844678 0.33607047]

train_loss at epoch19: 0.0007847921962433673
train_mses at epoch19: [0.11510157 0.02365071 0.02040775 0.02805148 0.01935695 0.02192303
 0.01980552 0.01853247 0.0210718  0.006754  ]
train_maes at epoch19: [0.25832767 0.10272972 0.10411105 0.11061975 0.08682575 0.09813826
 0.09233008 0.09975523 0.09866289 0.0605913 ]
test_loss at epoch19: 0.010877057331356596
test_mses at epoch19: [0.87993564 0.18328803 0.62971764 0.55823529 0.23375173 0.08020122
 0.19064144 0.21953946 0.24544561 0.19821551]
test_maes at epoch19: [0.71309374 0.25205354 0.66577166 0.4745272  0.20000945 0.15353854
 0.23589572 0.21398499 0.17885803 0.36246904]

train_loss at epoch20: 0.0007561913632014965
train_mses at epoch20: [0.11039909 0.01901253 0.02076667 0.02040409 0.02347873 0.02360967
 0.02215883 0.0193379  0.02006261 0.00688208]
train_maes at epoch20: [0.25127435 0.09614043 0.09959629 0.09751287 0.09572465 0.10040053
 0.10161241 0.09837388 0.09886384 0.05998609]
test_loss at epoch20: 0.011238201493595508
test_mses at epoch20: [0.90629564 0.22358307 0.6141273  0.55805727 0.25918574 0.10180589
 0.19255999 0.22606422 0.25902445 0.1904511 ]
test_maes at epoch20: [0.75040867 0.27966815 0.66108305 0.45634866 0.20432105 0.19309054
 0.22005929 0.2281302  0.18038972 0.35856216]

train_loss at epoch21: 0.0007189416623813041
train_mses at epoch21: [0.0994344  0.01849268 0.02207075 0.02247415 0.02047113 0.01589297
 0.02161763 0.01931007 0.02070625 0.00595103]
train_maes at epoch21: [0.24314972 0.09260489 0.10174357 0.10518629 0.09128731 0.08571068
 0.09759826 0.1002225  0.09781055 0.05536885]
test_loss at epoch21: 0.010740113131543423
test_mses at epoch21: [0.97509543 0.20466206 0.59598097 0.53533671 0.21143186 0.09725586
 0.17867879 0.23818728 0.25190846 0.18273848]
test_maes at epoch21: [0.76005015 0.29857682 0.64243036 0.44369338 0.17764767 0.21421757
 0.22973081 0.25113527 0.17270443 0.3500412 ]

train_loss at epoch22: 0.0006702893928803027
train_mses at epoch22: [0.10374457 0.01982925 0.02124255 0.01865882 0.01938726 0.01681727
 0.01995415 0.01814398 0.01503128 0.00579808]
train_maes at epoch22: [0.24028769 0.08712882 0.09977239 0.09370181 0.09013598 0.08749676
 0.08802715 0.09619383 0.08567483 0.05534699]
test_loss at epoch22: 0.010460512133988928
test_mses at epoch22: [0.88961657 0.20268608 0.54795311 0.56506964 0.22095484 0.07740409
 0.19086803 0.20917433 0.23908348 0.18082398]
test_maes at epoch22: [0.71602074 0.28132942 0.60552501 0.46457076 0.19589645 0.16805227
 0.26386911 0.20456705 0.17001678 0.34677973]

train_loss at epoch23: 0.0007292921357649438
train_mses at epoch23: [0.11693866 0.01879038 0.02176298 0.02394142 0.01861398 0.02200894
 0.01671515 0.02133059 0.01625918 0.00580875]
train_maes at epoch23: [0.24548133 0.08817456 0.09914018 0.1046524  0.08366711 0.09179643
 0.09107633 0.10560362 0.09014387 0.05573162]
test_loss at epoch23: 0.011373587095357003
test_mses at epoch23: [0.94801922 0.18685557 0.70675349 0.60007509 0.23116755 0.08557212
 0.1742957  0.22152225 0.22602171 0.2163704 ]
test_maes at epoch23: [0.74068139 0.23187179 0.71184861 0.48963885 0.21417917 0.20465747
 0.23898982 0.23396658 0.17058397 0.38705333]

train_loss at epoch24: 0.0006957248844047811
train_mses at epoch24: [0.09427958 0.02083332 0.02459753 0.0267054  0.01767042 0.01818945
 0.01633916 0.01656896 0.01671883 0.00482005]
train_maes at epoch24: [0.23413681 0.09262289 0.11244838 0.11223014 0.08754197 0.08653076
 0.08832245 0.09467523 0.08871784 0.05100914]
test_loss at epoch24: 0.00940068809989285
test_mses at epoch24: [0.80109759 0.20557293 0.48489227 0.46429046 0.20660496 0.08083744
 0.17722732 0.19637952 0.21470009 0.15343906]
test_maes at epoch24: [0.68281114 0.24489052 0.56452079 0.37926633 0.19788233 0.18124087
 0.28631153 0.20384429 0.16307697 0.31390598]

train_loss at epoch25: 0.0007057592213312362
train_mses at epoch25: [0.10569911 0.02222976 0.0296224  0.02267107 0.01140815 0.01655388
 0.0168838  0.01712866 0.02144473 0.00593518]
train_maes at epoch25: [0.24846503 0.09430924 0.11034481 0.10060621 0.07302191 0.08301917
 0.08806223 0.09635803 0.09562107 0.05476392]
test_loss at epoch25: 0.01051986314990419
test_mses at epoch25: [0.88602768 0.19171467 0.6571704  0.50647331 0.21734789 0.07944632
 0.17533089 0.2082652  0.26281449 0.15140558]
test_maes at epoch25: [0.7183014  0.24033307 0.68882656 0.43291264 0.19331635 0.17055638
 0.26284796 0.19934594 0.17306702 0.31629154]

train_loss at epoch26: 0.0007663008499335735
train_mses at epoch26: [0.12311737 0.0164594  0.03879364 0.02054391 0.01469157 0.02341733
 0.01856378 0.01684606 0.02212879 0.00518955]
train_maes at epoch26: [0.26512861 0.08726437 0.1314632  0.09473331 0.07814602 0.09602676
 0.08674123 0.09221096 0.10368992 0.05179122]
test_loss at epoch26: 0.009887030578040062
test_mses at epoch26: [0.75056038 0.20893371 0.37978544 0.47366807 0.24196304 0.10327892
 0.19868332 0.27538607 0.27322925 0.16052193]
test_maes at epoch26: [0.66996419 0.26650372 0.48757538 0.39291523 0.21334713 0.21891578
 0.24193393 0.26428537 0.19167549 0.32935806]

train_loss at epoch27: 0.0007015367593378463
train_mses at epoch27: [0.0984607  0.02075703 0.02142596 0.02146206 0.01453706 0.01757655
 0.01764981 0.02662578 0.01716059 0.00459765]
train_maes at epoch27: [0.22946125 0.08880455 0.10069899 0.10036592 0.08028991 0.09032137
 0.09358681 0.10789223 0.09299046 0.05002097]
test_loss at epoch27: 0.010243141718525835
test_mses at epoch27: [0.94914322 0.20888419 0.51945498 0.50322596 0.2079208  0.09652001
 0.18021045 0.236652   0.26360883 0.16190464]
test_maes at epoch27: [0.74599047 0.28846311 0.5877884  0.40609198 0.17942751 0.21770163
 0.24433383 0.23591194 0.18271927 0.32586368]

train_loss at epoch28: 0.0006350086704689137
train_mses at epoch28: [0.09819691 0.01535025 0.01989285 0.01971016 0.01625423 0.01807111
 0.01790323 0.01748867 0.0168395  0.00471599]
train_maes at epoch28: [0.23905849 0.0853674  0.09544181 0.09273126 0.07999675 0.0831727
 0.08664756 0.08691223 0.08414991 0.05047691]
test_loss at epoch28: 0.009210202624981708
test_mses at epoch28: [0.71082783 0.17962568 0.35985514 0.49417345 0.20098944 0.1133524
 0.18022139 0.22712439 0.25357757 0.1429133 ]
test_maes at epoch28: [0.65046354 0.2316499  0.46941962 0.40089024 0.17363787 0.23230589
 0.2226537  0.20702738 0.17145267 0.30378702]

train_loss at epoch29: 0.000725252619211344
train_mses at epoch29: [0.11357903 0.0140336  0.02560694 0.02653514 0.02165058 0.01571824
 0.01862274 0.02022699 0.01852738 0.00420212]
train_maes at epoch29: [0.24221999 0.08613408 0.10250247 0.09994112 0.08707833 0.08538814
 0.08212254 0.09760616 0.09216632 0.0468327 ]
test_loss at epoch29: 0.010016912190203972
test_mses at epoch29: [1.03545454 0.17948469 0.68142957 0.42273211 0.21051259 0.08259388
 0.15625319 0.19216563 0.24462219 0.14758678]
test_maes at epoch29: [0.7885644  0.25290695 0.70760209 0.36585336 0.18845314 0.16089918
 0.23486017 0.20183156 0.17546451 0.31389255]

train_loss at epoch30: 0.0007139803564294856
train_mses at epoch30: [0.11206596 0.02058692 0.02302738 0.02504279 0.02515008 0.01130634
 0.01757172 0.01536139 0.02105871 0.00501066]
train_maes at epoch30: [0.24756088 0.09458479 0.10885384 0.10451328 0.09248666 0.07360371
 0.08742918 0.09038748 0.09099894 0.05096492]
test_loss at epoch30: 0.009167527657081473
test_mses at epoch30: [0.72221969 0.18305267 0.39553345 0.48499312 0.19892752 0.09386759
 0.18221648 0.21702806 0.26017222 0.12274477]
test_maes at epoch30: [0.65932719 0.1967499  0.49819711 0.39509101 0.17355978 0.2068675
 0.26389844 0.19162289 0.17029154 0.27703118]

train_loss at epoch31: 0.0006179686873517138
train_mses at epoch31: [0.08652222 0.01390283 0.02184409 0.01611925 0.01431541 0.02045011
 0.01250046 0.01820827 0.02026591 0.00404711]
train_maes at epoch31: [0.21517829 0.08297297 0.10064788 0.08849714 0.08155062 0.08467494
 0.07599938 0.0926677  0.09217395 0.04577462]
test_loss at epoch31: 0.010175117763116005
test_mses at epoch31: [0.85307771 0.20052218 0.6183748  0.45232467 0.23399762 0.09416341
 0.15518702 0.21709968 0.24471782 0.15407128]
test_maes at epoch31: [0.71024083 0.19953441 0.66219398 0.38111518 0.21784043 0.21209036
 0.24139146 0.22177003 0.18274904 0.3212542 ]

train_loss at epoch32: 0.0006685780242402503
train_mses at epoch32: [0.09640602 0.01723367 0.02158492 0.02150914 0.0183619  0.01719499
 0.01717727 0.01619799 0.01891726 0.00441176]
train_maes at epoch32: [0.23043752 0.08417601 0.10165462 0.09600571 0.08524495 0.07932146
 0.0836923  0.0879107  0.08690691 0.0477872 ]
test_loss at epoch32: 0.009872674359444607
test_mses at epoch32: [0.80550157 0.19019524 0.54745879 0.47882854 0.24623394 0.09852527
 0.1801754  0.21357151 0.22766428 0.12145131]
test_maes at epoch32: [0.69690587 0.22003218 0.61751965 0.40227684 0.21233568 0.21826401
 0.22167655 0.19564245 0.17600853 0.27776922]

train_loss at epoch33: 0.0006097490542588082
train_mses at epoch33: [0.08486056 0.01868571 0.01701246 0.01719039 0.01394314 0.0160172
 0.01543819 0.01612806 0.0202399  0.00482854]
train_maes at epoch33: [0.21499384 0.08233541 0.08698398 0.0853097  0.07532505 0.08098874
 0.08014451 0.08764496 0.08950343 0.05001215]
test_loss at epoch33: 0.009495173208415508
test_mses at epoch33: [0.8321812  0.20589341 0.45154246 0.46364336 0.2222367  0.07856315
 0.1696568  0.24654393 0.23079223 0.14056388]
test_maes at epoch33: [0.69995604 0.23822035 0.53991583 0.384666   0.19244732 0.17722196
 0.2088696  0.22506154 0.1619487  0.30634821]

train_loss at epoch34: 0.0005433470307988055
train_mses at epoch34: [0.07392372 0.0144172  0.01523927 0.01636244 0.01304536 0.01373432
 0.01643813 0.01753986 0.01464906 0.00456363]
train_maes at epoch34: [0.20779532 0.07968831 0.08459164 0.0825947  0.06861655 0.07767084
 0.07894531 0.08696638 0.08176204 0.04707367]
test_loss at epoch34: 0.009878196682226151
test_mses at epoch34: [0.92971902 0.17228879 0.5367356  0.4217597  0.25264866 0.09435505
 0.17177263 0.23087187 0.27994345 0.13646275]
test_maes at epoch34: [0.74373068 0.21805685 0.60292922 0.36872588 0.19445393 0.20999663
 0.24896701 0.22143301 0.18155927 0.29765826]

train_loss at epoch35: 0.0005420292153003368
train_mses at epoch35: [0.0844096  0.01323559 0.0161993  0.01693679 0.01285075 0.0163308
 0.01345375 0.01682702 0.01453251 0.00353569]
train_maes at epoch35: [0.21495243 0.07594795 0.08794981 0.08808623 0.07756734 0.08099559
 0.07817427 0.08563012 0.08185748 0.04470071]
test_loss at epoch35: 0.009428251209728261
test_mses at epoch35: [0.86302388 0.18138586 0.50453319 0.43407258 0.21766413 0.08463715
 0.16405355 0.20779719 0.23985889 0.15736887]
test_maes at epoch35: [0.70899601 0.23963242 0.5826879  0.38073991 0.19311731 0.18184553
 0.20210534 0.1923401  0.16941681 0.32178156]

train_loss at epoch36: 0.0005881356273559813
train_mses at epoch36: [0.08340776 0.01388779 0.01515499 0.01701493 0.01536258 0.01632001
 0.01448593 0.01732606 0.02097496 0.00419369]
train_maes at epoch36: [0.21371225 0.0762601  0.08354703 0.08652084 0.07382549 0.08000279
 0.07908505 0.09011418 0.08814469 0.04741565]
test_loss at epoch36: 0.009382753327805944
test_mses at epoch36: [0.86326625 0.17455869 0.48859353 0.41217312 0.21552134 0.10733946
 0.16845739 0.22391543 0.25378277 0.13926535]
test_maes at epoch36: [0.71749009 0.24477745 0.56890526 0.3431912  0.17516709 0.20913738
 0.19774019 0.19651979 0.16394398 0.29765775]

train_loss at epoch37: 0.0005777588787865131
train_mses at epoch37: [0.07916774 0.02117976 0.01442226 0.0208619  0.01749118 0.01376781
 0.01138434 0.01537932 0.01511058 0.00436872]
train_maes at epoch37: [0.21322254 0.08290683 0.08363945 0.09966758 0.07675729 0.07975283
 0.06985035 0.08787759 0.07620329 0.04763821]
test_loss at epoch37: 0.008256417663490518
test_mses at epoch37: [0.93763919 0.17607615 0.45594369 0.34463698 0.18554245 0.07323471
 0.14609384 0.1850844  0.19798619 0.13368566]
test_maes at epoch37: [0.73951316 0.24329382 0.54387902 0.29764379 0.16174032 0.16426911
 0.21217568 0.18955261 0.13913239 0.29042459]

train_loss at epoch38: 0.0005961574673494126
train_mses at epoch38: [0.1008828  0.01729348 0.01816082 0.02152724 0.01506535 0.01577844
 0.01180285 0.01354465 0.01935757 0.00378089]
train_maes at epoch38: [0.24435218 0.08751816 0.09490774 0.10120148 0.07554011 0.07846983
 0.07066834 0.07839518 0.08384422 0.04573476]
test_loss at epoch38: 0.009156951720410205
test_mses at epoch38: [0.83569382 0.17878356 0.41217826 0.43719801 0.21161125 0.08706738
 0.17585924 0.22392913 0.26187049 0.14217745]
test_maes at epoch38: [0.69018604 0.23290358 0.50348849 0.35775529 0.19194873 0.17860649
 0.18776416 0.20625681 0.17007096 0.30274869]

train_loss at epoch39: 0.0005747734053813397
train_mses at epoch39: [0.08376726 0.01432049 0.01946227 0.01926869 0.01631056 0.01357995
 0.01244638 0.01177837 0.02293199 0.00338775]
train_maes at epoch39: [0.21056514 0.07609942 0.0931871  0.08861414 0.07543359 0.07599087
 0.07211086 0.07516189 0.08818656 0.04226095]
test_loss at epoch39: 0.009543136126817542
test_mses at epoch39: [0.84871301 0.18603391 0.45152792 0.44134535 0.2238474  0.09624032
 0.17888279 0.25214858 0.25794033 0.13496066]
test_maes at epoch39: [0.71893732 0.2350661  0.54988612 0.35393151 0.16667464 0.19651246
 0.20110376 0.21399792 0.16401729 0.29598459]

train_loss at epoch40: 0.0005122108860535824
train_mses at epoch40: [0.06459041 0.0129228  0.01681531 0.01249196 0.01359262 0.01236233
 0.01644195 0.01433716 0.01679174 0.00409368]
train_maes at epoch40: [0.19181336 0.07483318 0.08930546 0.07408723 0.07222484 0.07192477
 0.08378676 0.08636359 0.08183708 0.04460858]
test_loss at epoch40: 0.009456389940323983
test_mses at epoch40: [0.89252031 0.18674906 0.46321955 0.43168085 0.21984105 0.09074465
 0.1643643  0.25691826 0.2578264  0.12690315]
test_maes at epoch40: [0.71788198 0.26463431 0.55092533 0.34779392 0.18940672 0.18861907
 0.19731938 0.2392742  0.16350045 0.28375415]

train_loss at epoch41: 0.0005022977696771318
train_mses at epoch41: [0.06393194 0.01799653 0.01741777 0.01409782 0.0103118  0.0131435
 0.00962379 0.01704215 0.01259773 0.00418677]
train_maes at epoch41: [0.1904336  0.08720155 0.0849655  0.08016221 0.06494097 0.07438494
 0.06588231 0.0867407  0.0753423  0.04688781]
test_loss at epoch41: 0.008429067891011846
test_mses at epoch41: [0.77019284 0.17229326 0.34704229 0.40783191 0.20586861 0.08857247
 0.14550995 0.20932626 0.21932678 0.16288051]
test_maes at epoch41: [0.6550319  0.26678929 0.45334859 0.35022695 0.17464332 0.19538511
 0.20560559 0.18438561 0.15733445 0.33135388]

train_loss at epoch42: 0.0005140886384438961
train_mses at epoch42: [0.06990446 0.02177027 0.01543066 0.01341956 0.01075317 0.01490128
 0.01010417 0.01450031 0.01480762 0.00399574]
train_maes at epoch42: [0.19140608 0.09282448 0.0838334  0.07921659 0.07082641 0.07716866
 0.06637092 0.08565907 0.07534115 0.04458736]
test_loss at epoch42: 0.008452254262297077
test_mses at epoch42: [0.83242986 0.16040118 0.44910403 0.37884496 0.18644624 0.08575899
 0.15196318 0.19486448 0.21221284 0.13814757]
test_maes at epoch42: [0.67390955 0.20864628 0.54064881 0.32436349 0.17173608 0.19468945
 0.19120849 0.18179587 0.15285228 0.29519941]

train_loss at epoch43: 0.0005016113770134905
train_mses at epoch43: [0.07390778 0.01917697 0.01248185 0.01570897 0.0117554  0.01230397
 0.0149221  0.01619187 0.01059044 0.00350844]
train_maes at epoch43: [0.19983309 0.08094086 0.07494882 0.0815664  0.06821693 0.07317951
 0.07908999 0.08981622 0.06730064 0.04125796]
test_loss at epoch43: 0.00865278839668695
test_mses at epoch43: [0.78454412 0.17391809 0.4087466  0.40578269 0.19744469 0.0897058
 0.15416396 0.20494405 0.23534119 0.14145371]
test_maes at epoch43: [0.67156798 0.23130374 0.5102862  0.34093465 0.15733476 0.18392616
 0.19412922 0.17983406 0.16353465 0.30243298]

train_loss at epoch44: 0.000553144023139426
train_mses at epoch44: [0.08740336 0.02573047 0.01480944 0.01487731 0.01245553 0.01292598
 0.01282586 0.01681922 0.01172971 0.00291798]
train_maes at epoch44: [0.2143638  0.09056722 0.08383722 0.08003156 0.06731782 0.0742466
 0.07095292 0.0836196  0.07098551 0.03930778]
test_loss at epoch44: 0.008623679081334712
test_mses at epoch44: [0.75223202 0.21235798 0.44148253 0.38724237 0.20642295 0.07070594
 0.15821025 0.18998376 0.21689089 0.12165789]
test_maes at epoch44: [0.66982261 0.19540294 0.54099224 0.31825866 0.16213388 0.15714836
 0.19739207 0.17580801 0.14996233 0.27852305]

train_loss at epoch45: 0.0005034371933087389
train_mses at epoch45: [0.08455307 0.01988503 0.01801814 0.015776   0.00996994 0.01054159
 0.01261361 0.01232692 0.01260789 0.00302275]
train_maes at epoch45: [0.22028146 0.09182159 0.08524802 0.08022437 0.0673497  0.06863555
 0.0722323  0.0772443  0.07065391 0.03686405]
test_loss at epoch45: 0.008121524648146427
test_mses at epoch45: [0.67750557 0.19249511 0.36401385 0.34661708 0.1972142  0.10172876
 0.14930809 0.21027251 0.21646526 0.11348315]
test_maes at epoch45: [0.61914337 0.17386622 0.46622805 0.2865553  0.16438367 0.22943894
 0.19499243 0.18305835 0.14929081 0.26348315]

train_loss at epoch46: 0.0004744934631471938
train_mses at epoch46: [0.06743689 0.01190542 0.01430037 0.01642296 0.00994968 0.01685178
 0.01318184 0.01104986 0.01165054 0.00377499]
train_maes at epoch46: [0.19419007 0.0723326  0.07734991 0.08754674 0.06441054 0.08032741
 0.07230904 0.07536999 0.06940256 0.04453177]
test_loss at epoch46: 0.008671008629050661
test_mses at epoch46: [0.7598722  0.16013316 0.42211171 0.36742834 0.22611808 0.09058375
 0.17809345 0.20879556 0.24701784 0.12180137]
test_maes at epoch46: [0.66988266 0.19308343 0.52252583 0.30415492 0.17773767 0.18983167
 0.18516427 0.17419318 0.15742274 0.27807798]

train_loss at epoch47: 0.0005137282344413565
train_mses at epoch47: [0.0620393  0.01180794 0.01454149 0.01633686 0.01021616 0.01465516
 0.0157826  0.01581446 0.01687222 0.00283153]
train_maes at epoch47: [0.18707285 0.06853619 0.07927727 0.08354968 0.06406823 0.0745391
 0.07786153 0.0813778  0.07421315 0.0390567 ]
test_loss at epoch47: 0.009254543832007875
test_mses at epoch47: [0.87945666 0.19113974 0.46463553 0.43143534 0.21363378 0.09767195
 0.14261531 0.21397851 0.27626646 0.11651014]
test_maes at epoch47: [0.73043498 0.238538   0.55937836 0.34823271 0.16126152 0.19001302
 0.1832236  0.1762029  0.17454596 0.27220234]

train_loss at epoch48: 0.0005091785195660084
train_mses at epoch48: [0.0724792  0.01515493 0.0150788  0.01995896 0.00982099 0.01479263
 0.0105066  0.01168461 0.01676809 0.00266677]
train_maes at epoch48: [0.19924225 0.07625838 0.08120535 0.09160132 0.06662335 0.07260631
 0.06904263 0.07397937 0.07836979 0.03773248]
test_loss at epoch48: 0.008346729355051796
test_mses at epoch48: [0.82635682 0.15470101 0.39178091 0.33578993 0.22640721 0.08921844
 0.15766788 0.21101903 0.2565436  0.1144804 ]
test_maes at epoch48: [0.68638024 0.2033465  0.4971292  0.26513416 0.18609796 0.18601252
 0.22200426 0.18044099 0.16636889 0.26756911]

train_loss at epoch49: 0.0004479243935264171
train_mses at epoch49: [0.0676679  0.01097045 0.01296659 0.01536401 0.01177868 0.01214748
 0.01171816 0.0121143  0.01373111 0.00284729]
train_maes at epoch49: [0.18913049 0.06759867 0.07688611 0.07854297 0.06754642 0.06955176
 0.06943324 0.07611456 0.07163527 0.03821972]
test_loss at epoch49: 0.008726291671553825
test_mses at epoch49: [0.71537631 0.18964844 0.39081155 0.40517712 0.20770589 0.08648273
 0.1682063  0.23408949 0.24242886 0.11181538]
test_maes at epoch49: [0.64782413 0.20496586 0.49926694 0.332412   0.16361903 0.16869627
 0.20156366 0.1889353  0.15637835 0.2680402 ]

