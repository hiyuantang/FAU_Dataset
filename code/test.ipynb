{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9452, 0.0779, 0.3747, 0.1388, 0.0467, 0.2784, 0.0078, 0.1205, 0.0840,\n",
      "        0.0761], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('./code')\n",
    "import torch\n",
    "import cv2\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import argparse\n",
    "from vgg_face import *\n",
    "import matplotlib.pyplot as plt\n",
    "from facegenDataset import *\n",
    "from FAUDataset import *\n",
    "plt.switch_backend('agg')\n",
    "\n",
    "def set_parameter_requires_grad(model, feature_extracting):\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False if feature_extracting else True\n",
    "\n",
    "def test_model(model, crop_frame, device):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        crop_frame = cv2.cvtColor(crop_frame, cv2.COLOR_BGR2RGB)\n",
    "        crop_frame = cv2.resize(crop_frame, (224, 224))\n",
    "        crop_frame = crop_frame / 255.0\n",
    "        crop_frame = np.transpose(crop_frame, (2, 0, 1))\n",
    "        crop_frame = np.expand_dims(crop_frame, axis=0)  # add batch dimension\n",
    "        crop_frame = torch.from_numpy(crop_frame).float().to(device)\n",
    "        output = model(crop_frame)\n",
    "        output = output * torch.FloatTensor([16] + [5]*9).to(device)\n",
    "    return output\n",
    "\n",
    "num_classes = 10\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "model = VGG_16().to(device)\n",
    "set_parameter_requires_grad(model, feature_extracting=False)\n",
    "num_ftrs = model.fc8.in_features\n",
    "model.fc8 = nn.Linear(num_ftrs, num_classes).to(device)\n",
    "model.load_state_dict(torch.load('/Volumes/Yuan-T7/FAU_models/checkpoint_epoch_init.pth', map_location=device))\n",
    "\n",
    "crop_frame = cv2.imread('/Volumes/Yuan-T7/Datasets/facegen/images/African/a/4a_4.10.png')\n",
    "output = test_model(model, crop_frame, device)\n",
    "output = output.flatten()\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "a Tensor with 10 elements cannot be converted to Scalar",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m output_text \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m|PSPI \u001b[39m\u001b[39m{:.2f}\u001b[39;00m\u001b[39m |au4 \u001b[39m\u001b[39m{:.2f}\u001b[39;00m\u001b[39m |au6 \u001b[39m\u001b[39m{:.2f}\u001b[39;00m\u001b[39m |au7 \u001b[39m\u001b[39m{:.2f}\u001b[39;00m\u001b[39m |au10 \u001b[39m\u001b[39m{:.2f}\u001b[39;00m\u001b[39m |au12 \u001b[39m\u001b[39m{:.2f}\u001b[39;00m\u001b[39m |au20 \u001b[39m\u001b[39m{:.2f}\u001b[39;00m\u001b[39m |au25 \u001b[39m\u001b[39m{:.2f}\u001b[39;00m\u001b[39m |au26 \u001b[39m\u001b[39m{:.2f}\u001b[39;00m\u001b[39m |au43 \u001b[39m\u001b[39m{:.2f}\u001b[39;00m\u001b[39m|\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(output[\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39;49mitem(), output[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mitem(), output[\u001b[39m2\u001b[39m]\u001b[39m.\u001b[39mitem(), output[\u001b[39m3\u001b[39m]\u001b[39m.\u001b[39mitem(), output[\u001b[39m4\u001b[39m]\u001b[39m.\u001b[39mitem(), output[\u001b[39m5\u001b[39m]\u001b[39m.\u001b[39mitem(), output[\u001b[39m6\u001b[39m]\u001b[39m.\u001b[39mitem(), output[\u001b[39m7\u001b[39m]\u001b[39m.\u001b[39mitem(), output[\u001b[39m8\u001b[39m]\u001b[39m.\u001b[39mitem(), output[\u001b[39m9\u001b[39m]\u001b[39m.\u001b[39mitem())\n",
      "\u001b[0;31mRuntimeError\u001b[0m: a Tensor with 10 elements cannot be converted to Scalar"
     ]
    }
   ],
   "source": [
    "output_text = '|PSPI {:.2f} |au4 {:.2f} |au6 {:.2f} |au7 {:.2f} |au10 {:.2f} |au12 {:.2f} |au20 {:.2f} |au25 {:.2f} |au26 {:.2f} |au43 {:.2f}|'.format(output[0].item(), output[1].item(), output[2].item(), output[3].item(), output[4].item(), output[5].item(), output[6].item(), output[7].item(), output[8].item(), output[9].item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
