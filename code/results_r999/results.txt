(torch-gpu) yuantang@Yuans-MBP code % python train_facegen_alt.py --seed 999 --dataset_root /Volumes/Yuan-T7/Datasets/face_gen --resume /Volumes/Yuan-T7/FAU_models/checkpoint_epoch_init.pth
PyTorch Version:  2.0.0
Torchvision Version:  0.15.1
Initializing Datasets and Dataloaders...
Train Sets: ['aw' 'e']
Test Sets: ['a' 'eb']
Total Number of Train Sets: 92
Total Number of Test Sets: 92

train_loss at epoch0: 0.03928275989449542
train_mses at epoch0: [3.90855696 1.15032784 1.152095   1.13332101 1.17207981 0.96677485
 1.15716649 1.16021676 1.13986327 0.02521984]
train_maes at epoch0: [1.54125904 0.39677376 0.5757094  0.41061851 0.3716814  0.54483205
 0.36287268 0.40287354 0.38663707 0.07766786]
test_loss at epoch0: 0.03869532631791156
test_mses at epoch0: [3.43733282 1.11208935 1.1572302  1.0918869  1.13981793 1.03062461
 1.12183693 1.12912418 1.12315124 0.0226068 ]
test_maes at epoch0: [1.5331772  0.43677311 0.70812653 0.50900759 0.42688589 0.67238789
 0.41480525 0.47595844 0.41059254 0.0351486 ]

train_loss at epoch1: 0.037803022109943886
train_mses at epoch1: [3.19405106 1.10302814 1.13157755 1.03734295 1.14608309 0.93402091
 1.11233922 1.119929   1.1164481  0.02359716]
train_maes at epoch1: [1.50741352 0.4583699  0.68273171 0.54047591 0.43228313 0.59815809
 0.40974542 0.47207471 0.4476816  0.05461969]
test_loss at epoch1: 0.037669030220612236
test_mses at epoch1: [3.14115366 1.07021525 1.16035795 1.07388675 1.10764636 0.99868846
 1.08313884 1.10946418 1.08174857 0.02320594]
test_maes at epoch1: [1.56358036 0.51181322 0.75282949 0.6192244  0.51186164 0.64062559
 0.4721406  0.54919528 0.47772067 0.04087543]

train_loss at epoch2: 0.036149624249209526
train_mses at epoch2: [2.90357877 1.02361196 1.09986345 1.00811568 1.0999362  0.87277622
 1.08011583 1.07517109 1.07968017 0.02237378]
train_maes at epoch2: [1.51330858 0.52110857 0.69854722 0.63361742 0.4932761  0.54994291
 0.45211522 0.52010135 0.52259989 0.05816655]
test_loss at epoch2: 0.03666461939397066
test_mses at epoch2: [2.99178986 1.02171405 1.14368409 1.05629271 1.10320915 0.91927239
 1.06049202 1.10443111 1.05499315 0.02314912]
test_maes at epoch2: [1.53950453 0.54387978 0.72647913 0.62836368 0.56439906 0.56319145
 0.50829061 0.58495998 0.54775559 0.04289372]

train_loss at epoch3: 0.034659361061842545
train_mses at epoch3: [2.80196129 0.98191147 1.08355765 0.93316594 1.10529847 0.70799872
 1.01450437 1.0463425  1.04795338 0.0237955 ]
train_maes at epoch3: [1.48670097 0.51528851 0.67035168 0.59677861 0.52546821 0.44261793
 0.49191569 0.55343412 0.57655161 0.0628413 ]
test_loss at epoch3: 0.03565839451292287
test_mses at epoch3: [2.78999505 0.97340994 1.12649831 1.024529   1.10269653 0.82924133
 1.04310849 1.10579068 1.04384847 0.02337519]
test_maes at epoch3: [1.48231841 0.5045486  0.68333628 0.55504858 0.5880319  0.49247381
 0.55201137 0.61591288 0.58070494 0.04624984]

train_loss at epoch4: 0.03278913122156392
train_mses at epoch4: [2.59683738 0.96177086 1.05731014 0.88402607 1.08553623 0.61989231
 0.94835189 1.03574609 1.00893478 0.02183966]
train_maes at epoch4: [1.42788097 0.503073   0.64238102 0.56320306 0.55688193 0.38656177
 0.52870064 0.57998795 0.60486067 0.07063176]
test_loss at epoch4: 0.03455133541770603
test_mses at epoch4: [2.77767217 0.90068699 1.16054608 0.97104633 1.09796145 0.7326507
 1.01511632 1.09570714 1.01430192 0.02449275]
test_maes at epoch4: [1.50439063 0.54222814 0.75801856 0.54468127 0.63165142 0.5209718
 0.5948599  0.63367234 0.59911422 0.05720138]

train_loss at epoch5: 0.030663317312364994
train_mses at epoch5: [2.63802721 0.86876548 1.0456575  0.78190939 1.06082046 0.45028698
 0.94575694 0.96868493 1.00268193 0.02592578]
train_maes at epoch5: [1.45914294 0.51741033 0.66868459 0.54594823 0.57042215 0.38784183
 0.56463568 0.56030354 0.61271649 0.09318077]
test_loss at epoch5: 0.0332425022902696
test_mses at epoch5: [2.72323602 0.82824267 1.19302323 0.88740993 1.08419341 0.66724237
 0.97681373 1.07049861 0.97503758 0.02603771]
test_maes at epoch5: [1.4834986  0.53968366 0.81040478 0.49434985 0.63352507 0.5700625
 0.61530625 0.6168563  0.56531585 0.06727874]

train_loss at epoch6: 0.028085865404294884
train_mses at epoch6: [2.34587183 0.70696697 0.98407703 0.69250558 1.05788123 0.33514438
 0.8749448  0.95203321 0.87729297 0.02790197]
train_maes at epoch6: [1.37794215 0.45248233 0.62124997 0.50292656 0.54564889 0.35699496
 0.54294065 0.51709687 0.55681231 0.10320966]
test_loss at epoch6: 0.03107876622158548
test_mses at epoch6: [2.30820426 0.7387456  1.14498963 0.7991833  1.05894562 0.57171391
 0.91912039 1.04224496 0.92933875 0.02796199]
test_maes at epoch6: [1.34361787 0.47333156 0.74803292 0.42378188 0.60568647 0.52680546
 0.58571302 0.57845598 0.50133288 0.07792895]

train_loss at epoch7: 0.025169972492300945
train_mses at epoch7: [2.0304367  0.65337927 0.92081541 0.51139477 1.01777876 0.28402265
 0.78991762 0.87079074 0.79885064 0.03805588]
train_maes at epoch7: [1.24323061 0.42187378 0.55812661 0.43750857 0.52010586 0.35532384
 0.52144236 0.45658886 0.47584436 0.12308138]
test_loss at epoch7: 0.02883331542429717
test_mses at epoch7: [2.11590046 0.6507224  1.13825456 0.67653189 1.02495675 0.51913399
 0.8174892  1.00170777 0.85453558 0.03206279]
test_maes at epoch7: [1.26070293 0.45041792 0.74034038 0.40104757 0.58113009 0.5414699
 0.49525071 0.60584202 0.46188641 0.0972243 ]

train_loss at epoch8: 0.023399070553157642
train_mses at epoch8: [1.87162217 0.50960379 0.92012467 0.39763978 0.97218836 0.28821029
 0.68552132 0.82095845 0.7467529  0.04590137]
train_maes at epoch8: [1.09641267 0.42407809 0.57098282 0.39377155 0.49298128 0.40238476
 0.4711915  0.49525515 0.47752509 0.14929505]
test_loss at epoch8: 0.026281110618425453
test_mses at epoch8: [1.70332465 0.56936671 1.05042624 0.59685279 0.98863745 0.4201145
 0.74002481 0.97730218 0.7733545  0.0303494 ]
test_maes at epoch8: [1.05529396 0.43710122 0.63092589 0.40180883 0.58737516 0.44561306
 0.4412339  0.65102598 0.47637336 0.0897141 ]

train_loss at epoch9: 0.01999561229477758
train_mses at epoch9: [1.66317107 0.45874806 0.78601233 0.35331713 0.86130882 0.32434031
 0.57146992 0.69246235 0.55051957 0.05961823]
train_maes at epoch9: [0.99479759 0.43862244 0.55314306 0.39532763 0.47371618 0.45150828
 0.44975672 0.4663846  0.41701754 0.17419786]
test_loss at epoch9: 0.02446934321652288
test_mses at epoch9: [1.57513137 0.51035168 1.00850655 0.53525785 0.94705486 0.42238053
 0.64632248 0.93517655 0.69629966 0.03067745]
test_maes at epoch9: [0.97315212 0.45677674 0.63813984 0.43025225 0.60645    0.48255949
 0.39812259 0.68157888 0.42106177 0.09327372]

train_loss at epoch10: 0.018276501284993214
train_mses at epoch10: [1.61031593 0.31190026 0.68628305 0.29841602 0.88557823 0.35125546
 0.48527811 0.66335156 0.51511808 0.06025689]
train_maes at epoch10: [0.95889368 0.39583761 0.53469784 0.36317486 0.47026187 0.4604322
 0.44309882 0.49295227 0.42991158 0.17125192]
test_loss at epoch10: 0.02318508858266084
test_mses at epoch10: [1.62506768 0.47544778 0.95501696 0.54921476 0.89719196 0.4000533
 0.61515214 0.88539513 0.61883014 0.02890652]
test_maes at epoch10: [0.91746894 0.4397634  0.57817667 0.44394641 0.57857939 0.46529728
 0.36750206 0.65962469 0.45459087 0.08957183]

train_loss at epoch11: 0.015563283925471098
train_mses at epoch11: [1.46586774 0.28258834 0.65174147 0.2901396  0.81353175 0.20612552
 0.41974762 0.50119926 0.36585885 0.03613672]
train_maes at epoch11: [0.91261788 0.38024998 0.51648052 0.35611654 0.44688568 0.36841081
 0.40961932 0.44002403 0.36881855 0.13479954]
test_loss at epoch11: 0.02011587891889655
test_mses at epoch11: [1.35148231 0.43917327 0.87980524 0.33069828 0.83553026 0.36824667
 0.55621228 0.74526538 0.52798334 0.03573526]
test_maes at epoch11: [0.85809328 0.45669217 0.56268006 0.3305068  0.59069093 0.43573246
 0.33874223 0.561849   0.4076237  0.12064772]

train_loss at epoch12: 0.01352798614812934
train_mses at epoch12: [1.30380192 0.27414116 0.54842362 0.38902506 0.59781526 0.27839247
 0.25117625 0.40502975 0.32258069 0.03547923]
train_maes at epoch12: [0.85778758 0.37213146 0.52352432 0.37279806 0.42920618 0.41195077
 0.32861541 0.37756007 0.36957511 0.13730655]
test_loss at epoch12: 0.02099192984726118
test_mses at epoch12: [1.46894549 0.41702475 0.81863461 0.60391147 0.86177092 0.34528811
 0.51533853 0.84990208 0.49924633 0.02412483]
test_maes at epoch12: [0.87034519 0.40137358 0.51480219 0.45720223 0.68318031 0.40987096
 0.36348567 0.70003647 0.46205556 0.07709379]

train_loss at epoch13: 0.01134095891662266
train_mses at epoch13: [1.26979414 0.20790332 0.47672814 0.23081638 0.5781662  0.15311024
 0.24429045 0.33827737 0.33418667 0.03330069]
train_maes at epoch13: [0.87003456 0.33484896 0.46058521 0.27833657 0.45573219 0.29600456
 0.33796127 0.39015169 0.41788271 0.13477109]
test_loss at epoch13: 0.01861978743387305
test_mses at epoch13: [1.20371937 0.38010432 0.73466096 0.5433839  0.77843185 0.32037629
 0.47613617 0.70526625 0.43001313 0.02495486]
test_maes at epoch13: [0.78287072 0.38745742 0.49512396 0.41842437 0.65215304 0.39932865
 0.38859903 0.58076769 0.40877478 0.08706223]

train_loss at epoch14: 0.01024239542691604
train_mses at epoch14: [1.23887168 0.17705881 0.38144196 0.18433752 0.50718715 0.11577418
 0.28265259 0.30335356 0.34787172 0.03932836]
train_maes at epoch14: [0.83488228 0.31173523 0.40430706 0.26476822 0.37951131 0.26827582
 0.35311609 0.35168376 0.44430293 0.14269914]
test_loss at epoch14: 0.01842907621808674
test_mses at epoch14: [1.20781314 0.37016425 0.69082085 0.62921578 0.67872958 0.3870884
 0.47135807 0.66538699 0.44134403 0.02649119]
test_maes at epoch14: [0.77393116 0.35732474 0.47133392 0.44724428 0.57858343 0.46797836
 0.32560391 0.55739847 0.36536295 0.09190401]

train_loss at epoch15: 0.009407771994238314
train_mses at epoch15: [0.82221957 0.20120687 0.39278783 0.17143879 0.42088982 0.15124929
 0.275932   0.28227849 0.24467482 0.03827386]
train_maes at epoch15: [0.7072615  0.31931653 0.40387993 0.26625231 0.33874055 0.30571402
 0.31801177 0.3407982  0.35118812 0.14832324]
test_loss at epoch15: 0.01715818403855614
test_mses at epoch15: [1.05265604 0.35829451 0.63709774 0.57552746 0.63763279 0.29775934
 0.46537763 0.65716392 0.41013345 0.02592333]
test_maes at epoch15: [0.75721972 0.33087216 0.4458131  0.3914735  0.57019114 0.38589265
 0.32251122 0.54992757 0.32546313 0.08962208]

train_loss at epoch16: 0.008773399759893831
train_mses at epoch16: [0.93657381 0.12730098 0.29810752 0.25835911 0.38042028 0.12001601
 0.25379126 0.27266401 0.24479419 0.04077386]
train_maes at epoch16: [0.75885827 0.23513264 0.356566   0.30340133 0.34748399 0.25801782
 0.30181489 0.32044989 0.33203732 0.15032328]
test_loss at epoch16: 0.016839922122333362
test_mses at epoch16: [1.01392105 0.3209173  0.62720622 0.59116561 0.63598091 0.27792153
 0.44571623 0.71850215 0.35394526 0.02874348]
test_maes at epoch16: [0.76042075 0.31662836 0.4151306  0.42642299 0.60973801 0.38952973
 0.37158848 0.62315608 0.33527843 0.10081897]

train_loss at epoch17: 0.007353125706962917
train_mses at epoch17: [0.69494675 0.1078399  0.26759295 0.19267497 0.25341503 0.12748323
 0.23103961 0.23453787 0.24221801 0.03052105]
train_maes at epoch17: [0.62343956 0.22106743 0.3141916  0.27442907 0.31064814 0.28097994
 0.30856762 0.32365287 0.34338989 0.12943486]
test_loss at epoch17: 0.017548965047235073
test_mses at epoch17: [1.08802592 0.29762699 0.62646656 0.80099617 0.63091958 0.33981315
 0.45512792 0.62799014 0.36053634 0.03095851]
test_maes at epoch17: [0.75840845 0.30134618 0.3960742  0.56052529 0.61957509 0.44173209
 0.43403553 0.49761559 0.31676148 0.10686587]

train_loss at epoch18: 0.006559330646110618
train_mses at epoch18: [0.76805488 0.05699035 0.24410283 0.22429133 0.27535649 0.10876407
 0.18000443 0.21562621 0.16454506 0.03751442]
train_maes at epoch18: [0.66274328 0.1787524  0.30113225 0.29292545 0.32694751 0.24590256
 0.28243653 0.28537738 0.28598384 0.14440698]
test_loss at epoch18: 0.016678073483964672
test_mses at epoch18: [1.04533948 0.26446197 0.68187331 0.56692297 0.61019839 0.30988689
 0.44894844 0.67578825 0.36048016 0.02963853]
test_maes at epoch18: [0.75012818 0.26792707 0.40404889 0.38929085 0.6156351  0.35932446
 0.31660065 0.5530397  0.39767218 0.10004339]

train_loss at epoch19: 0.00614377823860749
train_mses at epoch19: [0.7507865  0.05669116 0.23848207 0.1617721  0.2218346  0.11473197
 0.17432073 0.20716177 0.19759073 0.02933144]
train_maes at epoch19: [0.63386965 0.16921472 0.3137488  0.25728022 0.26568046 0.25943217
 0.26448823 0.28152075 0.30884969 0.11789062]
test_loss at epoch19: 0.0166402834913005
test_mses at epoch19: [1.18804363 0.27769307 0.64157427 0.56530996 0.55655289 0.28485601
 0.44593656 0.71174352 0.42420202 0.02400205]
test_maes at epoch19: [0.77311695 0.25831626 0.36403474 0.35310802 0.57404796 0.34220041
 0.31474761 0.60885262 0.31182301 0.0809077 ]

train_loss at epoch20: 0.005999024633480155
train_mses at epoch20: [0.86163669 0.07916914 0.19270355 0.16254052 0.22486177 0.09448964
 0.13720753 0.23162548 0.18652219 0.0359196 ]
train_maes at epoch20: [0.68547734 0.17520484 0.27818714 0.20641696 0.28226203 0.2005294
 0.22620704 0.27467539 0.28021881 0.12672339]
test_loss at epoch20: 0.015560225948043491
test_mses at epoch20: [1.07017926 0.23489663 0.61295084 0.45232033 0.50494609 0.37906753
 0.41293664 0.62310928 0.43908003 0.02368608]
test_maes at epoch20: [0.78295123 0.26584676 0.36510235 0.30138002 0.54004598 0.48608419
 0.31420366 0.51423422 0.33542236 0.08236977]

train_loss at epoch21: 0.0054035026418126145
train_mses at epoch21: [0.66915074 0.04841734 0.1749515  0.14540534 0.16644173 0.08297482
 0.13994403 0.25790573 0.17585068 0.03141728]
train_maes at epoch21: [0.60504466 0.16170649 0.28201539 0.24912625 0.24400705 0.22556482
 0.21295871 0.25324164 0.29363851 0.13300435]
test_loss at epoch21: 0.015198503175507421
test_mses at epoch21: [1.05447542 0.22027381 0.64234271 0.41025935 0.59851526 0.27792976
 0.44711322 0.67520139 0.30461021 0.02304289]
test_maes at epoch21: [0.79894236 0.2634521  0.374372   0.31079156 0.63316673 0.39729118
 0.31660827 0.6226534  0.34173929 0.07715551]

train_loss at epoch22: 0.005520994572535805
train_mses at epoch22: [0.78288329 0.0673079  0.21319667 0.13394781 0.21377652 0.04786673
 0.12938242 0.20380193 0.21879855 0.02741782]
train_maes at epoch22: [0.67433609 0.18436497 0.26227447 0.25278378 0.28795656 0.17046749
 0.21691784 0.24864101 0.32509801 0.11698599]
test_loss at epoch22: 0.015876418546490047
test_mses at epoch22: [1.10358215 0.21706757 0.75919553 0.55917137 0.54839686 0.23068962
 0.44575532 0.66136624 0.31493559 0.02343582]
test_maes at epoch22: [0.76783611 0.23710725 0.40702509 0.40617011 0.60589381 0.32470049
 0.32949109 0.58287073 0.315658   0.07037501]

train_loss at epoch23: 0.004646515311754268
train_mses at epoch23: [0.68226916 0.04932456 0.15181629 0.12198316 0.15594301 0.05148883
 0.1587965  0.21360687 0.11722306 0.02705869]
train_maes at epoch23: [0.59908455 0.16540199 0.23831655 0.21664028 0.24711329 0.15376876
 0.25281565 0.23617397 0.22994448 0.11520544]
test_loss at epoch23: 0.014608395812304123
test_mses at epoch23: [0.9875911  0.20476425 0.64448738 0.4295256  0.47315732 0.23524823
 0.39916049 0.57788673 0.46671942 0.02704879]
test_maes at epoch23: [0.77068231 0.24621149 0.37086423 0.33990174 0.55262548 0.34656276
 0.33643961 0.49318586 0.33775401 0.08545732]

train_loss at epoch24: 0.005086873212586279
train_mses at epoch24: [0.58055913 0.07406607 0.22700592 0.11272086 0.13583716 0.04611649
 0.14534961 0.20339906 0.18890698 0.03147833]
train_maes at epoch24: [0.54106049 0.19752255 0.28422797 0.19643642 0.22868274 0.16355265
 0.25380989 0.24846759 0.28660146 0.12461606]
test_loss at epoch24: 0.014944688781448032
test_mses at epoch24: [0.96850232 0.20650779 0.70035828 0.39426074 0.58125603 0.22307192
 0.42500341 0.61319527 0.37547818 0.0239024 ]
test_maes at epoch24: [0.77047262 0.23096969 0.38183514 0.31888128 0.64968254 0.3380616
 0.30564675 0.53536311 0.28319464 0.07106028]

train_loss at epoch25: 0.004349138830666957
train_mses at epoch25: [0.55210721 0.05352882 0.18328675 0.14379824 0.12961853 0.03618676
 0.14679538 0.16323588 0.0963692  0.03213203]
train_maes at epoch25: [0.54020084 0.16023124 0.24982245 0.21933266 0.24168228 0.14371944
 0.22086676 0.24465235 0.20407537 0.13488875]
test_loss at epoch25: 0.01468469008155491
test_mses at epoch25: [0.91248282 0.21129896 0.68571462 0.40812167 0.55702293 0.21246888
 0.45142722 0.65038391 0.28871606 0.02194406]
test_maes at epoch25: [0.73342663 0.21747803 0.37131503 0.3224751  0.63436545 0.33128934
 0.30880897 0.57737841 0.31301521 0.06498849]

train_loss at epoch26: 0.0033753887628731522
train_mses at epoch26: [0.44723353 0.03057208 0.12396496 0.0808057  0.11612419 0.0596423
 0.05824881 0.17277001 0.09806023 0.02200061]
train_maes at epoch26: [0.50364498 0.1353477  0.21131827 0.17024779 0.2104202  0.17519506
 0.16383882 0.25255068 0.24206566 0.11393631]
test_loss at epoch26: 0.013350847622622614
test_mses at epoch26: [0.86490322 0.19492016 0.63740316 0.44824185 0.40190225 0.20883588
 0.36141855 0.61864507 0.27487224 0.0217315 ]
test_maes at epoch26: [0.70153381 0.2237223  0.34916315 0.34432007 0.51063808 0.29564145
 0.29126843 0.53722943 0.30401471 0.06559826]

train_loss at epoch27: 0.003482899993010189
train_mses at epoch27: [0.45202707 0.04446972 0.09707178 0.08674068 0.11408629 0.05504906
 0.07111046 0.18422498 0.11205302 0.02958215]
train_maes at epoch27: [0.49068522 0.1522117  0.20719699 0.19053997 0.21359962 0.15024683
 0.17410206 0.24534849 0.22927799 0.11745604]
test_loss at epoch27: 0.014565427342186804
test_mses at epoch27: [0.94983166 0.23486114 0.6971215  0.55361343 0.48379817 0.21900189
 0.31842337 0.60729831 0.32796228 0.02092555]
test_maes at epoch27: [0.7245097  0.22761954 0.36117082 0.38468846 0.59144939 0.32626939
 0.3103101  0.49061238 0.25630977 0.06323486]

train_loss at epoch28: 0.0035803443549767785
train_mses at epoch28: [0.49034197 0.070237   0.14121304 0.09017755 0.08025802 0.03631769
 0.09040386 0.16816082 0.09978208 0.02829204]
train_maes at epoch28: [0.53203855 0.17427665 0.20349957 0.19736839 0.19989904 0.15730456
 0.19697104 0.2461024  0.19231938 0.11933402]
test_loss at epoch28: 0.014441082658975021
test_mses at epoch28: [0.94364553 0.24191901 0.69282182 0.48494289 0.45534791 0.22741708
 0.37876286 0.62146379 0.308763   0.02182859]
test_maes at epoch28: [0.72014385 0.23133359 0.35278388 0.33926483 0.56540064 0.34025466
 0.2924707  0.54680514 0.2475566  0.06929478]

train_loss at epoch29: 0.003356320297588473
train_mses at epoch29: [0.43025845 0.06063688 0.11763718 0.10990086 0.08768669 0.04547919
 0.10401164 0.1485891  0.05515177 0.03150188]
train_maes at epoch29: [0.50165549 0.15890514 0.2220303  0.21259531 0.1967644  0.15745334
 0.20446244 0.23704063 0.17707027 0.11783199]
test_loss at epoch29: 0.013581204673518305
test_mses at epoch29: [0.90645195 0.19320182 0.60277083 0.38660614 0.41203848 0.21496179
 0.48935605 0.61463443 0.27973441 0.02550019]
test_maes at epoch29: [0.73067728 0.24684046 0.34185681 0.30981542 0.52814449 0.31444548
 0.31963549 0.56597616 0.24924634 0.08533709]

train_loss at epoch30: 0.002849581367943598
train_mses at epoch30: [0.25766447 0.04965417 0.0922813  0.0868908  0.06041497 0.07003925
 0.08593229 0.13002092 0.06118105 0.0298418 ]
train_maes at epoch30: [0.39576484 0.15820808 0.21127526 0.19676634 0.17973498 0.18914364
 0.1825534  0.22251418 0.15570962 0.12206913]
test_loss at epoch30: 0.01371179391508517
test_mses at epoch30: [0.91853309 0.19804379 0.64106934 0.42870606 0.52893719 0.20707573
 0.37840701 0.61790795 0.23384813 0.02671249]
test_maes at epoch30: [0.72975374 0.23002414 0.34960893 0.34657189 0.62601031 0.3228038
 0.29918972 0.57187294 0.23643158 0.093349  ]

train_loss at epoch31: 0.0027358920677848482
train_mses at epoch31: [0.30839839 0.04191501 0.11867424 0.06137264 0.06446307 0.05224325
 0.07095416 0.13604882 0.06277176 0.02348307]
train_maes at epoch31: [0.43552141 0.15204129 0.21658363 0.18706381 0.19276688 0.16748651
 0.15735346 0.23700373 0.17117127 0.10743043]
test_loss at epoch31: 0.014836291418127392
test_mses at epoch31: [0.94936513 0.24462006 0.72529734 0.59465311 0.52074044 0.20196343
 0.3665536  0.60973105 0.24327186 0.02705692]
test_maes at epoch31: [0.72270279 0.24008201 0.37471756 0.41570995 0.62699867 0.31236776
 0.32773987 0.51548474 0.23612599 0.09485569]

train_loss at epoch32: 0.0026733707636594772
train_mses at epoch32: [0.28519931 0.03443924 0.06145422 0.0873239  0.10294221 0.0333497
 0.07340829 0.1139119  0.07915916 0.02299619]
train_maes at epoch32: [0.42167717 0.14127055 0.16954642 0.1783576  0.17910362 0.13710748
 0.19192834 0.19913035 0.17018042 0.1072841 ]
test_loss at epoch32: 0.013245052617529163
test_mses at epoch32: [1.034737   0.18953813 0.5880383  0.44849455 0.40145997 0.19524815
 0.39863804 0.5991288  0.27140483 0.02885249]
test_maes at epoch32: [0.79678987 0.24479216 0.35337287 0.32373067 0.52838565 0.29519338
 0.29663988 0.48295396 0.23985135 0.09974724]

train_loss at epoch33: 0.0026793666588871374
train_mses at epoch33: [0.40213353 0.06492412 0.11493666 0.06480878 0.07622032 0.04258841
 0.06318596 0.10332835 0.05239791 0.02960118]
train_maes at epoch33: [0.4708255  0.18160101 0.21179932 0.16410905 0.18218266 0.1526188
 0.16034365 0.20836853 0.16634409 0.11505341]
test_loss at epoch33: 0.015476035359113113
test_mses at epoch33: [0.97455218 0.24127251 0.77020642 0.56410848 0.52327281 0.20120822
 0.4318803  0.68595288 0.2373362  0.02661848]
test_maes at epoch33: [0.74825154 0.22510222 0.39603985 0.34445325 0.63519316 0.29516385
 0.30272908 0.57560268 0.25283452 0.09401161]

train_loss at epoch34: 0.002881658384981363
train_mses at epoch34: [0.40350837 0.04534386 0.08973432 0.10061132 0.09673222 0.03439088
 0.06675034 0.11919576 0.06949923 0.02613681]
train_maes at epoch34: [0.49497682 0.15173659 0.19335651 0.20608222 0.20197696 0.13420411
 0.1659372  0.21004636 0.17329191 0.11523074]
test_loss at epoch34: 0.012923301400049873
test_mses at epoch34: [0.97931104 0.19909766 0.67992199 0.32352814 0.3809073  0.21956052
 0.37029228 0.62739321 0.23208848 0.02488811]
test_maes at epoch34: [0.77021458 0.22147697 0.37816114 0.26116584 0.51992637 0.33246816
 0.29628657 0.54829972 0.24615145 0.09544365]

train_loss at epoch35: 0.0024061956564369407
train_mses at epoch35: [0.32142805 0.02981471 0.08258459 0.09066011 0.06464142 0.04321275
 0.05943325 0.10735898 0.05100706 0.01651671]
train_maes at epoch35: [0.43772577 0.13261695 0.1924014  0.19613516 0.16287867 0.15354305
 0.16805354 0.19979784 0.16616886 0.09835803]
test_loss at epoch35: 0.014098165637773016
test_mses at epoch35: [0.93286535 0.19646173 0.71669933 0.48249903 0.33262373 0.29486705
 0.42832689 0.60267237 0.2717864  0.02321206]
test_maes at epoch35: [0.72112853 0.22631274 0.38319518 0.31978033 0.47415798 0.40234926
 0.29810448 0.52814913 0.23828261 0.08536434]

train_loss at epoch36: 0.001969398930668831
train_mses at epoch36: [0.27316525 0.05346137 0.07065257 0.04503182 0.04916707 0.06139259
 0.03639996 0.07446309 0.03702217 0.02333589]
train_maes at epoch36: [0.38935838 0.15020644 0.18603666 0.15797913 0.14404271 0.17957554
 0.14456183 0.15548539 0.13455336 0.11256996]
test_loss at epoch36: 0.013883310005716656
test_mses at epoch36: [0.95513049 0.19036272 0.67991062 0.48719816 0.42131179 0.31041494
 0.36574864 0.61184354 0.21705336 0.02386973]
test_maes at epoch36: [0.7291493  0.22128452 0.37682659 0.36562006 0.55441941 0.41407677
 0.30009502 0.58200892 0.23426453 0.09061321]

train_loss at epoch37: 0.0021598444765676622
train_mses at epoch37: [0.39026618 0.06225927 0.05186592 0.09032306 0.04941175 0.02896625
 0.03771244 0.08220191 0.04217045 0.02481317]
train_maes at epoch37: [0.45638288 0.14388513 0.15859319 0.18359152 0.14521024 0.11777557
 0.12843719 0.18916633 0.13856182 0.11531191]
test_loss at epoch37: 0.013880170395840769
test_mses at epoch37: [0.98767007 0.18975782 0.6849828  0.5389692  0.48986938 0.22863186
 0.35878038 0.58322388 0.20264317 0.02665192]
test_maes at epoch37: [0.75103885 0.21583096 0.36807198 0.40107258 0.60757519 0.33539243
 0.31017726 0.5627974  0.23422438 0.0971235 ]

train_loss at epoch38: 0.0019377153771726982
train_mses at epoch38: [0.24891962 0.03244159 0.0710054  0.07698482 0.05212219 0.04004656
 0.04478248 0.05473132 0.04995835 0.0170922 ]
train_maes at epoch38: [0.38677072 0.12637365 0.1817067  0.17336687 0.15170481 0.14335338
 0.1454554  0.13945813 0.14549564 0.098397  ]
test_loss at epoch38: 0.013679605301307596
test_mses at epoch38: [0.99734818 0.20034443 0.6543086  0.51239592 0.48407242 0.23401546
 0.35181394 0.56333006 0.2156798  0.03418174]
test_maes at epoch38: [0.76668892 0.21409086 0.35671469 0.37547079 0.60545174 0.33507864
 0.31323092 0.54534051 0.22973026 0.12131464]

train_loss at epoch39: 0.0017694728853909867
train_mses at epoch39: [0.25846863 0.03861873 0.04435513 0.07598598 0.04092945 0.02918455
 0.03652894 0.06437925 0.04874966 0.02173594]
train_maes at epoch39: [0.399507   0.12798251 0.15272993 0.17366617 0.14075029 0.12571039
 0.13030749 0.16766596 0.14055498 0.10585484]
test_loss at epoch39: 0.014455155671938606
test_mses at epoch39: [0.93081855 0.23324579 0.74451605 0.57810692 0.39059009 0.26586024
 0.37845861 0.56074008 0.25380895 0.03812047]
test_maes at epoch39: [0.71773904 0.22933307 0.37992642 0.38178433 0.53430801 0.3660296
 0.31125126 0.52326939 0.23447047 0.12933358]

train_loss at epoch40: 0.0016034509500731592
train_mses at epoch40: [0.22401401 0.0292193  0.05114548 0.06699498 0.03269379 0.03991841
 0.02597715 0.04577111 0.04070685 0.02804528]
train_maes at epoch40: [0.38072714 0.1267235  0.14972544 0.16803854 0.1372939  0.14701746
 0.12175121 0.14126256 0.14329816 0.11645731]
test_loss at epoch40: 0.01406028801980226
test_mses at epoch40: [0.9265246  0.2231453  0.76843588 0.555914   0.30594568 0.29369014
 0.38571767 0.55640802 0.22424313 0.03338921]
test_maes at epoch40: [0.71125568 0.26093166 0.40395229 0.37524741 0.46071172 0.39384892
 0.30436894 0.52242527 0.24397696 0.11690207]

train_loss at epoch41: 0.0014604902218865311
train_mses at epoch41: [0.2338309  0.04029924 0.0400999  0.0434608  0.05539432 0.03607263
 0.02038894 0.03400084 0.0370105  0.01876664]
train_maes at epoch41: [0.37895493 0.14211419 0.15922605 0.1539669  0.15983635 0.13922217
 0.1035487  0.13276871 0.13763329 0.09751507]
test_loss at epoch41: 0.011881549235271372
test_mses at epoch41: [1.03534058 0.19691203 0.66742075 0.31864814 0.2620196  0.22722879
 0.38674624 0.5189798  0.18817982 0.02666867]
test_maes at epoch41: [0.78320931 0.30026292 0.35603104 0.2711429  0.424045   0.32688673
 0.30347185 0.49078211 0.27274506 0.10301348]

train_loss at epoch42: 0.001772853545844555
train_mses at epoch42: [0.3018415  0.04446921 0.0262943  0.09594101 0.03604488 0.03816492
 0.04436523 0.06603802 0.03137547 0.01640156]
train_maes at epoch42: [0.42552783 0.16527945 0.12053443 0.19752984 0.136828   0.14300328
 0.13900755 0.16062642 0.13551707 0.09145583]
test_loss at epoch42: 0.014932651720617128
test_mses at epoch42: [1.01601077 0.21989334 0.78528257 0.65579743 0.52914382 0.23649979
 0.35216647 0.57290835 0.18732726 0.02263207]
test_maes at epoch42: [0.73601531 0.2123255  0.40497496 0.40784728 0.63760707 0.3473661
 0.30702116 0.55182946 0.2429472  0.09280231]

train_loss at epoch43: 0.0017572228756287823
train_mses at epoch43: [0.27066052 0.03792692 0.0467149  0.09844057 0.05781281 0.02995385
 0.03707362 0.04137329 0.02926116 0.01283863]
train_maes at epoch43: [0.37265017 0.14531333 0.14402037 0.17864006 0.18406522 0.12223311
 0.13125638 0.14209034 0.13226901 0.08667496]
test_loss at epoch43: 0.012973359421543453
test_mses at epoch43: [1.03369502 0.19747783 0.6291248  0.47859403 0.42989403 0.27280758
 0.28640334 0.50777528 0.25257262 0.02224693]
test_maes at epoch43: [0.76542311 0.21329624 0.34370198 0.32282125 0.56560073 0.3812622
 0.30250798 0.50006576 0.22628157 0.08882763]

train_loss at epoch44: 0.0016120984826398933
train_mses at epoch44: [0.16230589 0.0358888  0.04317274 0.05908687 0.02761835 0.04295355
 0.06306646 0.02935092 0.04779957 0.02078162]
train_maes at epoch44: [0.31802742 0.13344016 0.15806658 0.17670382 0.12163396 0.14155995
 0.15609896 0.12576048 0.15924721 0.11100949]
test_loss at epoch44: 0.012703282029732414
test_mses at epoch44: [1.01587532 0.18585991 0.69110753 0.4261508  0.28872306 0.25095342
 0.32032184 0.53042661 0.28093622 0.02215205]
test_maes at epoch44: [0.75662391 0.20056119 0.34947666 0.28370925 0.4492505  0.35371435
 0.28103899 0.42792316 0.2346428  0.08138742]

train_loss at epoch45: 0.0017057441417937694
train_mses at epoch45: [0.20177813 0.03258514 0.04808175 0.08161749 0.02364418 0.04756688
 0.02355046 0.08785721 0.03472632 0.01528628]
train_maes at epoch45: [0.34245102 0.11794716 0.14342429 0.18468951 0.12307463 0.14875801
 0.11056672 0.17396318 0.13856775 0.08475923]
test_loss at epoch45: 0.01490984711310138
test_mses at epoch45: [1.04241104 0.25212101 0.79903556 0.64408082 0.37943736 0.26752068
 0.38369033 0.56215735 0.23213189 0.02553931]
test_maes at epoch45: [0.74406636 0.21724995 0.37888764 0.39825122 0.52471906 0.37628251
 0.29471336 0.50630062 0.23593607 0.09134113]

train_loss at epoch46: 0.0015301841270664463
train_mses at epoch46: [0.18378579 0.05479041 0.0377554  0.04678198 0.04225142 0.03763688
 0.02824289 0.05938429 0.02410744 0.01440286]
train_maes at epoch46: [0.31330159 0.13652504 0.13256572 0.14941033 0.1362679  0.13830612
 0.12001538 0.1666756  0.11306658 0.08670911]
test_loss at epoch46: 0.013818959341101025
test_mses at epoch46: [1.07032273 0.1942119  0.63253062 0.52042104 0.48115697 0.23603825
 0.31358391 0.71119918 0.17060997 0.03124034]
test_maes at epoch46: [0.79902394 0.20495642 0.34517673 0.40092143 0.59831225 0.3525441
 0.29456307 0.69050875 0.25550926 0.12063722]

train_loss at epoch47: 0.0018370411885173423
train_mses at epoch47: [0.20214612 0.06604787 0.05387966 0.03841416 0.02675216 0.04009266
 0.05154572 0.09025525 0.03531786 0.01628637]
train_maes at epoch47: [0.34515211 0.16348764 0.15496899 0.14326331 0.12374279 0.13897652
 0.12399473 0.21297722 0.13965219 0.09603569]
test_loss at epoch47: 0.011820271287275396
test_mses at epoch47: [1.1134645  0.17574972 0.65811224 0.38402322 0.28691437 0.21826816
 0.34964692 0.48489622 0.18898328 0.02798301]
test_maes at epoch47: [0.82346263 0.2467643  0.33856645 0.30119136 0.44307301 0.30998917
 0.28370956 0.4546265  0.23742557 0.10627094]

train_loss at epoch48: 0.0018757407189063404
train_mses at epoch48: [0.26357545 0.03530481 0.0747611  0.05314326 0.03653765 0.03955957
 0.02243517 0.101136   0.04027675 0.01698699]
train_maes at epoch48: [0.3487249  0.13265509 0.16119983 0.13531096 0.12223862 0.13508581
 0.1009462  0.20272238 0.1440828  0.09101517]
test_loss at epoch48: 0.013091668162656866
test_mses at epoch48: [0.98297829 0.19619925 0.84458503 0.38539279 0.27348154 0.24093364
 0.39552684 0.5171504  0.21313287 0.02608423]
test_maes at epoch48: [0.74501246 0.21081734 0.39793631 0.24642634 0.41256339 0.3451011
 0.28995726 0.41033591 0.23051578 0.09144346]

train_loss at epoch49: 0.001589726049291051
train_mses at epoch49: [0.19319033 0.02795916 0.05397129 0.04170639 0.0444486  0.05149159
 0.02976488 0.05414237 0.04446359 0.01636666]
train_maes at epoch49: [0.31186964 0.10565987 0.14711511 0.14558335 0.13460568 0.1630411
 0.11119448 0.1757327  0.13890687 0.08517164]
test_loss at epoch49: 0.011332077176674553
test_mses at epoch49: [1.11451514 0.17520448 0.70721186 0.21029508 0.34146328 0.24684389
 0.32464243 0.47916395 0.15790714 0.02550258]
test_maes at epoch49: [0.8136358  0.20879266 0.36161299 0.21551728 0.49574314 0.35678885
 0.29382931 0.51731507 0.21581881 0.09689079]

