Train on FAU 5 dark, test on FAU 5 light
train_loss at epoch0: 0.0336257980224934
train_mses at epoch0: [3.35489453 1.05848859 1.12821334 0.97172483 1.04670072 0.53150463
 1.07083696 1.06528889 1.04652403 0.02438719]
train_maes at epoch0: [1.57635092 0.50119627 0.64949211 0.53892744 0.46346696 0.39393981
 0.44355478 0.5176492  0.56224487 0.06186922]
test_loss at epoch0: 0.029700443719295744
test_mses at epoch0: [2.76281261 0.92906256 1.19216086 0.87943765 0.93930669 0.1138395
 0.96003938 1.04143578 0.94239866 0.02048226]
test_maes at epoch0: [1.47766988 0.53849767 0.60099222 0.65341374 0.52185974 0.17549376
 0.4782189  0.58221221 0.58194144 0.03691762]

train_loss at epoch1: 0.026549118376792746
train_mses at epoch1: [2.27901863 0.84559295 1.11824767 0.69855653 0.84787    0.1261338
 0.83721804 0.92776854 0.83700945 0.03140669]
train_maes at epoch1: [1.32000097 0.52252047 0.6031557  0.50966987 0.51180767 0.23903224
 0.5063533  0.49202663 0.49528987 0.10517932]
test_loss at epoch1: 0.023297190539380338
test_mses at epoch1: [1.64052281 0.53858848 1.03130464 1.19553741 0.58958314 0.10286677
 0.62446268 0.86019646 0.59118561 0.02174202]
test_maes at epoch1: [1.16123664 0.49602504 0.56192806 0.97946818 0.41593681 0.26883024
 0.341389   0.43519231 0.38803287 0.06100955]

train_loss at epoch2: 0.016879354449028664
train_mses at epoch2: [1.10567232 0.40599634 0.85119352 0.45166139 0.51535277 0.16298642
 0.4635286  0.62177145 0.54511015 0.04469885]
train_maes at epoch2: [0.82914043 0.42220377 0.54581003 0.44374066 0.46210855 0.30571404
 0.40732702 0.49687185 0.47857004 0.14329123]
test_loss at epoch2: 0.012353838441219735
test_mses at epoch2: [0.72712277 0.19936834 0.77865255 0.42838126 0.32646061 0.05917524
 0.24709638 0.67350568 0.20255382 0.03002095]
test_maes at epoch2: [0.64155285 0.26004609 0.42611039 0.41931951 0.35956848 0.16703935
 0.26910949 0.42832413 0.2624233  0.13622458]

train_loss at epoch3: 0.009556006687752744
train_mses at epoch3: [0.78338398 0.15942559 0.60696798 0.38633096 0.28168192 0.09736876
 0.18147996 0.34311068 0.1818653  0.04588408]
train_maes at epoch3: [0.69414593 0.26671804 0.458135   0.40691722 0.37753276 0.21987169
 0.28379473 0.41312142 0.29514824 0.15238214]
test_loss at epoch3: 0.007950074621971618
test_mses at epoch3: [0.24628565 0.05872131 0.49738047 0.48292748 0.1728125  0.07498922
 0.0740733  0.47164155 0.04986805 0.03151245]
test_maes at epoch3: [0.34775378 0.15045646 0.3854011  0.5048501  0.31314396 0.24308903
 0.21931793 0.37243642 0.16720377 0.14733456]

train_loss at epoch4: 0.005800936022337447
train_mses at epoch4: [0.5011549  0.08765303 0.36800704 0.27026857 0.16466107 0.08698124
 0.08490688 0.18959857 0.08601076 0.0372241 ]
train_maes at epoch4: [0.53217022 0.21600547 0.39146251 0.33209377 0.30018603 0.21541868
 0.21279917 0.32566356 0.21093883 0.13479485]
test_loss at epoch4: 0.005034603749183898
test_mses at epoch4: [0.31535323 0.03767354 0.3245359  0.26483994 0.08079549 0.02998
 0.02278532 0.3752676  0.0369163  0.02307281]
test_maes at epoch4: [0.41929556 0.13708606 0.29699618 0.31265748 0.19520372 0.12600783
 0.10788522 0.33901644 0.14455398 0.10881982]

train_loss at epoch5: 0.003760227212246428
train_mses at epoch5: [0.42213412 0.06937192 0.18354884 0.1921132  0.08667185 0.06894073
 0.06273338 0.09659915 0.08603873 0.03312247]
train_maes at epoch5: [0.48995183 0.19511804 0.29716505 0.28716381 0.20809305 0.19274928
 0.17390467 0.23317051 0.20288588 0.12812669]
test_loss at epoch5: 0.0045443100973646695
test_mses at epoch5: [0.27727809 0.02757992 0.28250632 0.33035468 0.05067665 0.03369672
 0.01152175 0.29595973 0.03119111 0.02129807]
test_maes at epoch5: [0.38267604 0.11194962 0.31877521 0.45734991 0.14463983 0.11521453
 0.07648649 0.27873973 0.14649871 0.10230217]

train_loss at epoch6: 0.002488699087754209
train_mses at epoch6: [0.32983013 0.04867642 0.0817108  0.1281373  0.05395721 0.04499454
 0.05414155 0.07168261 0.06970447 0.02677782]
train_maes at epoch6: [0.4405812  0.15944862 0.21317389 0.2458844  0.16990369 0.15262053
 0.16588054 0.19859227 0.197853   0.11728331]
test_loss at epoch6: 0.0036813728511333466
test_mses at epoch6: [0.25243436 0.026054   0.21804989 0.21333421 0.04887668 0.04796032
 0.01917649 0.25535341 0.02855786 0.02064322]
test_maes at epoch6: [0.36500067 0.11611273 0.2617824  0.35780033 0.13733345 0.11375526
 0.086892   0.24252935 0.1278082  0.10243291]

train_loss at epoch7: 0.0019412290146376223
train_mses at epoch7: [0.29525172 0.04876521 0.0631078  0.0764782  0.04377125 0.03511834
 0.03944586 0.06071048 0.05689769 0.02373998]
train_maes at epoch7: [0.39568472 0.15683715 0.18344057 0.18602166 0.14920906 0.13031129
 0.14132509 0.17852859 0.16936864 0.10350558]
test_loss at epoch7: 0.0030037393595309966
test_mses at epoch7: [0.19721516 0.02965874 0.17716032 0.20108785 0.04059346 0.02688216
 0.01388668 0.18242514 0.02051709 0.0240865 ]
test_maes at epoch7: [0.34351486 0.10719065 0.22830021 0.35011297 0.113689   0.09888357
 0.07456673 0.21025158 0.10161007 0.12206758]

train_loss at epoch8: 0.001809149386083826
train_mses at epoch8: [0.2959369  0.04790991 0.06599803 0.06721695 0.04817523 0.02915846
 0.03766912 0.05620234 0.03778502 0.02392216]
train_maes at epoch8: [0.40164693 0.14637971 0.19103557 0.16888703 0.14079939 0.12639471
 0.13626476 0.16494557 0.14164893 0.10289541]
test_loss at epoch8: 0.002122302749689589
test_mses at epoch8: [0.20884229 0.01516491 0.06442344 0.2078773  0.01741613 0.0090341
 0.01033794 0.15000432 0.01318533 0.01394735]
test_maes at epoch8: [0.40018682 0.09557563 0.15417796 0.38725795 0.08378975 0.06908485
 0.06134468 0.18178232 0.07711895 0.07109322]

train_loss at epoch9: 0.001687044785060781
train_mses at epoch9: [0.22148585 0.05279434 0.05856815 0.05490057 0.03420834 0.03336219
 0.04032234 0.05909372 0.03992638 0.01963106]
train_maes at epoch9: [0.35486708 0.14806899 0.17862746 0.15893959 0.13040323 0.12601618
 0.13530078 0.17124583 0.14064818 0.09465533]
test_loss at epoch9: 0.0016096487680965282
test_mses at epoch9: [0.15544987 0.04345794 0.06244173 0.08719978 0.01415066 0.00504375
 0.01169741 0.12795685 0.01426917 0.01355554]
test_maes at epoch9: [0.32932334 0.12865941 0.14796574 0.21932079 0.08376319 0.04661308
 0.06701219 0.17738375 0.07686875 0.07002509]

train_loss at epoch10: 0.001451374075197159
train_mses at epoch10: [0.23041829 0.04522311 0.04818833 0.04972101 0.02953226 0.03425984
 0.02949825 0.04439236 0.03330075 0.01848041]
train_maes at epoch10: [0.37159954 0.14402867 0.15950911 0.15560013 0.11255452 0.13001534
 0.12061603 0.14968253 0.12723543 0.08909543]
test_loss at epoch10: 0.002156608464553001
test_mses at epoch10: [0.128734   0.01867779 0.09385209 0.17082733 0.02909846 0.01531237
 0.0173646  0.14126444 0.01503942 0.01533407]
test_maes at epoch10: [0.2910506  0.0821881  0.15442686 0.32465806 0.09799251 0.07491009
 0.07296274 0.19403248 0.07616646 0.08966856]

train_loss at epoch11: 0.0013541400749632652
train_mses at epoch11: [0.19022596 0.02961068 0.05408613 0.04289468 0.03334665 0.02438871
 0.03411429 0.04686535 0.03417099 0.01552774]
train_maes at epoch11: [0.33620507 0.12101942 0.16730164 0.14381634 0.12118337 0.11476336
 0.12186705 0.14892287 0.12748448 0.08606407]
test_loss at epoch11: 0.0018878971325590255
test_mses at epoch11: [0.13774581 0.01938826 0.11016397 0.08230429 0.03131591 0.01541404
 0.02425819 0.14261812 0.01034956 0.0136891 ]
test_maes at epoch11: [0.28971238 0.08558705 0.16593605 0.16526358 0.09282127 0.07119844
 0.09309572 0.18746479 0.07228461 0.08600058]

train_loss at epoch12: 0.0012219680948777401
train_mses at epoch12: [0.16173489 0.02894853 0.03775301 0.04841331 0.03015339 0.02280356
 0.0292128  0.04094229 0.03066536 0.01421235]
train_maes at epoch12: [0.30490877 0.1225557  0.13755297 0.14916353 0.11676693 0.10406726
 0.11581453 0.14311968 0.12117158 0.086462  ]
test_loss at epoch12: 0.0018502063018844483
test_mses at epoch12: [0.17795612 0.01301915 0.06096308 0.20552631 0.01414677 0.01392585
 0.01062313 0.10057174 0.00961311 0.00896259]
test_maes at epoch12: [0.3697175  0.08269157 0.12987221 0.36549469 0.07908643 0.07775725
 0.06901066 0.16351627 0.06599648 0.05831368]

train_loss at epoch13: 0.0012205230825124904
train_mses at epoch13: [0.17338556 0.03696175 0.03962177 0.0434882  0.02657094 0.02261316
 0.02002004 0.03958058 0.03923091 0.01319273]
train_maes at epoch13: [0.31025927 0.12286555 0.14514198 0.14583995 0.10786773 0.10407818
 0.10458065 0.14305014 0.1374242  0.08057522]
test_loss at epoch13: 0.0009201469910430147
test_mses at epoch13: [0.09283938 0.0069196  0.04262313 0.05456495 0.01335761 0.00411685
 0.00744523 0.06853183 0.00949394 0.00939268]
test_maes at epoch13: [0.24100417 0.06155797 0.11061749 0.14114948 0.06490938 0.04734705
 0.05376301 0.1395459  0.06885594 0.07123599]

train_loss at epoch14: 0.0011898458519197525
train_mses at epoch14: [0.17726496 0.02711802 0.04201701 0.05059335 0.02518322 0.02563846
 0.02782474 0.03521544 0.02900776 0.0127082 ]
train_maes at epoch14: [0.32357761 0.11528536 0.1454859  0.14865816 0.10882107 0.1103657
 0.11410508 0.13253243 0.11846694 0.08231537]
test_loss at epoch14: 0.0011012532054743868
test_mses at epoch14: [0.08748446 0.00692677 0.04586371 0.06180264 0.01781789 0.01192596
 0.01316289 0.0859724  0.01330213 0.00630446]
test_maes at epoch14: [0.24812665 0.06845278 0.10613712 0.17757406 0.08264546 0.06134085
 0.07427314 0.15357424 0.07504168 0.04686647]

train_loss at epoch15: 0.0010948686206594426
train_mses at epoch15: [0.15913285 0.02649792 0.03580639 0.0350289  0.02673128 0.02917426
 0.02393144 0.03770278 0.02857781 0.01015789]
train_maes at epoch15: [0.31119977 0.11306899 0.13071454 0.12785314 0.10291228 0.11309647
 0.11053411 0.13527716 0.11658519 0.07359566]
test_loss at epoch15: 0.0015731021960047964
test_mses at epoch15: [0.16237818 0.00684447 0.05651096 0.16665987 0.01870905 0.00606769
 0.00998119 0.08721108 0.00658531 0.01096824]
test_maes at epoch15: [0.34863401 0.05920103 0.12452659 0.32376674 0.08301856 0.05498929
 0.06701134 0.16466949 0.05610316 0.08585476]

train_loss at epoch16: 0.0010520014991151525
train_mses at epoch16: [0.16651072 0.02348195 0.04085279 0.04620532 0.01968642 0.02352573
 0.02524029 0.03380499 0.01693704 0.00966218]
train_maes at epoch16: [0.31075355 0.10762446 0.14203234 0.14240987 0.09449926 0.10119287
 0.10302141 0.13281309 0.09479157 0.07229821]
test_loss at epoch16: 0.0011884852411582114
test_mses at epoch16: [0.08802464 0.02252522 0.07058492 0.06992985 0.01233727 0.0068141
 0.00835215 0.07894437 0.00572706 0.00659542]
test_maes at epoch16: [0.2332309  0.09202957 0.13612417 0.19748728 0.0697247  0.05044834
 0.05215319 0.13702962 0.04630833 0.05427357]

train_loss at epoch17: 0.00102851576310523
train_mses at epoch17: [0.15329146 0.02504508 0.03826098 0.03436837 0.02110247 0.02356821
 0.02314245 0.03845682 0.0224844  0.00999053]
train_maes at epoch17: [0.29800863 0.11102531 0.13453751 0.12993933 0.10092696 0.09867037
 0.10228424 0.13821699 0.10551792 0.07324736]
test_loss at epoch17: 0.0007604040383817033
test_mses at epoch17: [0.05948791 0.0080555  0.02782111 0.06190873 0.00981953 0.00748097
 0.00678032 0.04818104 0.00548222 0.00584544]
test_maes at epoch17: [0.19959923 0.06548411 0.09181987 0.17739681 0.06033363 0.05222407
 0.04970858 0.13021087 0.04872975 0.05474477]

train_loss at epoch18: 0.0009223598471664368
train_mses at epoch18: [0.13278852 0.02386812 0.03392053 0.02889783 0.02490602 0.01945498
 0.0202752  0.03080797 0.02350865 0.00864144]
train_maes at epoch18: [0.27868689 0.1022762  0.12926985 0.12129183 0.10065481 0.09638756
 0.09795274 0.12028524 0.1076092  0.06763832]
test_loss at epoch18: 0.0013004987916730819
test_mses at epoch18: [0.070941   0.00639636 0.05030406 0.14778764 0.00959472 0.02305325
 0.00469721 0.05601972 0.00832575 0.00472516]
test_maes at epoch18: [0.22279151 0.05190938 0.12592304 0.29145778 0.06274576 0.09854122
 0.04953184 0.13501034 0.07246842 0.04576813]

train_loss at epoch19: 0.0009994647723246127
train_mses at epoch19: [0.16197968 0.02572473 0.03019568 0.04106262 0.01938611 0.02210281
 0.02142222 0.03210952 0.02401395 0.00974227]
train_maes at epoch19: [0.29251017 0.10359923 0.12142562 0.13312919 0.09338119 0.10502646
 0.09770418 0.12965202 0.10514908 0.07015659]
test_loss at epoch19: 0.0007724701387292527
test_mses at epoch19: [0.0774898  0.00573511 0.03583316 0.06203545 0.00722887 0.0091715
 0.00423441 0.04417552 0.00904596 0.00425215]
test_maes at epoch19: [0.21950105 0.05003433 0.10584296 0.1635861  0.05384341 0.07144894
 0.04513394 0.13206417 0.05981597 0.04287974]

train_loss at epoch20: 0.0008853796829885625
train_mses at epoch20: [0.11687006 0.02460508 0.02599203 0.03162804 0.01636646 0.02263594
 0.02202847 0.0302708  0.0231608  0.00872574]
train_maes at epoch20: [0.2650523  0.10720439 0.11608663 0.12624235 0.09158299 0.09206124
 0.09972927 0.12415578 0.10651355 0.06866532]
test_loss at epoch20: 0.0012181822805011526
test_mses at epoch20: [0.06356533 0.00880251 0.06983653 0.13382357 0.00833526 0.02412073
 0.00431919 0.03130427 0.0058302  0.00516008]
test_maes at epoch20: [0.20031738 0.06472785 0.14859836 0.28619802 0.06381069 0.09307107
 0.04476854 0.10345241 0.05390368 0.05353892]

train_loss at epoch21: 0.0008070713820609641
train_mses at epoch21: [0.11312424 0.02080377 0.02586618 0.03014695 0.02145103 0.01948164
 0.01939256 0.02529331 0.0183145  0.00670583]
train_maes at epoch21: [0.24694518 0.09493282 0.11184691 0.12106504 0.093324   0.09627966
 0.08998472 0.11567357 0.09308349 0.06049832]
test_loss at epoch21: 0.0006463560008225923
test_mses at epoch21: [0.06206891 0.00661293 0.02906618 0.06106827 0.00405052 0.00210666
 0.00342856 0.03858609 0.00444647 0.00265458]
test_maes at epoch21: [0.20126084 0.06021192 0.09262202 0.15953322 0.04641405 0.03329735
 0.04155757 0.11367629 0.04165388 0.02939221]

train_loss at epoch22: 0.0008040950732662323
train_mses at epoch22: [0.12620695 0.0200593  0.02573894 0.02796061 0.02008075 0.02405726
 0.01561589 0.02455329 0.0183987  0.00642765]
train_maes at epoch22: [0.26499396 0.09714472 0.11376253 0.11490052 0.09125272 0.09668837
 0.09031673 0.11334486 0.09198082 0.058461  ]
test_loss at epoch22: 0.0007616483249721375
test_mses at epoch22: [0.05633504 0.00599623 0.03164331 0.04362459 0.00624278 0.00814261
 0.01523884 0.06152045 0.00591369 0.00367875]
test_maes at epoch22: [0.19185109 0.05327693 0.08475937 0.12624635 0.05161506 0.05956217
 0.09135013 0.13396732 0.05581644 0.04165335]

train_loss at epoch23: 0.0008625411963526239
train_mses at epoch23: [0.1179653  0.02172809 0.02966143 0.02390916 0.01886782 0.02548142
 0.02561121 0.02581753 0.02149434 0.00611665]
train_maes at epoch23: [0.25719098 0.10297206 0.11889441 0.10784732 0.08999    0.10037763
 0.10104315 0.1076943  0.09717621 0.05865431]
test_loss at epoch23: 0.000885441623865924
test_mses at epoch23: [0.05677883 0.00832606 0.03680922 0.09233876 0.00619356 0.00832388
 0.01412753 0.03742597 0.00450561 0.00326974]
test_maes at epoch23: [0.19212071 0.05749111 0.09703656 0.23071751 0.05092236 0.05335754
 0.07841324 0.11781452 0.04652837 0.03982766]

train_loss at epoch24: 0.0007691155128339503
train_mses at epoch24: [0.1107156  0.02210695 0.02883302 0.02473637 0.01517326 0.02240761
 0.01967974 0.02068356 0.0187833  0.0068838 ]
train_maes at epoch24: [0.24413667 0.09559311 0.11668662 0.10429647 0.08402736 0.09463753
 0.09342031 0.10430979 0.09316858 0.0615192 ]
test_loss at epoch24: 0.0008804349942093199
test_mses at epoch24: [0.11857636 0.00956847 0.02019832 0.09465611 0.00543757 0.00428119
 0.01311449 0.03605037 0.0187952  0.00230216]
test_maes at epoch24: [0.29906475 0.05949353 0.09505095 0.24222024 0.05126466 0.0426249
 0.08417539 0.10815812 0.07949487 0.02695287]

train_loss at epoch25: 0.0008465978217885849
train_mses at epoch25: [0.13127297 0.01611783 0.03159521 0.0345019  0.01672845 0.02434758
 0.0210146  0.02281646 0.02042067 0.00638499]
train_maes at epoch25: [0.27230025 0.09083469 0.12553712 0.12034971 0.08453426 0.0994207
 0.09714082 0.10383104 0.1012863  0.05887794]
test_loss at epoch25: 0.0007518195349009747
test_mses at epoch25: [0.0805408  0.01535881 0.06007127 0.03724711 0.00656191 0.00479937
 0.0042687  0.03361438 0.00856537 0.00368931]
test_maes at epoch25: [0.2080138  0.06800256 0.118452   0.11691886 0.05995762 0.04439157
 0.04041369 0.1185565  0.05920665 0.04554394]

train_loss at epoch26: 0.0008068357653757359
train_mses at epoch26: [0.14085576 0.0208315  0.03065232 0.02634143 0.02380947 0.01869377
 0.01517898 0.02743751 0.01577231 0.00564579]
train_maes at epoch26: [0.27915791 0.09652797 0.11723374 0.11115573 0.0965677  0.09521251
 0.08600051 0.11827286 0.08691725 0.05611318]
test_loss at epoch26: 0.0005513262796275159
test_mses at epoch26: [0.08908021 0.00712292 0.01534618 0.04636756 0.00683463 0.01139115
 0.00499518 0.02688187 0.00634344 0.00174202]
test_maes at epoch26: [0.25382414 0.05334868 0.08555845 0.16611526 0.06062174 0.07757929
 0.05060427 0.09324366 0.0537535  0.02132811]

train_loss at epoch27: 0.0007623415202536481
train_mses at epoch27: [0.10451558 0.01718452 0.02805461 0.0244008  0.01888509 0.01810717
 0.02091939 0.02418777 0.01906944 0.00517014]
train_maes at epoch27: [0.24291965 0.08953902 0.11396901 0.10210714 0.08857857 0.08892571
 0.08898997 0.10983395 0.09733067 0.054102  ]
test_loss at epoch27: 0.0009017129468315459
test_mses at epoch27: [0.09527828 0.00713663 0.06255721 0.05640594 0.00363756 0.01737949
 0.00476609 0.04615199 0.01116566 0.00243011]
test_maes at epoch27: [0.26320992 0.06845954 0.12672741 0.15674686 0.04602607 0.07020526
 0.04411734 0.1133891  0.05991073 0.0336278 ]

train_loss at epoch28: 0.0007276817383442787
train_mses at epoch28: [0.0946853  0.01606355 0.02089706 0.02611597 0.01671579 0.02321
 0.01689529 0.02458007 0.01775286 0.00596632]
train_maes at epoch28: [0.22572506 0.08320458 0.09711503 0.10249373 0.08499849 0.09180107
 0.08639446 0.1113273  0.09048442 0.05516973]
test_loss at epoch28: 0.0004812097335432438
test_mses at epoch28: [0.06127985 0.0032397  0.01843526 0.03076607 0.01278337 0.0032786
 0.00315177 0.03276796 0.00617739 0.00165727]
test_maes at epoch28: [0.20758199 0.04021793 0.0848456  0.11670373 0.08971686 0.03972729
 0.03833782 0.09388519 0.05487565 0.02591406]

train_loss at epoch29: 0.0006611088053026097
train_mses at epoch29: [0.08190686 0.01972293 0.01945818 0.0243818  0.01821592 0.01244576
 0.01745818 0.01815173 0.01741584 0.00541788]
train_maes at epoch29: [0.21911903 0.09223773 0.10083945 0.1082211  0.09254361 0.0787643
 0.08743075 0.09468999 0.08756893 0.05261555]
test_loss at epoch29: 0.0007050900779505993
test_mses at epoch29: [0.05314606 0.01007904 0.03735537 0.05644795 0.00411879 0.00378198
 0.00499451 0.04461403 0.00387606 0.00241869]
test_maes at epoch29: [0.19096572 0.06527639 0.09328    0.17027858 0.0483247  0.04750688
 0.0449926  0.10645746 0.03969855 0.03375761]

train_loss at epoch30: 0.0006766858847534403
train_mses at epoch30: [0.10214127 0.01809772 0.02131787 0.01855673 0.01423006 0.01809724
 0.01961968 0.02380717 0.01694078 0.00483124]
train_maes at epoch30: [0.23465646 0.09236834 0.09786446 0.08979077 0.07662978 0.08504938
 0.08951662 0.1064147  0.0906891  0.05060615]
test_loss at epoch30: 0.0006319471099909316
test_mses at epoch30: [0.04682845 0.00596737 0.03549872 0.04215466 0.00320121 0.0075756
 0.00998074 0.03990673 0.00349541 0.00222698]
test_maes at epoch30: [0.17380316 0.0452945  0.08373935 0.14450444 0.0389129  0.04901523
 0.06988342 0.11488942 0.0450958  0.03077691]

train_loss at epoch31: 0.0006524874412633003
train_mses at epoch31: [0.090725   0.01759294 0.02097988 0.02308873 0.01549089 0.01613222
 0.01653576 0.0180419  0.01877083 0.0049344 ]
train_maes at epoch31: [0.23131305 0.08869039 0.10148749 0.10069791 0.07618834 0.08341495
 0.09086038 0.09473282 0.09311772 0.05064904]
test_loss at epoch31: 0.0007455236932381671
test_mses at epoch31: [0.06924369 0.0047084  0.01417037 0.09224614 0.00428061 0.00339726
 0.00812703 0.0451865  0.00221826 0.00188879]
test_maes at epoch31: [0.21993877 0.04447475 0.06170021 0.23386777 0.03998512 0.03872367
 0.06307372 0.11204454 0.03435432 0.03280191]

train_loss at epoch32: 0.0006493351323173402
train_mses at epoch32: [0.09546688 0.01386697 0.01853781 0.02457511 0.01525127 0.0126578
 0.02234737 0.0191605  0.01720949 0.00449992]
train_maes at epoch32: [0.23410179 0.0844136  0.09292402 0.09888074 0.07873701 0.07686365
 0.09485637 0.09704905 0.08884382 0.04922295]
test_loss at epoch32: 0.0008387217377411559
test_mses at epoch32: [0.06488248 0.01839484 0.03946133 0.07221968 0.01030378 0.00520392
 0.00267401 0.04334708 0.00552566 0.00201715]
test_maes at epoch32: [0.20940599 0.08237664 0.10234159 0.19911979 0.05327066 0.04011398
 0.03474531 0.10356116 0.04989252 0.03172707]

train_loss at epoch33: 0.0006234426170270493
train_mses at epoch33: [0.09753372 0.01749607 0.01888704 0.02105493 0.01503063 0.01806509
 0.01296784 0.02151954 0.01381652 0.0043002 ]
train_maes at epoch33: [0.22878866 0.09047457 0.09405803 0.09722509 0.07835084 0.08378075
 0.07640603 0.10329798 0.07964878 0.04722957]
test_loss at epoch33: 0.0007381734418425154
test_mses at epoch33: [0.04705179 0.01259629 0.05480347 0.03962991 0.00345954 0.00298411
 0.00792381 0.04642868 0.00566812 0.00217098]
test_maes at epoch33: [0.16474833 0.08223133 0.11113105 0.14006411 0.04564801 0.03437297
 0.04435848 0.09976014 0.04600116 0.0298143 ]

train_loss at epoch34: 0.0005979192126146023
train_mses at epoch34: [0.08049049 0.01429167 0.01888949 0.02164499 0.01336925 0.01442453
 0.01669725 0.01992781 0.01585756 0.00392923]
train_maes at epoch34: [0.21031276 0.0850297  0.09312485 0.09200945 0.0763557  0.07870417
 0.08392606 0.0960122  0.08231137 0.04533403]
test_loss at epoch34: 0.0004339159961710585
test_mses at epoch34: [0.06829783 0.00157537 0.01375116 0.03798674 0.00550954 0.00592247
 0.00663702 0.02489305 0.00197437 0.00184681]
test_maes at epoch34: [0.22299792 0.0286089  0.05742908 0.13755215 0.05444861 0.0532301
 0.04974101 0.08117703 0.03243927 0.03024767]

train_loss at epoch35: 0.0006428602369541817
train_mses at epoch35: [0.10056302 0.01665019 0.01844221 0.02023922 0.01854602 0.020108
 0.01827914 0.01601212 0.01634626 0.0042611 ]
train_maes at epoch35: [0.22925286 0.08323623 0.08892974 0.09459844 0.07794634 0.09228538
 0.08686381 0.08835561 0.08305735 0.04521745]
test_loss at epoch35: 0.0005778412790374553
test_mses at epoch35: [0.09592634 0.00371466 0.01723281 0.07033271 0.00431236 0.00285448
 0.00759394 0.02265414 0.00267364 0.00142599]
test_maes at epoch35: [0.25779656 0.04130795 0.06172164 0.19923812 0.03958527 0.03698678
 0.06906374 0.08227305 0.03923971 0.02506818]

train_loss at epoch36: 0.0005811313087952898
train_mses at epoch36: [0.07883176 0.01929795 0.01583894 0.02434563 0.01105294 0.01547671
 0.01348275 0.01667591 0.01429025 0.00424964]
train_maes at epoch36: [0.21315414 0.08321711 0.08816243 0.10061365 0.06949292 0.08003474
 0.08120934 0.08952509 0.08467119 0.04808703]
test_loss at epoch36: 0.0005812370555197939
test_mses at epoch36: [0.07094014 0.00529641 0.01133771 0.06588011 0.00352583 0.00502445
 0.01114668 0.02759893 0.00428178 0.00194277]
test_maes at epoch36: [0.22634821 0.05437558 0.05720916 0.19138943 0.03661161 0.0449085
 0.08082548 0.08757079 0.04667724 0.0344674 ]

train_loss at epoch37: 0.0005515130771759977
train_mses at epoch37: [0.08795956 0.01368129 0.0166961  0.01650811 0.01421026 0.01706192
 0.01333396 0.01552257 0.01489986 0.00393678]
train_maes at epoch37: [0.2106881  0.07539553 0.08933297 0.08703781 0.07801942 0.08168324
 0.07963422 0.08827643 0.08447593 0.04535248]
test_loss at epoch37: 0.0003699226116642673
test_mses at epoch37: [0.05693138 0.00307806 0.01330085 0.02835176 0.00601955 0.00470182
 0.00334017 0.01973511 0.0056772  0.00148803]
test_maes at epoch37: [0.19490583 0.03502705 0.06527996 0.11354048 0.05087591 0.05220805
 0.03962953 0.0766926  0.04045417 0.02850165]

train_loss at epoch38: 0.000593301025714646
train_mses at epoch38: [0.09161537 0.01571193 0.01857966 0.02175471 0.01473667 0.01789777
 0.01027208 0.01850888 0.01407657 0.00466369]
train_maes at epoch38: [0.23152816 0.07894268 0.09421217 0.10021307 0.07928197 0.08120624
 0.07102135 0.09169684 0.07787845 0.04776433]
test_loss at epoch38: 0.0008733833387018519
test_mses at epoch38: [0.06836113 0.0069393  0.04433465 0.08860823 0.00347711 0.00322305
 0.00321204 0.0500113  0.00580751 0.00146813]
test_maes at epoch38: [0.21182468 0.05296793 0.1021492  0.22473922 0.04102823 0.03782663
 0.03438313 0.10701515 0.04340223 0.02594576]

train_loss at epoch39: 0.0005845751771901516
train_mses at epoch39: [0.08478827 0.01837458 0.01621704 0.02210135 0.01641679 0.01274312
 0.0153806  0.01712721 0.0123148  0.00397876]
train_maes at epoch39: [0.21604325 0.08853685 0.08532679 0.09176234 0.07852756 0.07240602
 0.08206832 0.08989456 0.07769644 0.04601493]
test_loss at epoch39: 0.00038847371816952176
test_mses at epoch39: [0.0464829  0.00581432 0.0152544  0.02669142 0.00310311 0.00287528
 0.0044366  0.02859068 0.00190509 0.00209807]
test_maes at epoch39: [0.16856489 0.04892151 0.06095389 0.10476007 0.03943154 0.03358525
 0.04655481 0.08556196 0.03027621 0.03579587]

train_loss at epoch40: 0.0005501470033158647
train_mses at epoch40: [0.0833899  0.01139901 0.01369996 0.02146169 0.01418526 0.01054305
 0.01943187 0.01997733 0.01240915 0.00378325]
train_maes at epoch40: [0.21494698 0.07314338 0.07880128 0.09212123 0.07632949 0.06967746
 0.0866492  0.09551002 0.07602811 0.0441665 ]
test_loss at epoch40: 0.0005195570168422258
test_mses at epoch40: [0.09913556 0.00502986 0.01689953 0.06308735 0.0030212  0.00504825
 0.00416471 0.01574435 0.00375162 0.00131103]
test_maes at epoch40: [0.26397532 0.0398998  0.06395792 0.1808129  0.04439854 0.05082524
 0.04715805 0.06363661 0.04524676 0.02663701]

train_loss at epoch41: 0.0005599953214380335
train_mses at epoch41: [0.08739626 0.01867026 0.01409409 0.01649546 0.01459413 0.01359042
 0.01188519 0.01738639 0.01882545 0.00350746]
train_maes at epoch41: [0.22322145 0.09792419 0.0842466  0.09138035 0.08176551 0.07252092
 0.07540515 0.08455716 0.08282146 0.04422687]
test_loss at epoch41: 0.0006698081903952234
test_mses at epoch41: [0.13151209 0.00453404 0.01604718 0.09674261 0.00518323 0.00559876
 0.00306618 0.01512817 0.00387553 0.00077951]
test_maes at epoch41: [0.31028179 0.04124408 0.07694829 0.23390647 0.0432265  0.05413815
 0.03798123 0.06768055 0.03844895 0.01751882]

train_loss at epoch42: 0.0006318439154865894
train_mses at epoch42: [0.10736335 0.02121166 0.01784823 0.02746087 0.01161839 0.01458457
 0.01437733 0.01784801 0.01629624 0.00375544]
train_maes at epoch42: [0.23789031 0.09688263 0.08924019 0.1025558  0.07252717 0.07791076
 0.07846235 0.09298842 0.09325104 0.04455026]
test_loss at epoch42: 0.0005346594774659644
test_mses at epoch42: [0.05017354 0.00671198 0.03532889 0.02362871 0.0135995  0.00242312
 0.00290514 0.03317366 0.00658728 0.00127542]
test_maes at epoch42: [0.17007047 0.05701439 0.07897523 0.08553033 0.05427077 0.03465322
 0.03874337 0.09080422 0.05822278 0.02172789]

train_loss at epoch43: 0.0006384565336431594
train_mses at epoch43: [0.08616998 0.01626229 0.01784994 0.025163   0.02189907 0.01492865
 0.01286381 0.01627833 0.0175577  0.00369535]
train_maes at epoch43: [0.21074155 0.07949998 0.09135598 0.09911503 0.0876722  0.07753613
 0.07593736 0.08921805 0.08130424 0.04286908]
test_loss at epoch43: 0.0003150686671204389
test_mses at epoch43: [0.06541082 0.00320579 0.00694501 0.02801351 0.00370078 0.00267534
 0.00179097 0.02197505 0.0025515  0.00067592]
test_maes at epoch43: [0.21127033 0.04231861 0.04897471 0.11668623 0.03924959 0.03202202
 0.02889057 0.08045145 0.02958296 0.01759892]

train_loss at epoch44: 0.0005503663377083363
train_mses at epoch44: [0.08004061 0.01237183 0.02443144 0.01461367 0.01578632 0.01255533
 0.01285119 0.01634807 0.01405181 0.00422825]
train_maes at epoch44: [0.21145276 0.08151957 0.10116443 0.08302299 0.07813365 0.07333681
 0.07514614 0.08753084 0.07687268 0.04576829]
test_loss at epoch44: 0.0006749527132574548
test_mses at epoch44: [0.05772775 0.00504157 0.02577455 0.07204694 0.00528405 0.00402715
 0.00614169 0.03639235 0.00443671 0.00125282]
test_maes at epoch44: [0.1934351  0.04079798 0.07841385 0.19939693 0.0415035  0.0359345
 0.04513854 0.09004514 0.04688738 0.02434755]

train_loss at epoch45: 0.0005950376668826063
train_mses at epoch45: [0.08847128 0.01254789 0.03191049 0.01494278 0.01650315 0.01214526
 0.01471753 0.01459747 0.01682847 0.00375034]
train_maes at epoch45: [0.23073503 0.0791828  0.11397371 0.08155602 0.0705829  0.07241595
 0.07824699 0.08028242 0.0741876  0.04321776]
test_loss at epoch45: 0.0006270028660668338
test_mses at epoch45: [0.07419066 0.00236836 0.04012597 0.06979519 0.00454666 0.00188082
 0.00830377 0.01556226 0.00273399 0.00198953]
test_maes at epoch45: [0.21691076 0.0367908  0.10513049 0.18793196 0.04120342 0.02587553
 0.06053788 0.06418611 0.03151286 0.03676842]

train_loss at epoch46: 0.0005416390763476808
train_mses at epoch46: [0.07335136 0.01227172 0.01712889 0.02034974 0.00970893 0.01650032
 0.01399023 0.01979012 0.01233825 0.00414537]
train_maes at epoch46: [0.20844385 0.07699086 0.08685101 0.09252985 0.06694372 0.07943753
 0.07405121 0.09320167 0.07319836 0.04636838]
test_loss at epoch46: 0.0004109399185139448
test_mses at epoch46: [0.05338453 0.00198884 0.0151531  0.0522318  0.00436579 0.00313254
 0.00583864 0.00962224 0.00246308 0.00078342]
test_maes at epoch46: [0.18666098 0.03104158 0.06642952 0.16852405 0.04147295 0.03722707
 0.04614566 0.0599304  0.03677683 0.0190765 ]

train_loss at epoch47: 0.0004966025300165441
train_mses at epoch47: [0.06783789 0.01279981 0.01535407 0.01376052 0.01587826 0.01227124
 0.01138236 0.01622156 0.01414341 0.00319589]
train_maes at epoch47: [0.19293646 0.07059603 0.08806086 0.07680788 0.07925252 0.07429111
 0.07079302 0.08293585 0.07470603 0.04127081]
test_loss at epoch47: 0.0003677224225186287
test_mses at epoch47: [0.02345439 0.00225838 0.0244254  0.03171801 0.00475862 0.00528476
 0.00191994 0.009022   0.00620481 0.00227284]
test_maes at epoch47: [0.11875631 0.0311854  0.08550526 0.11512282 0.04087302 0.05406325
 0.02911013 0.05936757 0.04022418 0.03880628]

train_loss at epoch48: 0.0005818326402693353
train_mses at epoch48: [0.09513386 0.01648296 0.01909021 0.01590541 0.02180505 0.01256464
 0.01392968 0.0144043  0.014068   0.00341734]
train_maes at epoch48: [0.22501587 0.07973803 0.09316091 0.08107504 0.08550636 0.07291837
 0.07692138 0.08067168 0.07408302 0.04385627]
test_loss at epoch48: 0.0005412743595290058
test_mses at epoch48: [0.05690497 0.00255594 0.04185152 0.05792848 0.00446294 0.00136931
 0.00103641 0.01381506 0.00263062 0.00120699]
test_maes at epoch48: [0.19196954 0.03441192 0.11828401 0.17335332 0.05417108 0.02546319
 0.02325578 0.06661974 0.03341938 0.02545727]

train_loss at epoch49: 0.0004812597435839633
train_mses at epoch49: [0.07085363 0.01288192 0.01796305 0.01498074 0.01012568 0.01278083
 0.01278718 0.01552038 0.0107819  0.00382071]
train_maes at epoch49: [0.19625296 0.07538695 0.09008862 0.08033129 0.06549149 0.07196657
 0.07468282 0.0838834  0.07135942 0.04338488]
test_loss at epoch49: 0.00033004580640253866
test_mses at epoch49: [0.03348045 0.00485041 0.02015983 0.02231819 0.00258488 0.0017441
 0.00185141 0.01910146 0.00390678 0.00146751]
test_maes at epoch49: [0.13631782 0.05068132 0.06607998 0.10100642 0.03151342 0.03010425
 0.03258089 0.06614873 0.0372457  0.03058774]

