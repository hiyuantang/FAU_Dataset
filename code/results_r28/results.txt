train_loss at epoch0: 0.03222218813731315
train_mses at epoch0: [3.10721086 1.00524811 1.15666236 0.95887273 0.98850845 0.36841439
 1.04648449 1.05229234 1.01689323 0.02427415]
train_maes at epoch0: [1.51111146 0.51259154 0.63317657 0.56413235 0.48205145 0.32926989
 0.45710469 0.52174644 0.54449828 0.06760995]
test_loss at epoch0: 0.025877296290499098
test_mses at epoch0: [1.8700707  0.79618791 1.1186864  0.65189575 0.81496332 0.03988942
 0.87419331 0.98332103 0.84869396 0.02105194]
test_maes at epoch0: [1.17947927 0.53592663 0.54221321 0.42083834 0.5085276  0.14641014
 0.47343107 0.46533112 0.43041811 0.03866615]

train_loss at epoch1: 0.020081085529416166
train_mses at epoch1: [1.31126827 0.5570677  0.9508817  0.61095539 0.58421834 0.13353888
 0.60645086 0.75014236 0.59474035 0.03624794]
train_maes at epoch1: [0.92927296 0.45676001 0.56276186 0.48140342 0.4644296  0.27891358
 0.4507971  0.48248206 0.44292084 0.12137276]
test_loss at epoch1: 0.010662024166989834
test_mses at epoch1: [0.70658729 0.19288011 0.66213231 0.32255849 0.41468215 0.05444517
 0.17055098 0.52469677 0.16282153 0.02286638]
test_maes at epoch1: [0.64789147 0.31425438 0.44678157 0.39832432 0.52006428 0.16835722
 0.22928438 0.44257518 0.29276015 0.1021608 ]

train_loss at epoch2: 0.008934734151401419
train_mses at epoch2: [0.58380387 0.18035    0.62735191 0.34646605 0.23978732 0.06999246
 0.13453766 0.35239885 0.1628792  0.03889731]
train_maes at epoch2: [0.5786347  0.29747459 0.48009184 0.3936369  0.35353914 0.19879709
 0.25152736 0.41824418 0.29845277 0.13901224]
test_loss at epoch2: 0.004650967908983535
test_mses at epoch2: [0.37459354 0.05953954 0.3723948  0.2203648  0.07264772 0.04343923
 0.02212795 0.25258476 0.03389093 0.01664607]
test_maes at epoch2: [0.4344246  0.18220418 0.33146078 0.29486196 0.20413753 0.15858891
 0.10780474 0.31401594 0.12727644 0.07649461]

train_loss at epoch3: 0.0046560850905928205
train_mses at epoch3: [0.45617957 0.08470756 0.31415923 0.22256648 0.0969332  0.06860347
 0.06421212 0.14458212 0.07578124 0.03037931]
train_maes at epoch3: [0.49699102 0.20918984 0.35908406 0.30833148 0.22450649 0.1874914
 0.17998827 0.28520258 0.19604907 0.12038962]
test_loss at epoch3: 0.0017158220264505834
test_mses at epoch3: [0.20407618 0.01940363 0.11421703 0.06654709 0.03319275 0.02909089
 0.011284   0.09567598 0.01441536 0.01279988]
test_maes at epoch3: [0.331142   0.09981418 0.19093235 0.14725673 0.12680702 0.11962251
 0.08065518 0.2090799  0.08311864 0.05843504]

train_loss at epoch4: 0.0025064853426227545
train_mses at epoch4: [0.34233276 0.05061409 0.11323771 0.09415184 0.0697458  0.04223718
 0.05177936 0.07470073 0.0618725  0.0236623 ]
train_maes at epoch4: [0.43780472 0.16208568 0.23936915 0.19985902 0.17836733 0.14589012
 0.16668521 0.19934137 0.17642081 0.10445647]
test_loss at epoch4: 0.0008154397810551714
test_mses at epoch4: [0.1984241  0.00832815 0.0640738  0.01975675 0.01175131 0.00740762
 0.0087981  0.03657239 0.01107331 0.01204707]
test_maes at epoch4: [0.37106953 0.06761204 0.18972692 0.08899055 0.07722171 0.05899028
 0.06877895 0.11334654 0.07088801 0.05612618]

train_loss at epoch5: 0.0019477386463512766
train_mses at epoch5: [0.30865002 0.05019948 0.07290639 0.07696831 0.0448408  0.03220814
 0.04309623 0.06611722 0.04127835 0.02102064]
train_maes at epoch5: [0.43040296 0.15522409 0.19388165 0.17702066 0.14751505 0.12906433
 0.14235737 0.17651863 0.14121981 0.09964536]
test_loss at epoch5: 0.0005944400352049381
test_mses at epoch5: [0.09222773 0.01280369 0.03100921 0.01919803 0.00820646 0.00918339
 0.0107195  0.02712611 0.00970576 0.00905165]
test_maes at epoch5: [0.23039903 0.08559428 0.1342181  0.10131279 0.06141261 0.07614549
 0.06665428 0.10560749 0.06927728 0.0389165 ]

train_loss at epoch6: 0.0016197308184618645
train_mses at epoch6: [0.27366631 0.04384487 0.06158822 0.057115   0.03196202 0.03196729
 0.03047168 0.05206809 0.03959966 0.01947116]
train_maes at epoch6: [0.39312413 0.14816837 0.17149451 0.16196269 0.12033317 0.12496842
 0.12559993 0.16571167 0.13444323 0.09490839]
test_loss at epoch6: 0.0007763741924328373
test_mses at epoch6: [0.15474699 0.02492905 0.02205005 0.05871741 0.00564575 0.00447481
 0.00886632 0.03116161 0.00848082 0.00941196]
test_maes at epoch6: [0.28274471 0.10952142 0.11226896 0.15060497 0.05317336 0.04737121
 0.06738013 0.14200538 0.06788557 0.04421577]

train_loss at epoch7: 0.0013323364323599542
train_mses at epoch7: [0.20120125 0.03343256 0.04605648 0.05098135 0.02852576 0.02602945
 0.03448618 0.03741538 0.03390016 0.01616467]
train_maes at epoch7: [0.34165343 0.12994057 0.15826969 0.15385549 0.11536112 0.11233203
 0.13351659 0.14492575 0.12575939 0.08840859]
test_loss at epoch7: 0.0004193918244477282
test_mses at epoch7: [0.08171123 0.00724283 0.01078603 0.03429276 0.00443059 0.00530746
 0.00417395 0.01783226 0.00332172 0.00657499]
test_maes at epoch7: [0.22703074 0.06481423 0.06938932 0.12424005 0.04681704 0.04902139
 0.03968009 0.08882117 0.0383976  0.03282018]

train_loss at epoch8: 0.0013418952969120855
train_mses at epoch8: [0.1853499  0.03360632 0.04779274 0.05704642 0.03257233 0.02250725
 0.03372362 0.03700392 0.03082857 0.01514365]
train_maes at epoch8: [0.32831209 0.12753155 0.15746389 0.16112428 0.11771635 0.10418886
 0.12762036 0.13754694 0.1236185  0.08511362]
test_loss at epoch8: 0.0005264211138908534
test_mses at epoch8: [0.08307196 0.01806194 0.02243344 0.02287607 0.0031204  0.01311801
 0.00626906 0.01538115 0.01289564 0.00595845]
test_maes at epoch8: [0.23765026 0.08749123 0.08544207 0.12066844 0.04090089 0.06866902
 0.0493896  0.07619209 0.07779069 0.03855093]

train_loss at epoch9: 0.0010987261255727486
train_mses at epoch9: [0.15087626 0.02880074 0.03637621 0.03742688 0.02256654 0.02546811
 0.03078991 0.03288486 0.0271296  0.0124207 ]
train_maes at epoch9: [0.29341749 0.11317352 0.13459211 0.13383882 0.10002928 0.10952658
 0.11494736 0.12714852 0.11576345 0.0788529 ]
test_loss at epoch9: 0.0002444521920319567
test_mses at epoch9: [0.05206534 0.00314711 0.01362246 0.00855905 0.0049462  0.00371663
 0.00463498 0.00971475 0.00359738 0.00296787]
test_maes at epoch9: [0.17808735 0.03776789 0.09434217 0.06110982 0.04632998 0.03959787
 0.04468868 0.06762235 0.04587976 0.02571208]

train_loss at epoch10: 0.0010808006726562025
train_mses at epoch10: [0.16688709 0.0233276  0.04252492 0.04055913 0.02594176 0.02469712
 0.02157169 0.03139748 0.02662315 0.0115119 ]
train_maes at epoch10: [0.31230873 0.10686665 0.14492219 0.13333964 0.10396268 0.10778622
 0.10184972 0.12709281 0.11388324 0.07655827]
test_loss at epoch10: 0.0003729907478740875
test_mses at epoch10: [0.05433163 0.01182396 0.00897458 0.01277004 0.00780864 0.00629844
 0.00752433 0.01698025 0.01099762 0.00287785]
test_maes at epoch10: [0.17854327 0.06604087 0.07501346 0.07439695 0.05997575 0.06312222
 0.0527249  0.07786921 0.06139055 0.02412364]

train_loss at epoch11: 0.0008930184363209187
train_mses at epoch11: [0.1265702  0.021859   0.02742884 0.03212035 0.01875689 0.0219505
 0.02630943 0.02746263 0.02299613 0.00886899]
train_maes at epoch11: [0.27090841 0.10434085 0.11750186 0.12170393 0.09425466 0.09774194
 0.10559052 0.11824308 0.10606375 0.07020935]
test_loss at epoch11: 0.00023587923645576898
test_mses at epoch11: [0.03703511 0.0041521  0.00719104 0.01011828 0.0028282  0.00512309
 0.00562166 0.00863193 0.00817778 0.00214637]
test_maes at epoch11: [0.13332812 0.041884   0.05959083 0.06493198 0.03804108 0.04823341
 0.04702114 0.06980961 0.05563823 0.02584806]

train_loss at epoch12: 0.0009090145604011226
train_mses at epoch12: [0.12809243 0.01975723 0.02736057 0.02892717 0.02097162 0.02640152
 0.02714874 0.02862229 0.02332527 0.00762714]
train_maes at epoch12: [0.2734434  0.09719392 0.11768907 0.11623227 0.09500465 0.10325134
 0.11100099 0.11941795 0.10289712 0.06399348]
test_loss at epoch12: 0.00037142803317847406
test_mses at epoch12: [0.04023182 0.00352664 0.00821295 0.00962757 0.00523744 0.01615608
 0.0067569  0.01766667 0.0174517  0.00214681]
test_maes at epoch12: [0.14304844 0.04045163 0.05596093 0.05913508 0.05168442 0.06162373
 0.04931727 0.07999489 0.06371606 0.02363194]

train_loss at epoch13: 0.0009066527288288195
train_mses at epoch13: [0.13294102 0.02142533 0.02687394 0.03344555 0.02163697 0.02276638
 0.01907054 0.02909369 0.02677858 0.00762824]
train_maes at epoch13: [0.28127885 0.0999044  0.1110526  0.11861064 0.09108892 0.09904101
 0.09335133 0.11803398 0.11201913 0.06260248]
test_loss at epoch13: 0.00017144831709880779
test_mses at epoch13: [0.04334054 0.00196955 0.00633528 0.0049458  0.00313503 0.00752624
 0.00264036 0.00571528 0.00410726 0.00151657]
test_maes at epoch13: [0.15314846 0.03399927 0.04650333 0.04467774 0.03453653 0.04707224
 0.03300037 0.05369598 0.04840173 0.020596  ]

train_loss at epoch14: 0.0007937760092318058
train_mses at epoch14: [0.12556468 0.01991683 0.0245767  0.02652397 0.02338333 0.02011683
 0.01702282 0.02371277 0.02304045 0.00608417]
train_maes at epoch14: [0.26912004 0.09250021 0.10912162 0.10947146 0.09405017 0.0932357
 0.08933399 0.10953101 0.1024515  0.05758127]
test_loss at epoch14: 0.00027164487425792726
test_mses at epoch14: [0.04257865 0.00374505 0.00865604 0.01124977 0.00287456 0.00369114
 0.00867296 0.01121965 0.01029512 0.00146179]
test_maes at epoch14: [0.154108   0.04850841 0.04974563 0.06716329 0.03162653 0.04566751
 0.05445706 0.06965806 0.06134557 0.01672907]

train_loss at epoch15: 0.0008110548939278469
train_mses at epoch15: [0.12532872 0.02325147 0.03407754 0.0249311  0.01526222 0.01947578
 0.01907977 0.02343073 0.02055799 0.00632473]
train_maes at epoch15: [0.26935125 0.10108755 0.1220829  0.10836496 0.08115654 0.0910721
 0.09613776 0.10954574 0.09635669 0.05751308]
test_loss at epoch15: 0.0001810435725851579
test_mses at epoch15: [0.0304099  0.00141576 0.0102364  0.00725893 0.00499237 0.00121638
 0.00150685 0.00529067 0.00791717 0.00124475]
test_maes at epoch15: [0.13967482 0.02988709 0.07489503 0.05392467 0.04049142 0.02643016
 0.02727731 0.05337825 0.05742139 0.01963266]

train_loss at epoch16: 0.00079010890182504
train_mses at epoch16: [0.1159557  0.01752935 0.02503858 0.0242234  0.02078887 0.02121832
 0.02194223 0.02626294 0.0196799  0.00565517]
train_maes at epoch16: [0.25202205 0.08871424 0.11410381 0.10654302 0.09030878 0.09264453
 0.09580644 0.10740673 0.09258809 0.05509847]
test_loss at epoch16: 0.00043108235509630214
test_mses at epoch16: [0.06546221 0.00747053 0.01826197 0.01107832 0.02725684 0.00192705
 0.01437369 0.00492223 0.01314622 0.0011286 ]
test_maes at epoch16: [0.18378808 0.04727334 0.0621317  0.06805224 0.07048928 0.03514838
 0.07022688 0.0451472  0.05381625 0.01705387]

train_loss at epoch17: 0.0008367518522836109
train_mses at epoch17: [0.11584815 0.01813504 0.0280598  0.03367806 0.02048984 0.01733046
 0.02192776 0.02586262 0.02276936 0.00626559]
train_maes at epoch17: [0.2551913  0.09216315 0.11497874 0.11574485 0.08837718 0.09116532
 0.09147669 0.11047372 0.09777525 0.0556503 ]
test_loss at epoch17: 0.00028507596506004005
test_mses at epoch17: [0.03389384 0.00507115 0.00501518 0.02398322 0.00244911 0.01422065
 0.00129151 0.0060137  0.0073294  0.00110819]
test_maes at epoch17: [0.14257718 0.04739806 0.04191636 0.10954389 0.03152595 0.060019
 0.02671856 0.05625617 0.06271605 0.01964095]

train_loss at epoch18: 0.0007138831164450087
train_mses at epoch18: [0.11428718 0.01561437 0.02070455 0.02763125 0.01380132 0.02159569
 0.01897    0.02330027 0.01749795 0.00559892]
train_maes at epoch18: [0.25248546 0.08786089 0.09854466 0.11620355 0.07964259 0.09188218
 0.0895041  0.10822003 0.09012367 0.05458766]
test_loss at epoch18: 0.0002890078950316665
test_mses at epoch18: [0.04509963 0.00785299 0.00975367 0.00722821 0.00916037 0.01069268
 0.00398045 0.00939629 0.00781901 0.00083234]
test_maes at epoch18: [0.16177324 0.05066392 0.05030441 0.04670784 0.04816612 0.05155271
 0.0384677  0.05766429 0.04281658 0.01398777]

train_loss at epoch19: 0.0006751351831282707
train_mses at epoch19: [0.1013655  0.02006414 0.02100307 0.02061913 0.01525009 0.0183838
 0.01743369 0.01952015 0.01837543 0.00486183]
train_maes at epoch19: [0.23499956 0.08649852 0.0978976  0.09727021 0.07988387 0.08828221
 0.08786689 0.09582239 0.08496845 0.05118507]
test_loss at epoch19: 9.921335174325616e-05
test_mses at epoch19: [0.01965934 0.00322738 0.00171546 0.00464783 0.00303496 0.00192954
 0.00101126 0.00508003 0.00102288 0.00065174]
test_maes at epoch19: [0.1116445  0.04542321 0.02990942 0.0450306  0.04057764 0.02745905
 0.02252621 0.04707546 0.02494582 0.01402348]

train_loss at epoch20: 0.0006560157721941459
train_mses at epoch20: [0.09817187 0.01443816 0.02441874 0.02033269 0.01322108 0.0167568
 0.01319609 0.02508801 0.01914753 0.00485854]
train_maes at epoch20: [0.2418651  0.08261162 0.10593622 0.09776699 0.0760843  0.08412148
 0.07902548 0.10458207 0.09122665 0.05014489]
test_loss at epoch20: 0.00021953238768780486
test_mses at epoch20: [0.06293589 0.00324254 0.01856831 0.00489781 0.0022246  0.00712622
 0.00173502 0.00359275 0.00509668 0.00070303]
test_maes at epoch20: [0.18747239 0.04384566 0.07769044 0.04135261 0.03172101 0.04408292
 0.02672418 0.04064471 0.04169905 0.0183203 ]

train_loss at epoch21: 0.000637378927538211
train_mses at epoch21: [0.08703915 0.0177555  0.02247848 0.0170928  0.01883272 0.01620869
 0.01692548 0.01669089 0.01703746 0.00473091]
train_maes at epoch21: [0.22061862 0.08331463 0.10199615 0.08707977 0.08150422 0.08033067
 0.08319848 0.08886663 0.08524335 0.05018699]
test_loss at epoch21: 0.00024185071580429027
test_mses at epoch21: [0.04047754 0.00202209 0.01039593 0.01812801 0.00516502 0.0039199
 0.0033643  0.00881284 0.00320144 0.00041451]
test_maes at epoch21: [0.1538049  0.03252308 0.05122653 0.07727538 0.03841735 0.03773043
 0.0366558  0.0539716  0.03111056 0.01272189]

train_loss at epoch22: 0.0006517779213336712
train_mses at epoch22: [0.09591186 0.01784773 0.01958564 0.02183814 0.01620424 0.01803272
 0.01645517 0.01886764 0.0178737  0.00379752]
train_maes at epoch22: [0.22600072 0.08552086 0.09612198 0.09750251 0.07688995 0.08617124
 0.08191786 0.09245531 0.08347356 0.04511425]
test_loss at epoch22: 0.0001259933453687328
test_mses at epoch22: [0.01694137 0.0028997  0.00693791 0.00325539 0.00160051 0.00365939
 0.00234349 0.00439754 0.00311297 0.00074918]
test_maes at epoch22: [0.09138875 0.03366736 0.05872222 0.04073363 0.02862056 0.03964146
 0.03139215 0.04141296 0.03238227 0.01427709]

train_loss at epoch23: 0.0005927331748101464
train_mses at epoch23: [0.08216505 0.01462618 0.02147031 0.0141149  0.0169688  0.01971697
 0.01513581 0.01631908 0.01499804 0.00398722]
train_maes at epoch23: [0.21351864 0.07966031 0.10084593 0.08115004 0.08290787 0.08745652
 0.08038365 0.09020396 0.07883317 0.04623418]
test_loss at epoch23: 0.00011712161234599796
test_mses at epoch23: [0.0221608  0.00099926 0.00547275 0.00542793 0.00434577 0.00430705
 0.00140425 0.00287276 0.00128028 0.00040998]
test_maes at epoch23: [0.12090692 0.02265191 0.05514568 0.03900296 0.04077172 0.03902229
 0.02811639 0.03893322 0.02573914 0.01468751]

train_loss at epoch24: 0.0005567386075160764
train_mses at epoch24: [0.08362724 0.01579762 0.01627945 0.01650706 0.01422542 0.01155307
 0.01943435 0.01548013 0.01421224 0.00401685]
train_maes at epoch24: [0.21934389 0.08117864 0.08416695 0.08435902 0.07735716 0.07229641
 0.083365   0.08949431 0.0805246  0.04548223]
test_loss at epoch24: 8.852196550194887e-05
test_mses at epoch24: [0.01300308 0.00413137 0.00178787 0.00231373 0.00441411 0.00222299
 0.00141843 0.00176644 0.00198372 0.00042338]
test_maes at epoch24: [0.09109479 0.03497573 0.03225057 0.03162123 0.03812236 0.03279656
 0.02621749 0.02852794 0.03880337 0.01461895]

train_loss at epoch25: 0.0005946907195005011
train_mses at epoch25: [0.08630943 0.01576172 0.01465058 0.02132662 0.01369646 0.01627128
 0.01701288 0.01745822 0.01832376 0.00385406]
train_maes at epoch25: [0.21653277 0.08039779 0.08494763 0.09618094 0.07700067 0.08181773
 0.07982661 0.08911749 0.08755849 0.0442827 ]
test_loss at epoch25: 0.00047901355502928824
test_mses at epoch25: [0.07355355 0.01392853 0.01261035 0.04670239 0.00525621 0.00428148
 0.00826973 0.01242769 0.00523675 0.0005538 ]
test_maes at epoch25: [0.20113609 0.06100542 0.06444916 0.1357533  0.05015326 0.04178785
 0.04554882 0.06086173 0.06010555 0.01542481]

train_loss at epoch26: 0.0005603366992139119
train_mses at epoch26: [0.0851398  0.01589315 0.01850762 0.02269137 0.0156057  0.01198901
 0.01403868 0.01542542 0.01254414 0.00330552]
train_maes at epoch26: [0.21157274 0.08602126 0.09001552 0.09718934 0.07352096 0.06903754
 0.07896507 0.08808054 0.07745683 0.04107569]
test_loss at epoch26: 0.00013601123429636688
test_mses at epoch26: [0.01624166 0.00499125 0.00588092 0.00812961 0.00226386 0.00177975
 0.00337569 0.00339339 0.00104807 0.00061939]
test_maes at epoch26: [0.1009002  0.05437051 0.05153124 0.0721109  0.03174754 0.0278662
 0.04231897 0.03797997 0.02425848 0.01819187]

train_loss at epoch27: 0.0006014326998488383
train_mses at epoch27: [0.09416212 0.02178272 0.02127716 0.01932822 0.01650378 0.0107787
 0.01234271 0.01700618 0.01326187 0.00344237]
train_maes at epoch27: [0.22925884 0.08929757 0.09662539 0.09131931 0.07659134 0.06907025
 0.07161058 0.087556   0.07749377 0.04202067]
test_loss at epoch27: 0.00011611434363542085
test_mses at epoch27: [0.02278229 0.0080339  0.00326141 0.00167802 0.00490743 0.00172965
 0.00246035 0.00169078 0.00199652 0.00027819]
test_maes at epoch27: [0.11778728 0.06457335 0.03858193 0.02970345 0.03669549 0.02967121
 0.02665872 0.02939203 0.0366097  0.01051347]

train_loss at epoch28: 0.0005701887854771253
train_mses at epoch28: [0.0897918  0.01864796 0.02371432 0.01340659 0.01260206 0.01419693
 0.01611127 0.0154653  0.01323862 0.00337262]
train_maes at epoch28: [0.22173288 0.08941908 0.09634951 0.07844008 0.06694387 0.07516624
 0.07784855 0.08409898 0.07184023 0.04132496]
test_loss at epoch28: 9.135563208899917e-05
test_mses at epoch28: [0.01173088 0.00290243 0.00207974 0.00256541 0.00188537 0.00263274
 0.00099138 0.00263434 0.00514005 0.00048532]
test_maes at epoch28: [0.08498462 0.03250832 0.03286489 0.03327641 0.02428489 0.03020938
 0.02110779 0.03817457 0.04304563 0.01422515]

train_loss at epoch29: 0.0005387076699333464
train_mses at epoch29: [0.08360401 0.01767384 0.01456885 0.01538302 0.01411402 0.01569394
 0.01435379 0.01566624 0.01283061 0.00309132]
train_maes at epoch29: [0.21145289 0.07877315 0.08316669 0.08005529 0.07237667 0.07664637
 0.07417493 0.08457463 0.07542333 0.03958145]
test_loss at epoch29: 0.00016235536152615825
test_mses at epoch29: [0.02537241 0.01553507 0.00156453 0.00456903 0.00149995 0.00114735
 0.00129297 0.00638915 0.00478106 0.00027332]
test_maes at epoch29: [0.12530929 0.07157496 0.02689766 0.04337695 0.02592042 0.02290345
 0.02588608 0.04780888 0.04570434 0.00949682]

train_loss at epoch30: 0.0005236910841863681
train_mses at epoch30: [0.0812447  0.01620694 0.01303954 0.01749996 0.0145672  0.01389933
 0.01364884 0.01438785 0.01279527 0.00317822]
train_maes at epoch30: [0.21229903 0.07739657 0.07906369 0.08641187 0.07017254 0.07226117
 0.0750627  0.08083601 0.07713444 0.04028801]
test_loss at epoch30: 0.00014366243564662464
test_mses at epoch30: [0.01750643 0.00982195 0.00187175 0.00504355 0.00084253 0.009353
 0.00089154 0.00164975 0.00375151 0.00024147]
test_maes at epoch30: [0.09690596 0.04802158 0.02567828 0.05329085 0.01983375 0.04709704
 0.02196154 0.02886436 0.04227302 0.01006056]

train_loss at epoch31: 0.0005701654739281598
train_mses at epoch31: [0.08760078 0.0171785  0.01751166 0.0209929  0.01535879 0.01431909
 0.01619692 0.01386011 0.0133569  0.00316943]
train_maes at epoch31: [0.21915785 0.08144885 0.08452821 0.09163816 0.07317757 0.07437007
 0.07886509 0.07930559 0.07289263 0.04033233]
test_loss at epoch31: 0.00011446889046520154
test_mses at epoch31: [0.03246626 0.00384218 0.00401716 0.00843292 0.00243351 0.00116309
 0.00139685 0.00246262 0.00069826 0.00024418]
test_maes at epoch31: [0.13369498 0.04266149 0.03843714 0.05510509 0.0374265  0.0265648
 0.02258752 0.03696854 0.02026879 0.01003312]

train_loss at epoch32: 0.0005235547903291089
train_mses at epoch32: [0.07060944 0.01206797 0.01315118 0.01801277 0.01276757 0.01416136
 0.0149941  0.01423529 0.01869363 0.00313617]
train_maes at epoch32: [0.20096648 0.07081707 0.07671498 0.08601712 0.06778959 0.07302008
 0.07468714 0.08041312 0.07709961 0.04019145]
test_loss at epoch32: 0.00010402788076193092
test_mses at epoch32: [0.00985647 0.00172275 0.00190642 0.00175682 0.00083673 0.0013866
 0.00193832 0.00349479 0.01091274 0.00024356]
test_maes at epoch32: [0.08243676 0.02880876 0.03211055 0.02840417 0.02047558 0.02147396
 0.03507018 0.03986271 0.04135087 0.01071807]

train_loss at epoch33: 0.0005299220854872243
train_mses at epoch33: [0.07696332 0.0149769  0.01284705 0.02289318 0.01337578 0.01217504
 0.01378304 0.01327744 0.01597764 0.0029341 ]
train_maes at epoch33: [0.20638039 0.07631926 0.07641109 0.09317637 0.07089648 0.07033564
 0.07343756 0.07955736 0.08301759 0.0395646 ]
test_loss at epoch33: 0.00010512152468746013
test_mses at epoch33: [0.00939078 0.00177616 0.00188242 0.0014212  0.00340148 0.0069326
 0.0020503  0.00348739 0.00359836 0.00033198]
test_maes at epoch33: [0.08203979 0.02251886 0.02862172 0.02480027 0.03495865 0.0500719
 0.03806965 0.03838634 0.04936808 0.01395552]

train_loss at epoch34: 0.000491188509647358
train_mses at epoch34: [0.06977891 0.01079286 0.01615056 0.01543931 0.01206092 0.01907318
 0.01460028 0.01218723 0.01060723 0.00260338]
train_maes at epoch34: [0.19515149 0.06901618 0.08482591 0.0788758  0.06876199 0.08072603
 0.07226062 0.07371072 0.07071472 0.03764089]
test_loss at epoch34: 0.00013550375637776674
test_mses at epoch34: [0.00911721 0.00148515 0.00144662 0.00304592 0.00152735 0.00457719
 0.00374208 0.00256455 0.01312497 0.00046769]
test_maes at epoch34: [0.07457364 0.02072414 0.03044338 0.04355453 0.02955597 0.04575614
 0.04270304 0.03266692 0.04809854 0.01361499]

train_loss at epoch35: 0.0005146021659961565
train_mses at epoch35: [0.07557953 0.01168287 0.01690244 0.01426361 0.01215685 0.0125808
 0.01162172 0.01487135 0.02278801 0.0028429 ]
train_maes at epoch35: [0.20154861 0.0709506  0.08646298 0.07798153 0.06858179 0.07159281
 0.07091056 0.08103791 0.09420679 0.03858367]
test_loss at epoch35: 0.0001054683370773304
test_mses at epoch35: [0.01457317 0.00902257 0.00106163 0.00217248 0.00095435 0.0040072
 0.00331806 0.00263483 0.00106176 0.00024721]
test_maes at epoch35: [0.09067759 0.06643019 0.01897475 0.03308209 0.0246249  0.04012921
 0.03201716 0.03480253 0.02527945 0.01206887]

train_loss at epoch36: 0.0004901769372852559
train_mses at epoch36: [0.06801687 0.01567375 0.01389518 0.01536963 0.0115129  0.01067327
 0.01717187 0.01152419 0.01521516 0.00253364]
train_maes at epoch36: [0.19422264 0.078551   0.07710488 0.07751157 0.06622163 0.06554431
 0.07826979 0.07093104 0.07511404 0.03637531]
test_loss at epoch36: 7.299208973950528e-05
test_mses at epoch36: [0.01977229 0.00045376 0.00234467 0.0055598  0.00163812 0.00169809
 0.00113195 0.00187196 0.00094798 0.00026296]
test_maes at epoch36: [0.11045815 0.01425202 0.03692883 0.06019848 0.02596484 0.02604841
 0.02200905 0.02914238 0.01943652 0.01133897]

train_loss at epoch37: 0.0004902888334633663
train_mses at epoch37: [0.07393791 0.01571909 0.0175609  0.01265152 0.012223   0.01606641
 0.01223572 0.01195165 0.01089821 0.00270531]
train_maes at epoch37: [0.19832386 0.07289381 0.07993067 0.07414344 0.06773888 0.07866704
 0.06775469 0.07288217 0.06913686 0.03698355]
test_loss at epoch37: 0.0001602095905988616
test_mses at epoch37: [0.01796265 0.00478254 0.00398506 0.00368714 0.00575523 0.00105068
 0.00171633 0.01560071 0.00074115 0.00031309]
test_maes at epoch37: [0.10729161 0.03725591 0.04632952 0.03883986 0.03705361 0.02497451
 0.02170169 0.05727747 0.01985548 0.0128516 ]

train_loss at epoch38: 0.000519025488756597
train_mses at epoch38: [0.07613784 0.01138941 0.01926222 0.01730058 0.01743212 0.01405583
 0.01168583 0.01428741 0.01017052 0.00248542]
train_maes at epoch38: [0.19972367 0.06525108 0.0904602  0.07723276 0.07061955 0.07240603
 0.06696394 0.08023929 0.06457443 0.03557094]
test_loss at epoch38: 0.0001255742093409471
test_mses at epoch38: [0.01936382 0.00181655 0.00422082 0.00583444 0.00445602 0.00126134
 0.00116664 0.00670007 0.00307733 0.00025997]
test_maes at epoch38: [0.10141108 0.02473103 0.04396896 0.03908053 0.03384707 0.02640905
 0.02282484 0.04817524 0.03537033 0.01056568]

train_loss at epoch39: 0.0004438984633839511
train_mses at epoch39: [0.06257376 0.01202166 0.01715366 0.01189798 0.01239032 0.0149693
 0.00960627 0.01254699 0.01017643 0.00243512]
train_maes at epoch39: [0.18241084 0.07054773 0.08393542 0.07057059 0.06747412 0.07112216
 0.06266391 0.07424003 0.06358776 0.03531563]
test_loss at epoch39: 0.00011764497724064487
test_mses at epoch39: [0.01938109 0.00535174 0.00241691 0.00640665 0.00133688 0.00199053
 0.00121142 0.00129644 0.00636323 0.00029941]
test_maes at epoch39: [0.11080421 0.03396817 0.03424517 0.04151404 0.02510208 0.02675483
 0.01744817 0.02464596 0.03911865 0.01151174]

train_loss at epoch40: 0.00047002381154712526
train_mses at epoch40: [0.0556764  0.0143121  0.0172614  0.01199814 0.01122786 0.01552861
 0.01192996 0.01278541 0.01234219 0.00215653]
train_maes at epoch40: [0.17648443 0.07236269 0.08155371 0.0713794  0.06701381 0.07080268
 0.06609349 0.07458931 0.0666974  0.03384786]
test_loss at epoch40: 0.00019702433975056765
test_mses at epoch40: [0.04263606 0.00099504 0.0220421  0.0034858  0.00724378 0.00155277
 0.00253365 0.00138035 0.0041911  0.00025379]
test_maes at epoch40: [0.16495261 0.02426759 0.09749125 0.03814129 0.04077533 0.02512283
 0.02473697 0.02778945 0.03552216 0.01059551]

train_loss at epoch41: 0.00044750923568263965
train_mses at epoch41: [0.06611637 0.01675407 0.01488185 0.0127216  0.01003379 0.0123011
 0.009594   0.01036859 0.01508673 0.00222207]
train_maes at epoch41: [0.18545677 0.07423182 0.08016303 0.07246716 0.06192963 0.06829339
 0.06202944 0.0703196  0.0758122  0.03378978]
test_loss at epoch41: 0.00015043025028872045
test_mses at epoch41: [0.02145001 0.01154284 0.0040803  0.00164004 0.00094717 0.00103236
 0.00539935 0.003641   0.00602047 0.00034667]
test_maes at epoch41: [0.0952845  0.05334171 0.03181141 0.02635658 0.02202253 0.02487101
 0.0337963  0.03155402 0.0306324  0.01223825]

train_loss at epoch42: 0.00042111010840558945
train_mses at epoch42: [0.06375585 0.01285545 0.01300036 0.01400703 0.01009288 0.01068501
 0.01208792 0.01000257 0.01188621 0.0021883 ]
train_maes at epoch42: [0.19139379 0.07127214 0.07500364 0.07618297 0.06184346 0.0658914
 0.06680936 0.06844946 0.06487084 0.03437252]
test_loss at epoch42: 6.446131799825803e-05
test_mses at epoch42: [0.00659226 0.00121168 0.0018594  0.00247059 0.00120554 0.00202334
 0.00114739 0.00279204 0.00209277 0.00033518]
test_maes at epoch42: [0.06200297 0.02230139 0.03002811 0.03351952 0.02332798 0.02805762
 0.02145379 0.03377224 0.03146519 0.01354135]

train_loss at epoch43: 0.00040409151846146644
train_mses at epoch43: [0.06664892 0.01120355 0.01189034 0.01191884 0.01207238 0.01028949
 0.01179553 0.01083011 0.01101576 0.00237413]
train_maes at epoch43: [0.18431876 0.06469465 0.07118429 0.06939245 0.06431783 0.06256724
 0.06483115 0.06999822 0.0670673  0.03490364]
test_loss at epoch43: 8.334884137668191e-05
test_mses at epoch43: [0.01204187 0.00149017 0.00091533 0.0023891  0.00307722 0.0029338
 0.00084739 0.0037912  0.00334087 0.00051214]
test_maes at epoch43: [0.08331379 0.02774031 0.01973194 0.02784736 0.0254006  0.03117228
 0.01983547 0.03455167 0.03244139 0.0139652 ]

train_loss at epoch44: 0.00042217655325348075
train_mses at epoch44: [0.05429077 0.01121546 0.01223433 0.01037393 0.01101573 0.01226633
 0.01179364 0.01230278 0.0142045  0.00196358]
train_maes at epoch44: [0.17462566 0.06643463 0.07378687 0.06599912 0.06086089 0.06552519
 0.06694395 0.06883632 0.06938708 0.03167081]
test_loss at epoch44: 7.362480453671292e-05
test_mses at epoch44: [0.0106425  0.00259221 0.00176209 0.00326034 0.00065945 0.00254829
 0.00075133 0.00079811 0.00437686 0.00016803]
test_maes at epoch44: [0.08235977 0.026344   0.02215576 0.04133353 0.01872213 0.02938211
 0.01524109 0.02127695 0.04735661 0.00883053]

train_loss at epoch45: 0.000417134359211443
train_mses at epoch45: [0.05916234 0.01245699 0.01378944 0.01468278 0.00983709 0.01206834
 0.00942637 0.00967448 0.01284734 0.00199406]
train_maes at epoch45: [0.17988683 0.0664268  0.07334258 0.07215218 0.06079049 0.06333396
 0.06178387 0.06692414 0.06845331 0.0321813 ]
test_loss at epoch45: 0.00012077491215251862
test_mses at epoch45: [0.01779088 0.00217492 0.00372854 0.00316433 0.00978085 0.00160131
 0.00389873 0.00201418 0.00164448 0.00016043]
test_maes at epoch45: [0.09346872 0.028314   0.03815948 0.03145416 0.041596   0.02524993
 0.03146027 0.03041318 0.02544677 0.00859408]

train_loss at epoch46: 0.00041775552412257234
train_mses at epoch46: [0.05606271 0.01224736 0.0139029  0.01306046 0.0116251  0.01112278
 0.01297779 0.01054998 0.00905514 0.00204101]
train_maes at epoch46: [0.17531309 0.06479598 0.07602238 0.07034063 0.06282494 0.06298466
 0.06666252 0.06503037 0.06288223 0.03205204]
test_loss at epoch46: 4.555268938890956e-05
test_mses at epoch46: [0.01449123 0.00158596 0.00148282 0.00129566 0.0010772  0.00090498
 0.00047618 0.00096986 0.0018157  0.00010707]
test_maes at epoch46: [0.0970586  0.0220853  0.02371186 0.02431416 0.02431552 0.02424892
 0.0163252  0.02259191 0.02979916 0.00634551]

train_loss at epoch47: 0.0004066685183677188
train_mses at epoch47: [0.0633643  0.01326085 0.01307314 0.01134893 0.01015681 0.01197394
 0.01051953 0.01179295 0.00979773 0.00197243]
train_maes at epoch47: [0.18230891 0.07132862 0.07536902 0.06809409 0.06006749 0.06499161
 0.06093093 0.07060824 0.06111613 0.03081005]
test_loss at epoch47: 0.0001479133894231091
test_mses at epoch47: [0.02651452 0.00346095 0.00788433 0.00976453 0.0051255  0.0039695
 0.00098129 0.00068195 0.00112147 0.00048647]
test_maes at epoch47: [0.11092768 0.03923625 0.05743778 0.04900458 0.03858022 0.03695461
 0.02320217 0.01992177 0.02162467 0.01334449]

train_loss at epoch48: 0.0004227069092597416
train_mses at epoch48: [0.05840028 0.01211369 0.01542487 0.01389825 0.01125671 0.01131512
 0.00920848 0.0113837  0.01067539 0.00185828]
train_maes at epoch48: [0.17695843 0.06824827 0.0752343  0.07565942 0.06346884 0.06561218
 0.05956686 0.06926007 0.0636706  0.03151885]
test_loss at epoch48: 0.00010870427309357105
test_mses at epoch48: [0.0095594  0.0005862  0.00947381 0.0013245  0.00080445 0.0078277
 0.00088762 0.00168648 0.00283545 0.00026738]
test_maes at epoch48: [0.07559281 0.01611519 0.05741453 0.02636819 0.01930189 0.04998025
 0.01587026 0.02706606 0.02547644 0.0095465 ]

train_loss at epoch49: 0.000419439107645303
train_mses at epoch49: [0.06376902 0.01045442 0.01431137 0.01596268 0.00831833 0.01161514
 0.0138394  0.01112007 0.00835943 0.0018181 ]
train_maes at epoch49: [0.18169639 0.0651372  0.07619047 0.0750777  0.0580742  0.06449541
 0.06551171 0.0665124  0.05791604 0.02978147]
test_loss at epoch49: 7.341305745091844e-05
test_mses at epoch49: [0.00725872 0.00202403 0.00374316 0.0019379  0.00320187 0.0014463
 0.00094968 0.00315826 0.00050972 0.00028385]
test_maes at epoch49: [0.06717309 0.02022755 0.04102562 0.02886318 0.03880862 0.02257244
 0.01798066 0.02811702 0.01579039 0.01259321]

