train_loss at epoch0: 0.03145071562934429
train_mses at epoch0: [3.03676483 0.95207188 1.1556797  0.92504387 0.99651207 0.35280045
 1.01287551 1.06174658 0.9766534  0.02346309]
train_maes at epoch0: [1.49274345 0.49237489 0.65631561 0.55812864 0.48438196 0.33014334
 0.46420503 0.52359435 0.53270696 0.06651499]
test_loss at epoch0: 0.02815969985850314
test_mses at epoch0: [2.3479111  0.89534699 1.04984492 0.83773822 0.82851444 0.18413281
 0.9283219  0.96393638 0.86467693 0.11573289]
test_maes at epoch0: [1.38025142 0.59220325 0.59770472 0.34521686 0.47489664 0.27409651
 0.46693589 0.39486572 0.47188274 0.32593135]

train_loss at epoch1: 0.017316778130987857
train_mses at epoch1: [0.96989812 0.44043353 0.92203948 0.49711302 0.51119871 0.14996522
 0.4806552  0.72154629 0.42070695 0.03627116]
train_maes at epoch1: [0.77842198 0.44017306 0.54246827 0.46770659 0.42276375 0.28939533
 0.39832221 0.48480428 0.39835303 0.12235692]
test_loss at epoch1: 0.013926987793851407
test_mses at epoch1: [0.67407396 0.36346066 0.77261494 0.46493361 0.32178446 0.26869769
 0.36213189 0.48460573 0.26163484 0.04412388]
test_maes at epoch1: [0.66719308 0.38932147 0.39326095 0.43966518 0.36390062 0.32391665
 0.3914544  0.39940303 0.2890365  0.14582839]

train_loss at epoch2: 0.007071354149028342
train_mses at epoch2: [0.58804434 0.1128382  0.49641563 0.26960813 0.16924432 0.07777842
 0.09962056 0.32167846 0.10039628 0.03605184]
train_maes at epoch2: [0.59126932 0.23920628 0.42501945 0.34740998 0.30666564 0.20033099
 0.22207885 0.40607755 0.23639391 0.13659862]
test_loss at epoch2: 0.007790055601520741
test_mses at epoch2: [0.59511567 0.17526754 0.45669946 0.32416617 0.19242693 0.07937053
 0.19658448 0.20403249 0.16068795 0.06073462]
test_maes at epoch2: [0.65084083 0.22475233 0.38311115 0.28596408 0.28195901 0.14791018
 0.29913307 0.28988626 0.20343309 0.21390398]

train_loss at epoch3: 0.0033201531041413546
train_mses at epoch3: [0.42811292 0.06937612 0.20760945 0.1256628  0.07932373 0.05443227
 0.04823009 0.11306265 0.0535054  0.0237895 ]
train_maes at epoch3: [0.4702257  0.18653982 0.30020096 0.24659373 0.19585861 0.16746865
 0.16070232 0.26037531 0.16898444 0.10832307]
test_loss at epoch3: 0.006665668906049526
test_mses at epoch3: [0.44535143 0.14157625 0.28308349 0.26853969 0.18397163 0.11387346
 0.20006833 0.16456203 0.18412744 0.05508661]
test_maes at epoch3: [0.55294641 0.21571656 0.27573218 0.35318847 0.1933784  0.15812689
 0.21727901 0.25064305 0.20659433 0.20145957]

train_loss at epoch4: 0.001953138206946723
train_mses at epoch4: [0.30030286 0.05179556 0.06374312 0.06687015 0.04400177 0.04196874
 0.04636126 0.07247984 0.04429971 0.01998722]
train_maes at epoch4: [0.40749497 0.16056588 0.18924449 0.18401892 0.14556707 0.14156182
 0.14571244 0.19916203 0.1518909  0.09940161]
test_loss at epoch4: 0.005054504511521217
test_mses at epoch4: [0.36571051 0.11229079 0.19302139 0.24309579 0.11679887 0.0648926
 0.13165831 0.14875331 0.15328364 0.04532482]
test_maes at epoch4: [0.50144496 0.20281975 0.24240198 0.24282752 0.17444626 0.1390308
 0.20223296 0.25509829 0.17544164 0.17941121]

train_loss at epoch5: 0.0015878829599774264
train_mses at epoch5: [0.25337263 0.03568785 0.06796944 0.05690848 0.03234987 0.03349857
 0.03225925 0.04850408 0.04087325 0.01588319]
train_maes at epoch5: [0.38238306 0.1401528  0.18463149 0.1661855  0.1261484  0.12903154
 0.12442591 0.16342185 0.13457735 0.08926593]
test_loss at epoch5: 0.003961382790448818
test_mses at epoch5: [0.29160291 0.1017957  0.14636508 0.16171772 0.09712026 0.07539169
 0.11198049 0.11409535 0.10045452 0.03834375]
test_maes at epoch5: [0.4503987  0.2315293  0.21581369 0.22309624 0.14678697 0.14026748
 0.19838842 0.21089751 0.1401787  0.16505149]

train_loss at epoch6: 0.0014184562890651696
train_mses at epoch6: [0.19952389 0.03826826 0.04786776 0.04625455 0.03414235 0.02548013
 0.03802386 0.04492864 0.03649964 0.01464251]
train_maes at epoch6: [0.33995433 0.13087913 0.15032267 0.14845464 0.12844659 0.11602198
 0.13450965 0.15338391 0.12954927 0.08508599]
test_loss at epoch6: 0.0032927750589999746
test_mses at epoch6: [0.2928284  0.07518906 0.13723799 0.13850224 0.07600806 0.07234881
 0.08965479 0.07645188 0.08330482 0.03503213]
test_maes at epoch6: [0.44390568 0.14827275 0.21164557 0.22910349 0.16400899 0.12879005
 0.15180624 0.14697988 0.14162705 0.15696784]

train_loss at epoch7: 0.0012727347053983745
train_mses at epoch7: [0.19987233 0.03018011 0.043602   0.04977003 0.03138932 0.02624308
 0.02815686 0.0412403  0.02941053 0.01258252]
train_maes at epoch7: [0.3379993  0.11888294 0.14979847 0.14854556 0.11937692 0.113276
 0.11099271 0.14600502 0.1237025  0.08022658]
test_loss at epoch7: 0.0028668603404088224
test_mses at epoch7: [0.24149009 0.08079855 0.13286477 0.12668105 0.04788884 0.05058992
 0.08337972 0.06125855 0.07525459 0.024366  ]
test_maes at epoch7: [0.39463293 0.14081509 0.21010916 0.18294142 0.10825626 0.11299661
 0.17761201 0.12484228 0.12903371 0.12732953]

train_loss at epoch8: 0.0010932174640053765
train_mses at epoch8: [0.17604328 0.02776732 0.03680673 0.03574929 0.02320362 0.02692563
 0.02672981 0.03500882 0.02844534 0.01265764]
train_maes at epoch8: [0.32334995 0.1140546  0.13409882 0.12467462 0.10387739 0.11018318
 0.10622059 0.13397738 0.11841526 0.07955033]
test_loss at epoch8: 0.0026159694101265136
test_mses at epoch8: [0.21641748 0.08346314 0.09429337 0.10738949 0.05526962 0.03051162
 0.04682238 0.08221413 0.09804798 0.0256703 ]
test_maes at epoch8: [0.37874642 0.12653076 0.18048288 0.16358405 0.10846808 0.08809992
 0.12111083 0.14568269 0.147798   0.13343008]

train_loss at epoch9: 0.0009828202718710328
train_mses at epoch9: [0.1513263  0.02513057 0.03242471 0.02954941 0.02716694 0.02367411
 0.02610951 0.02823864 0.02417843 0.00883541]
train_maes at epoch9: [0.28987261 0.11159957 0.12539497 0.12112088 0.10463063 0.10690931
 0.11329937 0.12014391 0.10631492 0.06843801]
test_loss at epoch9: 0.0025377551569266523
test_mses at epoch9: [0.2203044  0.06226868 0.10179437 0.11177067 0.07258905 0.03759306
 0.06334065 0.06002804 0.06914522 0.02609527]
test_maes at epoch9: [0.36890956 0.11759118 0.16668547 0.15871642 0.12175635 0.11364582
 0.14064532 0.13917101 0.1244829  0.13223057]

train_loss at epoch10: 0.0010549248830276602
train_mses at epoch10: [0.16015521 0.022      0.04233457 0.03615186 0.02974895 0.02487957
 0.0243461  0.02562992 0.02223179 0.00947848]
train_maes at epoch10: [0.30196236 0.10443849 0.13818546 0.12503101 0.10418359 0.1059888
 0.10690937 0.11535786 0.09981391 0.06969115]
test_loss at epoch10: 0.002797707360475621
test_mses at epoch10: [0.20485776 0.06559974 0.09827425 0.16637854 0.04406803 0.02743263
 0.06332627 0.08004733 0.09869409 0.0235812 ]
test_maes at epoch10: [0.35914775 0.14912223 0.22115532 0.2269012  0.10431709 0.08225012
 0.12607744 0.13808385 0.13653148 0.12824371]

train_loss at epoch11: 0.0009543064590345355
train_mses at epoch11: [0.14432682 0.02320029 0.02844088 0.02965865 0.02913559 0.02811214
 0.02211297 0.02474373 0.02699405 0.0073514 ]
train_maes at epoch11: [0.29367461 0.10342135 0.11748428 0.11492323 0.1127833  0.10406764
 0.09652858 0.11184042 0.10583219 0.06219436]
test_loss at epoch11: 0.0025482005935083046
test_mses at epoch11: [0.21901772 0.0707568  0.07860873 0.12163696 0.07715025 0.02887389
 0.07307787 0.07035427 0.06763376 0.01880992]
test_maes at epoch11: [0.36068419 0.11758666 0.16739895 0.1597052  0.13821824 0.10015814
 0.12687011 0.13631092 0.15099259 0.11019864]

train_loss at epoch12: 0.0008582128558624932
train_mses at epoch12: [0.14795851 0.0243476  0.02468991 0.03146578 0.01922228 0.01962772
 0.02447786 0.02359344 0.02266026 0.00677033]
train_maes at epoch12: [0.2871318  0.10839724 0.10978844 0.12076711 0.0977838  0.09538446
 0.10039384 0.10884166 0.10387881 0.05895279]
test_loss at epoch12: 0.0023258658165627335
test_mses at epoch12: [0.20526736 0.07166864 0.07107279 0.14284205 0.05391712 0.01799185
 0.05886849 0.0588733  0.05715511 0.02035342]
test_maes at epoch12: [0.35664233 0.12262258 0.15802079 0.20998449 0.11703056 0.08157686
 0.17787591 0.12727723 0.11203297 0.11722464]

train_loss at epoch13: 0.0008435763141259234
train_mses at epoch13: [0.12715784 0.02304146 0.02393198 0.02900364 0.02130695 0.02557515
 0.02436059 0.02237544 0.01896952 0.00608768]
train_maes at epoch13: [0.26347131 0.09764977 0.10597805 0.11297508 0.0913417  0.10402678
 0.10787477 0.1080239  0.09441769 0.05620633]
test_loss at epoch13: 0.0019298357928686954
test_mses at epoch13: [0.16405552 0.04190829 0.07852902 0.10166842 0.04971138 0.0267474
 0.04139589 0.05212956 0.04877648 0.01935705]
test_maes at epoch13: [0.32787358 0.10743964 0.17774525 0.138566   0.10159186 0.08042711
 0.113404   0.12764057 0.10590855 0.11263629]

train_loss at epoch14: 0.000809982495798551
train_mses at epoch14: [0.11949513 0.02007604 0.025645   0.02469206 0.01877213 0.02146264
 0.0282294  0.02211588 0.02013524 0.0056613 ]
train_maes at epoch14: [0.25828867 0.09272512 0.11160055 0.10899861 0.08785429 0.09256892
 0.1083559  0.10577333 0.09612801 0.05383615]
test_loss at epoch14: 0.002045277129620948
test_mses at epoch14: [0.1918691  0.04964514 0.088571   0.08654479 0.03623876 0.04838627
 0.04794871 0.05099489 0.0541911  0.0237132 ]
test_maes at epoch14: [0.36087276 0.12281417 0.17860617 0.15990925 0.09270272 0.0976034
 0.10405255 0.11172125 0.10435317 0.13001793]

train_loss at epoch15: 0.0007440219806665753
train_mses at epoch15: [0.10769423 0.01743476 0.02165875 0.02359512 0.01355103 0.02170166
 0.02121756 0.02531627 0.02037048 0.00571306]
train_maes at epoch15: [0.24541058 0.09052134 0.10708669 0.11085286 0.07936105 0.09525036
 0.0985713  0.11118578 0.09911114 0.0551342 ]
test_loss at epoch15: 0.002355911075434786
test_mses at epoch15: [0.16665623 0.06099693 0.11607923 0.08954526 0.04712802 0.02927581
 0.04301579 0.08168719 0.07271508 0.02412599]
test_maes at epoch15: [0.30560429 0.14042108 0.15038483 0.13165223 0.10137801 0.11214281
 0.12481479 0.17300335 0.12343024 0.13416457]

train_loss at epoch16: 0.0007592238054828758
train_mses at epoch16: [0.12945678 0.02092346 0.02405834 0.02874306 0.01826832 0.02252327
 0.01774211 0.02015007 0.01578513 0.00577627]
train_maes at epoch16: [0.27184043 0.09415996 0.11007109 0.11218379 0.08643502 0.09368713
 0.08359002 0.10181685 0.08537966 0.05559098]
test_loss at epoch16: 0.001796226572007575
test_mses at epoch16: [0.16570056 0.04377611 0.07878876 0.08375371 0.02955877 0.04266888
 0.03878315 0.03244679 0.06300715 0.01390446]
test_maes at epoch16: [0.33170757 0.1399932  0.15546218 0.13018828 0.09437074 0.09261167
 0.09999004 0.10821185 0.10904097 0.08908197]

train_loss at epoch17: 0.0007034734342286879
train_mses at epoch17: [0.11317381 0.01672815 0.02411804 0.02799953 0.01622334 0.01728012
 0.01625103 0.0208509  0.0176235  0.00478998]
train_maes at epoch17: [0.24759089 0.0873595  0.1036654  0.10639882 0.08198185 0.08897771
 0.08404459 0.09534581 0.08862806 0.05101797]
test_loss at epoch17: 0.0016453262973339
test_mses at epoch17: [0.14883743 0.03461011 0.07678781 0.09691126 0.0294929  0.01410714
 0.03019113 0.03913389 0.05211783 0.01699292]
test_maes at epoch17: [0.30088758 0.10826864 0.1509744  0.13325264 0.078351   0.05728913
 0.08737865 0.103105   0.09835161 0.10187801]

train_loss at epoch18: 0.0006354404080837489
train_mses at epoch18: [0.0965148  0.0175628  0.02326164 0.01840942 0.01500013 0.0153466
 0.01797227 0.01478839 0.01887073 0.0051436 ]
train_maes at epoch18: [0.23408637 0.08354896 0.10141564 0.09354977 0.07478292 0.08375882
 0.08671587 0.08980925 0.09209326 0.05330764]
test_loss at epoch18: 0.0016343029949100727
test_mses at epoch18: [0.13726605 0.03368733 0.06581413 0.10099758 0.01413365 0.01644852
 0.04103515 0.03757798 0.05819739 0.02030148]
test_maes at epoch18: [0.28124304 0.09210196 0.13228788 0.14943272 0.07231126 0.06340627
 0.09736977 0.11131784 0.10533469 0.11765104]

train_loss at epoch19: 0.0006833353273055338
train_mses at epoch19: [0.09896906 0.0234598  0.01950259 0.0190548  0.01838048 0.01776281
 0.0164875  0.01923015 0.01808276 0.00521131]
train_maes at epoch19: [0.22927842 0.0958481  0.09630644 0.09497873 0.08632606 0.08839233
 0.08476315 0.09753025 0.08708282 0.05126348]
test_loss at epoch19: 0.0015684618376829522
test_mses at epoch19: [0.15945353 0.03939328 0.05626905 0.09986067 0.02481306 0.01338125
 0.03492646 0.04014276 0.04843466 0.013723  ]
test_maes at epoch19: [0.29583423 0.10806412 0.1661203  0.13900697 0.08219627 0.05759128
 0.11552419 0.11275611 0.09467224 0.09624938]

train_loss at epoch20: 0.0006774133603841542
train_mses at epoch20: [0.09651693 0.02183203 0.02342504 0.0209107  0.01750972 0.01437227
 0.01602325 0.02031818 0.01743362 0.00486952]
train_maes at epoch20: [0.22636903 0.0962895  0.10737882 0.09746928 0.08055102 0.07853724
 0.08231758 0.09585925 0.08343114 0.04999267]
test_loss at epoch20: 0.0014569671091405635
test_mses at epoch20: [0.13474986 0.06047964 0.04655442 0.08548431 0.01937672 0.01090149
 0.0255847  0.04289422 0.04170618 0.01285974]
test_maes at epoch20: [0.2905246  0.09811525 0.12680074 0.12735065 0.08835679 0.06124783
 0.09529684 0.1207662  0.09047373 0.0902544 ]

train_loss at epoch21: 0.0006378692998352678
train_mses at epoch21: [0.10856891 0.0185581  0.02011295 0.01806248 0.01741843 0.019667
 0.01430672 0.01812789 0.01494555 0.00525164]
train_maes at epoch21: [0.24564577 0.08650246 0.09196706 0.09144839 0.08341682 0.0909219
 0.07722378 0.08971649 0.08013579 0.05089371]
test_loss at epoch21: 0.0012896330135458327
test_mses at epoch21: [0.12766544 0.03861881 0.04050973 0.09125842 0.02182863 0.02123288
 0.02203194 0.020577   0.03384082 0.01606437]
test_maes at epoch21: [0.27723351 0.15521623 0.13395285 0.15151339 0.06796941 0.1094013
 0.09153899 0.09124006 0.08990814 0.10171175]

train_loss at epoch22: 0.0005844234875304268
train_mses at epoch22: [0.07954946 0.01370289 0.01973346 0.01650548 0.0152627  0.01990992
 0.01457841 0.01541617 0.01506951 0.00427201]
train_maes at epoch22: [0.21118411 0.08090668 0.09366905 0.08711751 0.07731314 0.09176341
 0.07959134 0.08484863 0.08095236 0.04726856]
test_loss at epoch22: 0.001597679596632085
test_mses at epoch22: [0.12595587 0.02945992 0.04706797 0.08619121 0.03202948 0.01525909
 0.0285943  0.04159196 0.08765044 0.01392123]
test_maes at epoch22: [0.27805999 0.08164428 0.15281752 0.13068469 0.0846386  0.08843917
 0.08649486 0.10427986 0.11900837 0.09138324]

train_loss at epoch23: 0.0006142829165873217
train_mses at epoch23: [0.08717138 0.0191968  0.01540957 0.01935123 0.01470281 0.01487663
 0.01997449 0.01770504 0.01704605 0.00359345]
train_maes at epoch23: [0.21530518 0.08650707 0.08557579 0.09302972 0.07640512 0.07821055
 0.08978829 0.0890764  0.08449025 0.04283937]
test_loss at epoch23: 0.0010923428480770993
test_mses at epoch23: [0.09221363 0.0255799  0.03915985 0.06788138 0.02098419 0.01204877
 0.02573379 0.02605501 0.03215697 0.0116391 ]
test_maes at epoch23: [0.21908904 0.07163735 0.11267931 0.10795176 0.06671052 0.08340098
 0.11269349 0.10062209 0.08524622 0.08598143]

train_loss at epoch24: 0.0005647331287965496
train_mses at epoch24: [0.08288289 0.01800549 0.02244914 0.01723772 0.01239084 0.01161221
 0.01560407 0.01567534 0.01392134 0.00419433]
train_maes at epoch24: [0.21093616 0.08099722 0.10429427 0.08672577 0.0711788  0.07206757
 0.08037776 0.08525991 0.07525207 0.04656141]
test_loss at epoch24: 0.0015868293220533969
test_mses at epoch24: [0.16579809 0.03087133 0.0700716  0.08868488 0.04019476 0.01680953
 0.04751801 0.03267678 0.0324169  0.01553823]
test_maes at epoch24: [0.29791782 0.08834546 0.12444001 0.11868443 0.08465208 0.08581457
 0.09185142 0.09870359 0.08867817 0.09747749]

train_loss at epoch25: 0.0005572873001739859
train_mses at epoch25: [0.0801707  0.01612627 0.01718632 0.01853924 0.01372196 0.01375842
 0.01484175 0.01650286 0.01473942 0.00342696]
train_maes at epoch25: [0.20733231 0.08198615 0.0864297  0.0896696  0.07193674 0.07621427
 0.07721971 0.08355527 0.07575134 0.04230033]
test_loss at epoch25: 0.0010332941830633803
test_mses at epoch25: [0.09165608 0.02633817 0.032744   0.06646721 0.02537107 0.00956867
 0.02028577 0.02058137 0.03011675 0.01492849]
test_maes at epoch25: [0.22338654 0.07557766 0.1292122  0.12820817 0.07862319 0.05677232
 0.070413   0.08628761 0.10778317 0.0970811 ]

train_loss at epoch26: 0.0004991530861824434
train_mses at epoch26: [0.07582021 0.01135294 0.01548666 0.01635508 0.01564012 0.01164817
 0.0138606  0.01386575 0.01220305 0.00349731]
train_maes at epoch26: [0.20758966 0.07229018 0.08432538 0.08513624 0.07431853 0.07284277
 0.0741042  0.07968573 0.07729812 0.04150216]
test_loss at epoch26: 0.0012556931420051036
test_mses at epoch26: [0.09824298 0.03407957 0.03385055 0.06472537 0.02487849 0.0078753
 0.02712352 0.05387893 0.04424858 0.00989385]
test_maes at epoch26: [0.23891543 0.08135811 0.11378184 0.1084713  0.06859556 0.05146809
 0.09175987 0.111245   0.08652348 0.07229939]

train_loss at epoch27: 0.0005430866112100317
train_mses at epoch27: [0.07202533 0.01460536 0.01646443 0.01364297 0.01493368 0.0161789
 0.01473519 0.01572491 0.01505828 0.00335987]
train_maes at epoch27: [0.20014991 0.07613314 0.08520858 0.08033669 0.07149984 0.07727742
 0.07806312 0.082679   0.07580911 0.04066912]
test_loss at epoch27: 0.001120473169028125
test_mses at epoch27: [0.10402841 0.03145785 0.04414732 0.05885074 0.0246675  0.00895771
 0.02141104 0.03218485 0.03297448 0.01223822]
test_maes at epoch27: [0.2500697  0.08395407 0.14463813 0.09824429 0.07155522 0.06664122
 0.08944258 0.09091943 0.08410334 0.08938072]

train_loss at epoch28: 0.0005400037316189326
train_mses at epoch28: [0.07752163 0.01422145 0.01572617 0.01599513 0.01713603 0.01426397
 0.01589223 0.01318833 0.01477469 0.00352951]
train_maes at epoch28: [0.2051671  0.07268951 0.08274701 0.0839436  0.07761772 0.07607071
 0.07927638 0.07975078 0.07375843 0.04220603]
test_loss at epoch28: 0.000833744445736421
test_mses at epoch28: [0.10228628 0.02496497 0.03805093 0.04428977 0.01761848 0.00678663
 0.017331   0.01400147 0.02415767 0.00897214]
test_maes at epoch28: [0.2490593  0.07497121 0.15228149 0.08554396 0.0904248  0.04944366
 0.07534874 0.07942165 0.07105413 0.070973  ]

train_loss at epoch29: 0.0004969531003920797
train_mses at epoch29: [0.08206483 0.01451217 0.01653648 0.01400718 0.01456658 0.01119273
 0.0110851  0.01328573 0.01479601 0.00312905]
train_maes at epoch29: [0.2095262  0.0755054  0.08440316 0.080408   0.07536016 0.06980064
 0.06845724 0.07895958 0.07574669 0.04059106]
test_loss at epoch29: 0.0013123123827291296
test_mses at epoch29: [0.14681619 0.03203637 0.05068703 0.07535469 0.03603422 0.01355726
 0.02217512 0.03037548 0.03851699 0.01041182]
test_maes at epoch29: [0.27740561 0.08229897 0.10405514 0.12478965 0.07685337 0.09052114
 0.06936517 0.09127156 0.07978228 0.08145602]

train_loss at epoch30: 0.0005142168349962919
train_mses at epoch30: [0.08191471 0.01507233 0.01758227 0.01756254 0.01049036 0.01399459
 0.01262523 0.01341589 0.01401987 0.00319808]
train_maes at epoch30: [0.20965005 0.07303644 0.08736248 0.08246485 0.06205747 0.07678074
 0.07396368 0.07745594 0.07698792 0.04037673]
test_loss at epoch30: 0.0011188444443681138
test_mses at epoch30: [0.11545719 0.03415866 0.03974723 0.06244566 0.02746555 0.01098185
 0.02742356 0.01827081 0.0340475  0.01042942]
test_maes at epoch30: [0.24651609 0.08280453 0.15221023 0.1138829  0.08517715 0.04854271
 0.07902724 0.08408296 0.088932   0.07775424]

train_loss at epoch31: 0.0005203294728763719
train_mses at epoch31: [0.07378173 0.01440123 0.01693943 0.01303525 0.01135778 0.01410649
 0.01474705 0.01446737 0.0161139  0.0030713 ]
train_maes at epoch31: [0.20204106 0.0750439  0.08960885 0.07737575 0.06717048 0.07085573
 0.0737934  0.08083436 0.07563643 0.04046821]
test_loss at epoch31: 0.0014841987137147721
test_mses at epoch31: [0.10439458 0.03011821 0.06396994 0.08195121 0.02858633 0.01011066
 0.05202304 0.03096768 0.04388984 0.01361236]
test_maes at epoch31: [0.24740089 0.12067843 0.111863   0.11879781 0.07616764 0.061343
 0.09722876 0.09328742 0.08806032 0.09242148]

train_loss at epoch32: 0.0005206405123239977
train_mses at epoch32: [0.08535988 0.01392841 0.01720035 0.01671613 0.01374706 0.01339535
 0.01519787 0.01721252 0.0102572  0.00283018]
train_maes at epoch32: [0.217367   0.07239848 0.08458304 0.08515035 0.07078299 0.07119582
 0.07461411 0.08625658 0.06643134 0.03817516]
test_loss at epoch32: 0.00100832312130072
test_mses at epoch32: [0.10761663 0.02216791 0.03472227 0.05861955 0.01181479 0.01223878
 0.03726398 0.01948771 0.03251396 0.00913473]
test_maes at epoch32: [0.23060769 0.07407562 0.116853   0.09891148 0.05884123 0.07653723
 0.08365877 0.0932233  0.07351549 0.07203146]

train_loss at epoch33: 0.0004871045685115647
train_mses at epoch33: [0.07246369 0.01291999 0.01377791 0.01944078 0.01180509 0.01256668
 0.0123675  0.01210916 0.01370883 0.00308641]
train_maes at epoch33: [0.1898323  0.06966273 0.07630016 0.0865041  0.06807406 0.06952404
 0.06852215 0.07377082 0.06823254 0.039446  ]
test_loss at epoch33: 0.0008964449999497292
test_mses at epoch33: [0.10571081 0.01711841 0.03291661 0.05204746 0.01347164 0.00609555
 0.02310138 0.02690746 0.03019404 0.00951505]
test_maes at epoch33: [0.25507503 0.08665635 0.11716762 0.09413619 0.05751858 0.04543456
 0.07090401 0.09488663 0.07094063 0.07450348]

train_loss at epoch34: 0.0004883238183770408
train_mses at epoch34: [0.06897613 0.01392038 0.0135036  0.01733191 0.01128099 0.01269592
 0.01683158 0.01268829 0.01240969 0.00287381]
train_maes at epoch34: [0.18688685 0.0717626  0.07963041 0.08690665 0.0657122  0.07076553
 0.07540751 0.07538831 0.0683516  0.03879109]
test_loss at epoch34: 0.0016951877326565855
test_mses at epoch34: [0.16725586 0.06045365 0.05535326 0.1334217  0.01590152 0.0124016
 0.04365508 0.03109157 0.03705493 0.01064336]
test_maes at epoch34: [0.28932364 0.10479539 0.11089227 0.19188406 0.06720235 0.06144446
 0.09395441 0.09180879 0.09489074 0.08408644]

train_loss at epoch35: 0.0004978077164157591
train_mses at epoch35: [0.07426044 0.01336521 0.0189308  0.01667651 0.01342301 0.01116842
 0.01339802 0.01121273 0.01502132 0.00256514]
train_maes at epoch35: [0.19930262 0.07166497 0.0894967  0.07990099 0.06682828 0.06758245
 0.07006164 0.07373231 0.06937668 0.03620055]
test_loss at epoch35: 0.0011727987451756255
test_mses at epoch35: [0.11910563 0.02757116 0.06160436 0.0635926  0.01897961 0.02300138
 0.02316469 0.02106395 0.02861408 0.00947914]
test_maes at epoch35: [0.25427643 0.08829866 0.11015523 0.10490659 0.06292491 0.06835
 0.06665934 0.08440829 0.073116   0.07377365]

train_loss at epoch36: 0.000517025285281558
train_mses at epoch36: [0.0708893  0.01563034 0.01669254 0.01775126 0.01120068 0.01339862
 0.01518365 0.01409835 0.0139947  0.00235452]
train_maes at epoch36: [0.1960189  0.07614806 0.08188838 0.0831995  0.06421588 0.07186784
 0.07142363 0.07475685 0.07508892 0.03543946]
test_loss at epoch36: 0.0010788465968947461
test_mses at epoch36: [0.12636728 0.05063075 0.04140601 0.06884683 0.01016024 0.01241597
 0.01865731 0.01604898 0.02539921 0.00869385]
test_maes at epoch36: [0.25051257 0.09444991 0.09086686 0.10435225 0.06344228 0.06979199
 0.0853218  0.07246912 0.06600593 0.07286619]

train_loss at epoch37: 0.0004689614375201153
train_mses at epoch37: [0.07673397 0.01145651 0.01485202 0.01578109 0.01112661 0.00967814
 0.01179505 0.0142425  0.01527444 0.00247983]
train_maes at epoch37: [0.20547379 0.06821831 0.07918167 0.0832264  0.06110601 0.06334657
 0.06956303 0.07702782 0.07093809 0.03573082]
test_loss at epoch37: 0.0006567000391635489
test_mses at epoch37: [0.06499403 0.02553046 0.02596137 0.04139622 0.00732434 0.00601495
 0.01354651 0.0107372  0.01562751 0.01000687]
test_maes at epoch37: [0.18718349 0.07181639 0.07740404 0.09155393 0.05102347 0.04466476
 0.06866686 0.06430857 0.0854479  0.07815613]

train_loss at epoch38: 0.0004966488229408068
train_mses at epoch38: [0.06684082 0.01133762 0.01519744 0.01776883 0.01371244 0.01457619
 0.01353478 0.01239744 0.01374972 0.00255054]
train_maes at epoch38: [0.19297424 0.07230485 0.07940081 0.0814494  0.06762162 0.07130605
 0.06916529 0.07501434 0.06674313 0.03479505]
test_loss at epoch38: 0.0012305373662805303
test_mses at epoch38: [0.14047843 0.04061109 0.07217059 0.08144552 0.0170258  0.00899752
 0.01880711 0.01733889 0.01980353 0.01208936]
test_maes at epoch38: [0.25556445 0.10098405 0.11334351 0.13267309 0.06117519 0.06782188
 0.08076776 0.07487681 0.07011441 0.0858458 ]

train_loss at epoch39: 0.00042350150575425397
train_mses at epoch39: [0.06208583 0.0115922  0.0128046  0.01358043 0.01183544 0.01118418
 0.01076004 0.01303318 0.01023048 0.0020197 ]
train_maes at epoch39: [0.18245036 0.07117533 0.07386542 0.07057586 0.06915097 0.0666132
 0.06490273 0.07172965 0.06426    0.03318948]
test_loss at epoch39: 0.0011097988648776044
test_mses at epoch39: [0.10171463 0.02801636 0.05446374 0.05950153 0.01269377 0.02030495
 0.03596255 0.02250807 0.02018421 0.01022331]
test_maes at epoch39: [0.23237346 0.08969297 0.09853671 0.09409848 0.0647612  0.11987071
 0.0938975  0.08573426 0.06419493 0.07787515]

train_loss at epoch40: 0.0004028020255723374
train_mses at epoch40: [0.06124307 0.01137473 0.01219876 0.01485453 0.01103468 0.01166822
 0.00864981 0.01068291 0.01003605 0.00245914]
train_maes at epoch40: [0.18416101 0.06862458 0.07444888 0.07679311 0.06401435 0.0665557
 0.05799506 0.06986092 0.06490074 0.03599779]
test_loss at epoch40: 0.0009950488567986387
test_mses at epoch40: [0.07742228 0.0257182  0.03235358 0.06524338 0.02073377 0.01230476
 0.02532824 0.03076207 0.01828089 0.00762584]
test_maes at epoch40: [0.20526948 0.07122309 0.13914045 0.10998553 0.06506943 0.05443988
 0.0803916  0.08833901 0.06457162 0.06475427]

train_loss at epoch41: 0.00042978349758530075
train_mses at epoch41: [0.06486753 0.01195535 0.01354323 0.01580944 0.01010441 0.0083417
 0.01281188 0.01227904 0.01163135 0.00204712]
train_maes at epoch41: [0.18510937 0.06629287 0.07501664 0.07805428 0.06428945 0.06030637
 0.06978916 0.07204583 0.06614073 0.0333507 ]
test_loss at epoch41: 0.0011602574166782358
test_mses at epoch41: [0.09041361 0.02328896 0.04605786 0.05352826 0.02303093 0.01418945
 0.03068722 0.02663967 0.04866737 0.01152056]
test_maes at epoch41: [0.22266686 0.07352237 0.09867961 0.09919239 0.07182431 0.05410238
 0.08511743 0.07961339 0.08640492 0.08192013]

train_loss at epoch42: 0.0004158151308768132
train_mses at epoch42: [0.06224337 0.01261375 0.013566   0.01196005 0.01216119 0.00889235
 0.01033402 0.01311529 0.01098122 0.00221453]
train_maes at epoch42: [0.17875643 0.06789946 0.0734445  0.07057382 0.06384264 0.06123813
 0.06402324 0.07317545 0.06166956 0.03412379]
test_loss at epoch42: 0.0012788698473509322
test_mses at epoch42: [0.12675506 0.04751068 0.06339264 0.05042185 0.02044857 0.01767713
 0.0371368  0.02683024 0.03078188 0.00903826]
test_maes at epoch42: [0.25637892 0.08977187 0.10516418 0.0943338  0.06236392 0.06603077
 0.09203853 0.08595045 0.07272463 0.07727483]

train_loss at epoch43: 0.00042129257394179224
train_mses at epoch43: [0.06302893 0.011559   0.01559713 0.008538   0.01124233 0.0101691
 0.01370085 0.01144298 0.0132216  0.00206415]
train_maes at epoch43: [0.18260862 0.06647036 0.07394295 0.06350775 0.06091082 0.06502284
 0.07066861 0.07133485 0.06647548 0.03104507]
test_loss at epoch43: 0.0009056934096077655
test_mses at epoch43: [0.08649808 0.01563964 0.02070426 0.05683158 0.0300411  0.00711741
 0.03291538 0.01831482 0.02123154 0.01291622]
test_maes at epoch43: [0.21373709 0.07835665 0.10351031 0.11403286 0.09392868 0.04847405
 0.09265559 0.09250236 0.07274945 0.09298334]

train_loss at epoch44: 0.0004001620616961984
train_mses at epoch44: [0.06837313 0.01164775 0.01190749 0.01234483 0.01031488 0.01087614
 0.00976208 0.01103254 0.01127818 0.00184497]
train_maes at epoch44: [0.18748371 0.06418778 0.06822856 0.07021457 0.06337146 0.06293451
 0.0640275  0.07021045 0.0655664  0.03111905]
test_loss at epoch44: 0.0008502218555262748
test_mses at epoch44: [0.06348788 0.02519942 0.02322507 0.04855034 0.01246361 0.00946229
 0.02474694 0.02084918 0.03189392 0.00765928]
test_maes at epoch44: [0.17531231 0.06588838 0.11638229 0.0863483  0.05161141 0.047167
 0.06655586 0.07836312 0.07497436 0.06504285]

train_loss at epoch45: 0.0004073243818562874
train_mses at epoch45: [0.06400748 0.00979281 0.01387801 0.01492274 0.00848915 0.01352388
 0.00873255 0.01068581 0.01101018 0.00217438]
train_maes at epoch45: [0.18422622 0.0640664  0.07355117 0.07562007 0.05847095 0.06868922
 0.06071735 0.06898543 0.06223625 0.0320018 ]
test_loss at epoch45: 0.0014327746280964385
test_mses at epoch45: [0.15708424 0.03070895 0.0638435  0.09767539 0.02585331 0.01842952
 0.0311828  0.03581952 0.02557833 0.00793713]
test_maes at epoch45: [0.27214473 0.07129943 0.10358866 0.13808719 0.06668884 0.06502007
 0.07370118 0.08966696 0.0688     0.0666547 ]

train_loss at epoch46: 0.0004665639097901418
train_mses at epoch46: [0.07410965 0.01603171 0.01410916 0.01232897 0.01220946 0.01356542
 0.01550231 0.01007738 0.01203063 0.00191547]
train_maes at epoch46: [0.1957458  0.07153111 0.07420925 0.06944399 0.05844695 0.06677296
 0.06969551 0.06619332 0.06221154 0.03127093]
test_loss at epoch46: 0.0009954564194095896
test_mses at epoch46: [0.09203359 0.02445217 0.02881998 0.05910396 0.01607145 0.02030081
 0.02154195 0.01715374 0.04268922 0.00674488]
test_maes at epoch46: [0.21773941 0.08875765 0.1035295  0.09900619 0.05587833 0.05898139
 0.09938007 0.06248183 0.0964645  0.05991017]

train_loss at epoch47: 0.00042803678166200505
train_mses at epoch47: [0.0534045  0.01313571 0.01114866 0.01432971 0.01115165 0.01054036
 0.01149705 0.01419747 0.01156124 0.00192392]
train_maes at epoch47: [0.16920387 0.06864914 0.0685212  0.07353781 0.06044659 0.06503409
 0.06564715 0.07415337 0.0645474  0.03182217]
test_loss at epoch47: 0.0010090988486054096
test_mses at epoch47: [0.07647938 0.05304933 0.04309327 0.05011286 0.01879208 0.01003076
 0.02032245 0.02072298 0.01787643 0.00781933]
test_maes at epoch47: [0.19872929 0.08850293 0.09057041 0.09656834 0.06204741 0.04367058
 0.06735854 0.07586702 0.06607871 0.06638637]

train_loss at epoch48: 0.0004470281307882768
train_mses at epoch48: [0.06147208 0.01365982 0.01218854 0.01520161 0.01085327 0.01074853
 0.01255129 0.01417003 0.01207183 0.00180349]
train_maes at epoch48: [0.17817617 0.0689752  0.06896405 0.07339436 0.06272176 0.06334971
 0.06383587 0.07457666 0.0661477  0.02954881]
test_loss at epoch48: 0.0010805036278164132
test_mses at epoch48: [0.08851486 0.02081999 0.04577677 0.05040127 0.03024735 0.01362957
 0.02150198 0.03330921 0.03632422 0.00672127]
test_maes at epoch48: [0.22452848 0.11314243 0.10658828 0.09266097 0.06709694 0.05334985
 0.08068091 0.08886809 0.08736248 0.05674711]

train_loss at epoch49: 0.00043986454492117816
train_mses at epoch49: [0.06687088 0.01256855 0.01419177 0.01186881 0.01390943 0.0112457
 0.00967603 0.0120435  0.01408011 0.00173172]
train_maes at epoch49: [0.19018174 0.06681617 0.07658575 0.06663667 0.06656675 0.06381288
 0.06289573 0.07128148 0.06365632 0.02937859]
test_loss at epoch49: 0.0011952903240602067
test_mses at epoch49: [0.11142459 0.02250233 0.05091965 0.0870474  0.01849649 0.00908172
 0.02051324 0.03424237 0.03312092 0.00790383]
test_maes at epoch49: [0.23090236 0.08791849 0.09793175 0.1421578  0.05722294 0.07079831
 0.07539588 0.09033887 0.07554515 0.06550755]

