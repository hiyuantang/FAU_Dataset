train_loss at epoch0: 0.03224409775848084
train_mses at epoch0: [3.19722639 0.98616477 1.1593263  0.95635882 0.99334855 0.37312174
 1.03750292 1.04790441 0.99525538 0.02218358]
train_maes at epoch0: [1.550096   0.50610604 0.63566268 0.54849694 0.48391624 0.33409886
 0.45395966 0.52060094 0.56368721 0.06357032]
test_loss at epoch0: 0.028284329366176687
test_mses at epoch0: [2.30973541 1.4985198  1.51363445 1.32285944 0.11857381 0.03816678
 0.10875166 0.5016096  1.49614779 0.07578128]
test_maes at epoch0: [1.32334659 0.7167032  0.70723606 0.68892128 0.33438353 0.14191293
 0.3268181  0.37196575 0.59644671 0.24204207]

train_loss at epoch1: 0.01805463070644343
train_mses at epoch1: [1.12761505 0.44720177 0.94337005 0.51934808 0.5032528  0.10510694
 0.53052605 0.79174365 0.48705171 0.04011851]
train_maes at epoch1: [0.86201235 0.43402261 0.53919425 0.45569107 0.44112671 0.24088047
 0.42334891 0.50505628 0.40285869 0.13378293]
test_loss at epoch1: 0.015267123408774112
test_mses at epoch1: [0.74642387 0.67288849 1.1566301  0.71198778 0.13464737 0.0474063
 0.16880028 0.30842783 0.42498259 0.05179258]
test_maes at epoch1: [0.65511075 0.73336897 0.68229434 0.55493806 0.31220672 0.17306543
 0.38714573 0.34671438 0.51405551 0.16495034]

train_loss at epoch2: 0.007506934856560002
train_mses at epoch2: [0.53156143 0.12463849 0.5237452  0.26338216 0.17063307 0.08289968
 0.10191907 0.34528935 0.1212805  0.03609858]
train_maes at epoch2: [0.54931805 0.24648377 0.43588324 0.35156776 0.30726224 0.19964811
 0.22007854 0.40976894 0.25220904 0.13218051]
test_loss at epoch2: 0.008684410931582147
test_mses at epoch2: [0.63288015 0.27561454 0.72636092 0.52253017 0.04802867 0.01818773
 0.05164532 0.08839019 0.2573599  0.04533366]
test_maes at epoch2: [0.62842338 0.41324639 0.58075005 0.39226259 0.19014477 0.0885357
 0.2073163  0.20683301 0.34623607 0.15619129]

train_loss at epoch3: 0.003352236349452683
train_mses at epoch3: [0.42358051 0.07239072 0.19516306 0.11417841 0.07385217 0.06343689
 0.06771428 0.10756871 0.06498387 0.02456868]
train_maes at epoch3: [0.46715814 0.19656154 0.29604255 0.23549121 0.19758997 0.17985504
 0.18430914 0.25572486 0.18057424 0.10976878]
test_loss at epoch3: 0.007073302693823551
test_mses at epoch3: [1.1401233  0.28238928 0.30247746 0.42222852 0.02143288 0.00316417
 0.03806401 0.0382371  0.18712415 0.06031657]
test_maes at epoch3: [0.83465696 0.32291555 0.31983847 0.35291618 0.12281147 0.04212382
 0.1792179  0.10907868 0.28629909 0.22107215]

train_loss at epoch4: 0.002004823414608836
train_mses at epoch4: [0.33673939 0.04598042 0.06737729 0.07652531 0.05116298 0.04138665
 0.0500414  0.0636673  0.04317845 0.02106356]
train_maes at epoch4: [0.43645164 0.15002945 0.18879905 0.19236561 0.15475657 0.14323477
 0.157809   0.18465602 0.15019326 0.10319498]
test_loss at epoch4: 0.003870931473818231
test_mses at epoch4: [0.23741272 0.10812759 0.1194648  0.23030438 0.0156845  0.00371508
 0.01037089 0.02050252 0.15024444 0.04692694]
test_maes at epoch4: [0.40533877 0.27873655 0.26813819 0.29200286 0.08381921 0.04279835
 0.08920394 0.10049327 0.17935399 0.17888412]

train_loss at epoch5: 0.001524856096093959
train_mses at epoch5: [0.25046768 0.04435221 0.05230577 0.04621309 0.03044505 0.03517845
 0.0340322  0.0538379  0.03572694 0.01741356]
train_maes at epoch5: [0.36925244 0.14275033 0.16938433 0.1539938  0.12355304 0.13154795
 0.12904035 0.16935663 0.13324715 0.09499105]
test_loss at epoch5: 0.0036999887608467263
test_mses at epoch5: [0.2466428  0.08701721 0.11001538 0.16567218 0.01095428 0.00337897
 0.01868526 0.01948138 0.16670348 0.03209206]
test_maes at epoch5: [0.418609   0.22807796 0.26206638 0.24901794 0.06928711 0.04535554
 0.12871616 0.07293947 0.20226349 0.13101339]

train_loss at epoch6: 0.0014809381087964519
train_mses at epoch6: [0.22247018 0.0415312  0.06158074 0.04955541 0.03703373 0.02840397
 0.03177627 0.04988497 0.02911869 0.01489891]
train_maes at epoch6: [0.36457007 0.137381   0.18188192 0.14616872 0.12509894 0.1156256
 0.126623   0.15946321 0.12465916 0.08823189]
test_loss at epoch6: 0.0033732879352062306
test_mses at epoch6: [0.16957447 0.06099773 0.08895328 0.14350425 0.00489498 0.00275466
 0.01032975 0.03028348 0.16151621 0.0333914 ]
test_maes at epoch6: [0.3404717  0.15897916 0.24205992 0.24234072 0.05882038 0.03823113
 0.09164413 0.09550816 0.17945819 0.15329144]

train_loss at epoch7: 0.0013374397754074728
train_mses at epoch7: [0.22779964 0.04046341 0.04411835 0.05152674 0.02596414 0.03400716
 0.02780898 0.04100088 0.03200737 0.01160314]
train_maes at epoch7: [0.35998738 0.13685567 0.15090131 0.14959976 0.10949312 0.12200038
 0.11536162 0.14821674 0.12465689 0.07710738]
test_loss at epoch7: 0.003252134044119652
test_mses at epoch7: [0.2213613  0.1210363  0.08979156 0.20033123 0.00681809 0.00361537
 0.00248918 0.03129862 0.15465615 0.02290874]
test_maes at epoch7: [0.40098086 0.24000571 0.19420706 0.24564587 0.06250444 0.04796754
 0.0437341  0.11538889 0.18491754 0.11654237]

train_loss at epoch8: 0.00112581214907163
train_mses at epoch8: [0.18146293 0.02893823 0.03308005 0.03817867 0.03065668 0.02498309
 0.02545508 0.03733735 0.02704262 0.011263  ]
train_maes at epoch8: [0.31899611 0.11650277 0.13092445 0.13748181 0.11545503 0.10970968
 0.11140019 0.13815455 0.1170714  0.07598238]
test_loss at epoch8: 0.002886626274978861
test_mses at epoch8: [0.16039846 0.07215841 0.0834883  0.11965119 0.00622992 0.00666102
 0.00498283 0.00954978 0.1552862  0.02667213]
test_maes at epoch8: [0.33655093 0.21583572 0.19479039 0.22811693 0.06832022 0.0675637
 0.04914775 0.0541184  0.19462911 0.13209605]

train_loss at epoch9: 0.0010733699198178155
train_mses at epoch9: [0.14731298 0.02479699 0.03137043 0.03059454 0.02366636 0.0282295
 0.02572619 0.0333392  0.04130097 0.01019132]
train_maes at epoch9: [0.29787138 0.10980368 0.12909878 0.1237995  0.09982359 0.11101398
 0.10894602 0.13052925 0.12937747 0.07339035]
test_loss at epoch9: 0.0026011484021202045
test_mses at epoch9: [0.15996724 0.05806388 0.09905928 0.12454799 0.0027237  0.00605301
 0.00411663 0.01104055 0.14692552 0.01975363]
test_maes at epoch9: [0.3351234  0.17396512 0.18979994 0.20792149 0.04156613 0.06419715
 0.05208417 0.05874757 0.17930127 0.10939921]

train_loss at epoch10: 0.0009869107718955964
train_mses at epoch10: [0.14490408 0.02697427 0.03154242 0.02895605 0.0212695  0.03183304
 0.02323716 0.02898705 0.02696169 0.00808071]
train_maes at epoch10: [0.28957816 0.10968869 0.12660136 0.11689134 0.09775454 0.11300609
 0.098765   0.12193989 0.11200828 0.06749383]
test_loss at epoch10: 0.002592763051073602
test_mses at epoch10: [0.28597763 0.12076267 0.0554614  0.22660302 0.00458298 0.00561321
 0.01025525 0.00661883 0.06763068 0.01937969]
test_maes at epoch10: [0.4020347  0.17265679 0.14531559 0.27126272 0.05303854 0.05755571
 0.0966378  0.04379015 0.12516306 0.11620691]

train_loss at epoch11: 0.0009636764013347157
train_mses at epoch11: [0.14097138 0.02722257 0.03220176 0.03842605 0.02112572 0.02401388
 0.02768435 0.02319615 0.02346891 0.00683532]
train_maes at epoch11: [0.29362674 0.10945759 0.12567088 0.12752624 0.09403152 0.09869139
 0.10785654 0.11051337 0.10777076 0.06189417]
test_loss at epoch11: 0.0017524701047767985
test_mses at epoch11: [0.15144499 0.0475319  0.04338549 0.06748612 0.00622768 0.0051331
 0.01264521 0.00651102 0.04754071 0.01577094]
test_maes at epoch11: [0.31542258 0.13381231 0.15885196 0.14955071 0.0674462  0.05015031
 0.10381195 0.07149387 0.11525521 0.10016385]

train_loss at epoch12: 0.0011083443520313247
train_mses at epoch12: [0.17600983 0.02662814 0.03880434 0.05544552 0.02633019 0.02166212
 0.02450463 0.02843134 0.02434831 0.00659257]
train_maes at epoch12: [0.31984081 0.10608778 0.13502218 0.15409759 0.09770379 0.10015536
 0.10627013 0.11798816 0.10471542 0.06002108]
test_loss at epoch12: 0.0016330438269738187
test_mses at epoch12: [0.11026455 0.05698266 0.0422536  0.09308191 0.00568692 0.00250681
 0.00150159 0.00789416 0.03963514 0.01391881]
test_maes at epoch12: [0.27937554 0.13871836 0.16020851 0.17258001 0.0580285  0.04016145
 0.02949747 0.07884029 0.15810654 0.09084803]

train_loss at epoch13: 0.000885954968869052
train_mses at epoch13: [0.13332851 0.02331535 0.0268117  0.03817323 0.02306788 0.02090711
 0.02105238 0.02505705 0.01948514 0.00719137]
train_maes at epoch13: [0.28044283 0.10443901 0.11490933 0.12936604 0.09222249 0.09313016
 0.09997928 0.11290199 0.0971059  0.06139402]
test_loss at epoch13: 0.0022534397926102295
test_mses at epoch13: [0.14682812 0.03169555 0.05786444 0.15005322 0.00466016 0.00253658
 0.01231063 0.00995397 0.09186427 0.01558682]
test_maes at epoch13: [0.31590795 0.11195977 0.20347639 0.24403708 0.04975712 0.04397765
 0.10201705 0.05603913 0.15138534 0.10175562]

train_loss at epoch14: 0.0007500567057348312
train_mses at epoch14: [0.12907905 0.02038968 0.02330183 0.02409901 0.01766483 0.01725983
 0.01801217 0.02244754 0.02111773 0.00589046]
train_maes at epoch14: [0.26480492 0.08715259 0.11056726 0.10751113 0.08152667 0.08743948
 0.09003937 0.10753932 0.09554189 0.05498678]
test_loss at epoch14: 0.001956319476061679
test_mses at epoch14: [0.15952399 0.05124828 0.0413378  0.13288904 0.00347974 0.00187805
 0.01307764 0.00546289 0.05714694 0.01593749]
test_maes at epoch14: [0.30880786 0.11583391 0.15836822 0.20165061 0.05112229 0.03484118
 0.10390754 0.04447753 0.12043204 0.10569221]

train_loss at epoch15: 0.0007103652720577381
train_mses at epoch15: [0.09672843 0.01999176 0.0191285  0.02540141 0.01722995 0.01882234
 0.01721696 0.01952859 0.02085014 0.00553231]
train_maes at epoch15: [0.23657716 0.09754066 0.099123   0.10733592 0.0863283  0.0882884
 0.08859925 0.09915744 0.09304708 0.05488916]
test_loss at epoch15: 0.001655137523057613
test_mses at epoch15: [0.1153342  0.03701059 0.03472405 0.07497478 0.00352555 0.00284891
 0.00465419 0.01111451 0.06408368 0.01473475]
test_maes at epoch15: [0.27915414 0.10216692 0.12990155 0.16033647 0.0420059  0.04387502
 0.06161702 0.07392938 0.12512271 0.0974573 ]

train_loss at epoch16: 0.0006981922110463393
train_mses at epoch16: [0.10444882 0.01870156 0.02093806 0.02456997 0.01711707 0.0209882
 0.01770238 0.01790832 0.01880089 0.00526821]
train_maes at epoch16: [0.23793747 0.09266214 0.10262861 0.10057809 0.0818743  0.09303282
 0.08583993 0.0967923  0.09385808 0.05333179]
test_loss at epoch16: 0.001883565864347397
test_mses at epoch16: [0.13343554 0.03731169 0.04838423 0.11256104 0.00322805 0.00212675
 0.00825666 0.01013574 0.0782849  0.01051547]
test_maes at epoch16: [0.2927399  0.12458302 0.17412304 0.1861616  0.05228901 0.03412802
 0.08393819 0.06347476 0.13009294 0.0782746 ]

train_loss at epoch17: 0.0006372329699707792
train_mses at epoch17: [0.10419798 0.02336174 0.01807569 0.01914277 0.01388386 0.01683248
 0.01541694 0.01846371 0.0161418  0.00527551]
train_maes at epoch17: [0.23488525 0.09372882 0.09648973 0.09289969 0.07781937 0.08639973
 0.08251991 0.09614671 0.09044354 0.05417708]
test_loss at epoch17: 0.0014370864633708558
test_mses at epoch17: [0.08905898 0.02170472 0.03012907 0.08133562 0.00239732 0.00231734
 0.00583263 0.00620915 0.04577734 0.01090864]
test_maes at epoch17: [0.23434428 0.1107209  0.13331642 0.18715358 0.04153683 0.03985877
 0.07092468 0.06842067 0.11367907 0.08278664]

train_loss at epoch18: 0.0006486314480153328
train_mses at epoch18: [0.09383324 0.01933726 0.0230785  0.01612566 0.0122697  0.0233433
 0.01786047 0.01752778 0.0158728  0.00560526]
train_maes at epoch18: [0.22990396 0.08895639 0.10181639 0.09024961 0.07742327 0.08877102
 0.08506821 0.09259109 0.08376591 0.05358794]
test_loss at epoch18: 0.0012537072353223538
test_mses at epoch18: [0.07338743 0.02972635 0.02662151 0.06069391 0.00363953 0.00389724
 0.00089735 0.01953533 0.05237686 0.00924246]
test_maes at epoch18: [0.23170368 0.12478652 0.13883244 0.15388518 0.05526124 0.04386213
 0.02188218 0.11962828 0.12930209 0.07610324]

train_loss at epoch19: 0.0006037588154480971
train_mses at epoch19: [0.09740072 0.01872387 0.01792729 0.01593705 0.01523149 0.01391358
 0.01594013 0.01614927 0.01977116 0.00466479]
train_maes at epoch19: [0.22643741 0.08568472 0.09592765 0.08800362 0.07639187 0.07927424
 0.08337146 0.0884745  0.08759524 0.04922738]
test_loss at epoch19: 0.0012635751132001269
test_mses at epoch19: [0.09178577 0.02219614 0.02479477 0.09743686 0.00383154 0.00177599
 0.00668884 0.01318676 0.01613044 0.00840428]
test_maes at epoch19: [0.23757037 0.09833269 0.11858982 0.19824494 0.0467744  0.03335158
 0.06693297 0.10096135 0.10824799 0.07138493]

train_loss at epoch20: 0.0006096283025900892
train_mses at epoch20: [0.09047499 0.01628731 0.01875784 0.0199022  0.0161486  0.01635051
 0.01604327 0.01446615 0.01885357 0.00442096]
train_maes at epoch20: [0.22478771 0.08435315 0.09303812 0.0955297  0.08032241 0.08179102
 0.08462809 0.08348045 0.08951113 0.04794559]
test_loss at epoch20: 0.0014016390282739985
test_mses at epoch20: [0.07644305 0.02239086 0.03458551 0.12016138 0.00648643 0.00098987
 0.00667116 0.00661842 0.02863435 0.00837662]
test_maes at epoch20: [0.2304163  0.11862949 0.14525509 0.24434813 0.05723878 0.02419088
 0.07202517 0.06032195 0.08053939 0.07544502]

train_loss at epoch21: 0.0006183762650718873
train_mses at epoch21: [0.09729068 0.01454295 0.02401011 0.0177448  0.01730198 0.01667852
 0.01440229 0.01561924 0.01656219 0.00466898]
train_maes at epoch21: [0.23311049 0.08046352 0.1046396  0.09348312 0.07971461 0.08037255
 0.07943397 0.08704268 0.0842724  0.0490591 ]
test_loss at epoch21: 0.0012711410688117464
test_mses at epoch21: [0.06921814 0.04215559 0.02734083 0.06658378 0.0039164  0.00236706
 0.00629625 0.00745417 0.02248623 0.00810137]
test_maes at epoch21: [0.22225909 0.1125147  0.11643833 0.15374549 0.04992549 0.03739237
 0.07006508 0.0466468  0.11050981 0.0692355 ]

train_loss at epoch22: 0.0006008556241446987
train_mses at epoch22: [0.08846299 0.0135077  0.01723615 0.02035511 0.01402669 0.01653412
 0.01789999 0.01819933 0.01761734 0.00394713]
train_maes at epoch22: [0.21995719 0.07690421 0.08576165 0.0932032  0.07398079 0.0816072
 0.08157953 0.09563872 0.08526468 0.046165  ]
test_loss at epoch22: 0.001158063190969381
test_mses at epoch22: [0.10236442 0.02336869 0.01972964 0.04861496 0.00353988 0.00129931
 0.0065347  0.0085507  0.04014688 0.01118975]
test_maes at epoch22: [0.2649592  0.07867899 0.08791576 0.12219    0.05544261 0.02726506
 0.07059048 0.0783841  0.10006803 0.08969772]

train_loss at epoch23: 0.000579634989720789
train_mses at epoch23: [0.08981896 0.01579201 0.01407268 0.01361379 0.01527527 0.01989454
 0.01835011 0.01645628 0.01635716 0.00390147]
train_maes at epoch23: [0.21886784 0.07994969 0.08498117 0.08054147 0.07869865 0.08709759
 0.08188552 0.0904315  0.08132017 0.04704774]
test_loss at epoch23: 0.0014293161914386648
test_mses at epoch23: [0.16395564 0.04304649 0.05991712 0.05012061 0.00264738 0.00113361
 0.00744677 0.00530618 0.0227572  0.00902126]
test_maes at epoch23: [0.32759972 0.10657109 0.12875698 0.1146487  0.04690932 0.02514584
 0.07882696 0.04858194 0.08401601 0.07567794]

train_loss at epoch24: 0.0006109288958040007
train_mses at epoch24: [0.09473292 0.01952158 0.01647586 0.02208966 0.01735648 0.01576261
 0.0194945  0.01178515 0.01310565 0.00404149]
train_maes at epoch24: [0.22016856 0.0907788  0.0903881  0.09208783 0.0802189  0.08391571
 0.08811235 0.07938208 0.07586611 0.0468457 ]
test_loss at epoch24: 0.0013931978077806057
test_mses at epoch24: [0.1100244  0.02232958 0.03622681 0.09529621 0.00246517 0.0014101
 0.00187472 0.00666031 0.04188877 0.01228993]
test_maes at epoch24: [0.24854327 0.09986837 0.12261331 0.17663972 0.0422727  0.03037194
 0.03195187 0.05687714 0.11678919 0.09500707]

train_loss at epoch25: 0.0005528915591755921
train_mses at epoch25: [0.07822278 0.01556944 0.01554894 0.02177282 0.01355065 0.01299776
 0.01035303 0.01858112 0.01446504 0.00404565]
train_maes at epoch25: [0.20498665 0.07624529 0.08334235 0.10141769 0.07249475 0.07087862
 0.06840507 0.08760886 0.0785216  0.04498609]
test_loss at epoch25: 0.0011164529367964319
test_mses at epoch25: [0.12275038 0.03557946 0.04556088 0.05059009 0.00159382 0.00331485
 0.00215491 0.00537236 0.01985756 0.00628147]
test_maes at epoch25: [0.27254902 0.09289748 0.11520398 0.13121969 0.03515778 0.0488822
 0.04054752 0.05941236 0.07520416 0.05407214]

train_loss at epoch26: 0.0005504715100346886
train_mses at epoch26: [0.07287787 0.01299003 0.0176108  0.02009787 0.01406432 0.01355196
 0.01454761 0.01556418 0.01519196 0.00415691]
train_maes at epoch26: [0.20200217 0.07494947 0.09062537 0.09575792 0.07516636 0.07704989
 0.07404934 0.08398709 0.07524332 0.04683416]
test_loss at epoch26: 0.0016052843408381685
test_mses at epoch26: [0.12328932 0.02552511 0.0249308  0.08423985 0.00280862 0.0024914
 0.0015288  0.00785561 0.06216364 0.00566693]
test_maes at epoch26: [0.27524248 0.08867429 0.10728406 0.17197543 0.04502532 0.04532579
 0.02909125 0.04018703 0.10948058 0.05107597]

train_loss at epoch27: 0.0006045361601826834
train_mses at epoch27: [0.07913226 0.01424536 0.01597807 0.01988737 0.01984232 0.01145441
 0.01817594 0.01596134 0.02180541 0.00317091]
train_maes at epoch27: [0.20598981 0.0764974  0.08759165 0.08633892 0.08394431 0.07269271
 0.0832316  0.08456816 0.0900615  0.03980082]
test_loss at epoch27: 0.0014624723649405418
test_mses at epoch27: [0.07898082 0.0218951  0.02246436 0.09064166 0.00194472 0.00374154
 0.00099577 0.00633668 0.08286147 0.00749895]
test_maes at epoch27: [0.21863259 0.09241961 0.11143479 0.17249503 0.03725779 0.04973917
 0.02774127 0.03622063 0.13283182 0.0714579 ]

train_loss at epoch28: 0.0005253583386025213
train_mses at epoch28: [0.07516555 0.01314725 0.01676526 0.0152332  0.01411686 0.01406304
 0.01263177 0.01776311 0.01554972 0.00276283]
train_maes at epoch28: [0.20595676 0.07423704 0.08536527 0.08084481 0.07497407 0.07327522
 0.07533254 0.09014943 0.07701493 0.03891432]
test_loss at epoch28: 0.0014774096099303125
test_mses at epoch28: [0.14301998 0.05132877 0.08579461 0.02852962 0.00270402 0.00229497
 0.00115881 0.00556031 0.04761853 0.00916759]
test_maes at epoch28: [0.31916624 0.1234716  0.17888882 0.11240183 0.04130394 0.04131535
 0.02411457 0.04766834 0.1122179  0.08113667]

train_loss at epoch29: 0.0005304720661444391
train_mses at epoch29: [0.08103125 0.0138938  0.02063515 0.01259967 0.01336026 0.01382948
 0.0152215  0.0156025  0.01351341 0.00321568]
train_maes at epoch29: [0.20974423 0.07900493 0.09749636 0.07845483 0.07182429 0.07665557
 0.07613113 0.08418249 0.07608177 0.03995272]
test_loss at epoch29: 0.0011149809220211302
test_mses at epoch29: [0.10464428 0.00795519 0.01621751 0.09764017 0.002851   0.00074096
 0.00050475 0.01696373 0.02800619 0.00655221]
test_maes at epoch29: [0.21941576 0.06416546 0.09245584 0.19797029 0.03542589 0.02136726
 0.01770083 0.10893313 0.09547727 0.064448  ]

train_loss at epoch30: 0.0005111480627129686
train_mses at epoch30: [0.0839835  0.01387818 0.0161318  0.01634974 0.01336688 0.01251454
 0.01193538 0.01297494 0.01687852 0.00301744]
train_maes at epoch30: [0.20948513 0.07399305 0.08979629 0.08155291 0.06723109 0.07266095
 0.07063166 0.07950807 0.07853463 0.04002361]
test_loss at epoch30: 0.001397596335315958
test_mses at epoch30: [0.18755122 0.04166678 0.04452594 0.08177688 0.00112379 0.00038228
 0.00094482 0.00983961 0.04179607 0.00639518]
test_maes at epoch30: [0.36114177 0.09862361 0.10721013 0.1657261  0.02790862 0.01694282
 0.02403313 0.07988757 0.10164263 0.06146195]

train_loss at epoch31: 0.00047293000983351723
train_mses at epoch31: [0.06840809 0.01270808 0.01679607 0.01535149 0.012689   0.01190613
 0.0130409  0.01291035 0.01155709 0.00300142]
train_maes at epoch31: [0.19650645 0.07383846 0.0828293  0.07849888 0.07058512 0.06848102
 0.0698225  0.07999407 0.0664028  0.03890757]
test_loss at epoch31: 0.0012535166411482273
test_mses at epoch31: [0.08952739 0.01696884 0.04324921 0.09792247 0.00157349 0.00080289
 0.00104381 0.00492212 0.03678136 0.00584797]
test_maes at epoch31: [0.23026643 0.09880075 0.10615238 0.1763198  0.03030869 0.02265525
 0.02665903 0.03635443 0.08537346 0.06070875]

train_loss at epoch32: 0.0004915804874667145
train_mses at epoch32: [0.07312342 0.01512328 0.01873733 0.01559298 0.01472113 0.01189145
 0.01112333 0.0115395  0.01166333 0.00268243]
train_maes at epoch32: [0.20184557 0.07480113 0.08439586 0.08021195 0.06894291 0.06957184
 0.06715161 0.07438339 0.06987503 0.03607235]
test_loss at epoch32: 0.0009910800355546019
test_mses at epoch32: [0.07985565 0.01765921 0.01226889 0.06174134 0.00179769 0.00128607
 0.00085253 0.00179496 0.05479195 0.0069721 ]
test_maes at epoch32: [0.21438875 0.09316723 0.07771171 0.11912475 0.03193889 0.0309889
 0.02474316 0.03487312 0.11165544 0.06651504]

train_loss at epoch33: 0.0005109398501944985
train_mses at epoch33: [0.08102412 0.0135154  0.01850743 0.01482315 0.01290591 0.01415779
 0.01172939 0.01313399 0.0152936  0.00287736]
train_maes at epoch33: [0.20626821 0.0734505  0.08431557 0.08117209 0.07240809 0.07180166
 0.06980782 0.07183545 0.07371782 0.03782138]
test_loss at epoch33: 0.0009299289989978709
test_mses at epoch33: [0.06646613 0.0490405  0.01843946 0.05945152 0.00148895 0.00033471
 0.00086916 0.00360171 0.02341669 0.0060066 ]
test_maes at epoch33: [0.20582075 0.12538451 0.10101259 0.12003011 0.0242977  0.01433651
 0.02043729 0.04746927 0.10216015 0.06209006]

train_loss at epoch34: 0.00048243891858951524
train_mses at epoch34: [0.06466368 0.0207987  0.01229595 0.01411381 0.01159068 0.01169676
 0.01086176 0.01272533 0.0141762  0.00273151]
train_maes at epoch34: [0.185578   0.08731248 0.07624481 0.07731136 0.06627493 0.06713009
 0.06575268 0.07703463 0.07545508 0.03678116]
test_loss at epoch34: 0.0008347483272565172
test_mses at epoch34: [0.06054309 0.02990899 0.01197411 0.02521945 0.00146767 0.00268425
 0.00090998 0.00259615 0.01482087 0.00630446]
test_maes at epoch34: [0.19491708 0.1125922  0.08049655 0.09369003 0.0266994  0.04262023
 0.02483587 0.03592893 0.06939265 0.06200213]

train_loss at epoch35: 0.0004811435108112687
train_mses at epoch35: [0.07149761 0.01421153 0.01291243 0.01378926 0.01438241 0.01118189
 0.01208071 0.01118385 0.01932605 0.00241664]
train_maes at epoch35: [0.19248697 0.07657871 0.07577464 0.07521233 0.07034008 0.06613012
 0.06963259 0.0708536  0.08117273 0.03578195]
test_loss at epoch35: 0.0011127684463528877
test_mses at epoch35: [0.06277977 0.03421237 0.01796273 0.02721092 0.0015741  0.00278828
 0.00089724 0.0051341  0.08354185 0.00523456]
test_maes at epoch35: [0.17963279 0.09510858 0.07881276 0.09327224 0.02843772 0.04734707
 0.02331058 0.05286486 0.15393105 0.04239229]

train_loss at epoch36: 0.0004833410865567783
train_mses at epoch36: [0.0740356  0.01534434 0.01625772 0.01428439 0.01609846 0.01064983
 0.0104743  0.01174    0.01467512 0.00245555]
train_maes at epoch36: [0.19601868 0.07254602 0.08093366 0.07447732 0.0733641  0.06633884
 0.06533422 0.07413488 0.07402948 0.0350485 ]
test_loss at epoch36: 0.0014140795658719033
test_mses at epoch36: [0.17289639 0.0451543  0.10224034 0.05559798 0.00115035 0.00559499
 0.00067905 0.00322756 0.01541414 0.00845458]
test_maes at epoch36: [0.32935133 0.10444385 0.15683275 0.1218507  0.02858032 0.06095304
 0.0207547  0.0339891  0.06793708 0.07379357]

train_loss at epoch37: 0.0005323461521951917
train_mses at epoch37: [0.08696698 0.01537922 0.0226617  0.0180456  0.01254888 0.01145786
 0.0133579  0.009879   0.01556729 0.00233194]
train_maes at epoch37: [0.21556659 0.07492907 0.09291708 0.08103091 0.06558517 0.06889407
 0.06944199 0.06719866 0.0706825  0.03480673]
test_loss at epoch37: 0.0009400272999513657
test_mses at epoch37: [0.0536689  0.01163861 0.05064419 0.02735116 0.00419874 0.00128394
 0.00210525 0.00349002 0.0082809  0.00951811]
test_maes at epoch37: [0.1892342  0.07982472 0.12143228 0.09257018 0.04632081 0.03024604
 0.03636238 0.04086229 0.07858416 0.08481194]

train_loss at epoch38: 0.0004961748193810753
train_mses at epoch38: [0.07179613 0.02179966 0.01580414 0.01329359 0.01388422 0.00874647
 0.01127783 0.01328728 0.01215275 0.00262567]
train_maes at epoch38: [0.19394186 0.09377053 0.08071686 0.07259887 0.06687614 0.05909187
 0.06626283 0.07425678 0.06880371 0.03519618]
test_loss at epoch38: 0.0010250225821708111
test_mses at epoch38: [0.06942876 0.03925714 0.02329489 0.02903537 0.00122135 0.0006825
 0.00683143 0.00747792 0.03795485 0.00606929]
test_maes at epoch38: [0.20712951 0.09957244 0.08898018 0.08971937 0.02940586 0.01947962
 0.0742881  0.06591284 0.10162848 0.06271959]

train_loss at epoch39: 0.00044247411449063333
train_mses at epoch39: [0.06728024 0.01156599 0.01293919 0.01641744 0.01198877 0.01203894
 0.00873438 0.0146283  0.01138104 0.00226394]
train_maes at epoch39: [0.19172073 0.06728073 0.07622917 0.08102688 0.06497125 0.06908962
 0.06182419 0.07912226 0.06482504 0.03406204]
test_loss at epoch39: 0.0010280740160019473
test_mses at epoch39: [0.06042274 0.02623816 0.01831421 0.03229587 0.00122317 0.00072981
 0.00124342 0.01527579 0.01853088 0.00580038]
test_maes at epoch39: [0.19813097 0.08044365 0.11003825 0.1000609  0.02700078 0.02262224
 0.02795395 0.04570771 0.07153572 0.06285837]

train_loss at epoch40: 0.00042487391483712384
train_mses at epoch40: [0.07256082 0.0114065  0.01369964 0.01292614 0.01014781 0.00945593
 0.01194594 0.0135401  0.01186863 0.00203304]
train_maes at epoch40: [0.19995543 0.06402029 0.07525789 0.07287692 0.06012098 0.06212057
 0.06937921 0.07525308 0.06594237 0.03170835]
test_loss at epoch40: 0.0011876387264024705
test_mses at epoch40: [0.06424149 0.03425884 0.01917803 0.10963155 0.00093504 0.00032683
 0.00065295 0.00667542 0.01135649 0.00746972]
test_maes at epoch40: [0.21553805 0.1130513  0.11470616 0.21737873 0.02355715 0.01428773
 0.01769232 0.04440218 0.06136352 0.077468  ]

train_loss at epoch41: 0.0004489820121113766
train_mses at epoch41: [0.06233833 0.01027603 0.0162816  0.01218129 0.01126102 0.01310613
 0.01331434 0.01282519 0.0122586  0.00252538]
train_maes at epoch41: [0.18204789 0.06525024 0.08400072 0.07196631 0.06491456 0.06704274
 0.07013532 0.07409466 0.0677766  0.03542604]
test_loss at epoch41: 0.0011313948879375102
test_mses at epoch41: [0.05112023 0.01349654 0.02117934 0.05531692 0.00099032 0.00099741
 0.00103405 0.00534801 0.07043622 0.00895612]
test_maes at epoch41: [0.17586742 0.07090455 0.08947274 0.13876567 0.02064626 0.02538179
 0.0243456  0.0579496  0.12332256 0.08057245]

train_loss at epoch42: 0.00042565111343332744
train_mses at epoch42: [0.05719211 0.01288471 0.01320893 0.01343329 0.00999121 0.01119438
 0.01126986 0.0108373  0.01333132 0.00225267]
train_maes at epoch42: [0.17445682 0.07113898 0.07500378 0.07465566 0.05716781 0.06350774
 0.0642943  0.06765978 0.06967069 0.03354468]
test_loss at epoch42: 0.0013898407961142824
test_mses at epoch42: [1.73711459e-01 5.04654533e-02 8.38877095e-02 5.00832929e-02
 2.25854643e-03 1.54610928e-04 3.54198804e-03 7.10792934e-03
 8.72237226e-03 8.60953576e-03]
test_maes at epoch42: [0.33667673 0.12371753 0.14194518 0.11817424 0.03293887 0.01032436
 0.05523684 0.06444489 0.07785896 0.07776002]

train_loss at epoch43: 0.00043082807431037124
train_mses at epoch43: [0.06798398 0.01042313 0.01263165 0.0142418  0.01064375 0.01308464
 0.01426138 0.00973165 0.01250453 0.00195343]
train_maes at epoch43: [0.19221086 0.06717216 0.07395341 0.07398032 0.06033944 0.06933233
 0.06792258 0.06741947 0.06879114 0.03151056]
test_loss at epoch43: 0.0012335844952216807
test_mses at epoch43: [0.07272123 0.02726251 0.03366296 0.07373582 0.00384633 0.00138894
 0.00088022 0.00826511 0.02763085 0.00508337]
test_maes at epoch43: [0.21091241 0.0978116  0.1075456  0.13591257 0.04543239 0.03116641
 0.02540313 0.0407396  0.07328864 0.05125991]

train_loss at epoch44: 0.00045014030740972845
train_mses at epoch44: [0.06895962 0.01053809 0.01603185 0.01612125 0.01279204 0.01136654
 0.01381224 0.00851445 0.01087605 0.00233033]
train_maes at epoch44: [0.19045091 0.06658363 0.07591517 0.07579139 0.06689322 0.06738884
 0.06714635 0.06515153 0.06601555 0.03283665]
test_loss at epoch44: 0.0009712790178650237
test_mses at epoch44: [0.06527025 0.00897752 0.02648428 0.08307655 0.00191176 0.00066241
 0.00110996 0.00537703 0.02578292 0.00385839]
test_maes at epoch44: [0.19443543 0.07891831 0.10915432 0.17102014 0.02993297 0.02121451
 0.02588255 0.05573043 0.07836029 0.04109655]

train_loss at epoch45: 0.00040620696567434896
train_mses at epoch45: [0.06042581 0.01080995 0.01019934 0.01384595 0.01239409 0.01189377
 0.01079041 0.01030454 0.01208648 0.00209791]
train_maes at epoch45: [0.17807876 0.06409436 0.06732755 0.07197306 0.0629083  0.06717788
 0.06095509 0.06973523 0.06679304 0.0313165 ]
test_loss at epoch45: 0.0014677438488666047
test_mses at epoch45: [0.22095273 0.04724594 0.07138358 0.0998474  0.00205731 0.00066713
 0.00072534 0.00697075 0.01697275 0.00462136]
test_maes at epoch45: [0.37262143 0.10279632 0.12532765 0.14532351 0.03281819 0.02291887
 0.02266107 0.05794971 0.11377582 0.04639347]

train_loss at epoch46: 0.000449274158838423
train_mses at epoch46: [0.07429452 0.01161195 0.01834601 0.01476527 0.01235269 0.00983084
 0.0100369  0.01064082 0.01259781 0.00186668]
train_maes at epoch46: [0.19562536 0.06429794 0.08507445 0.07165102 0.06334982 0.06216781
 0.06486795 0.06640055 0.06793383 0.03009727]
test_loss at epoch46: 0.001031122289280942
test_mses at epoch46: [0.10845604 0.01035377 0.04122789 0.09638005 0.00120346 0.00095687
 0.00284139 0.00835259 0.01050671 0.00416304]
test_maes at epoch46: [0.23748778 0.06994665 0.11264429 0.18953254 0.02994827 0.02567585
 0.04663688 0.0750479  0.07149818 0.04964897]

train_loss at epoch47: 0.00044490192135042964
train_mses at epoch47: [0.06392462 0.01039105 0.01439415 0.0164722  0.01255145 0.00962843
 0.01266542 0.01141325 0.01217167 0.00192721]
train_maes at epoch47: [0.18662796 0.06412571 0.07532488 0.0785783  0.06283986 0.06247213
 0.06625026 0.06869742 0.06210085 0.03029433]
test_loss at epoch47: 0.0008647605291310143
test_mses at epoch47: [0.06279284 0.01602747 0.0231351  0.01849376 0.00294148 0.0003454
 0.00190264 0.0024993  0.00904693 0.00443167]
test_maes at epoch47: [0.21347031 0.06666599 0.12119894 0.06870398 0.03779071 0.01533228
 0.03820575 0.02962241 0.08119513 0.04778376]

train_loss at epoch48: 0.00045113948154005596
train_mses at epoch48: [0.06238092 0.01305669 0.01444686 0.01686898 0.00940226 0.01134205
 0.0127071  0.01175228 0.01305427 0.00211386]
train_maes at epoch48: [0.18538736 0.07268517 0.07376523 0.07807943 0.06051657 0.06536252
 0.06700455 0.06904254 0.06545761 0.03223761]
test_loss at epoch48: 0.0009971725100532491
test_mses at epoch48: [0.06130801 0.02734313 0.03079647 0.02576716 0.0004895  0.00014713
 0.00418953 0.00474972 0.02152571 0.00395133]
test_maes at epoch48: [0.18450964 0.08226028 0.0867335  0.08170891 0.01786168 0.01039674
 0.05404961 0.03155755 0.06607611 0.03378548]

train_loss at epoch49: 0.0004067172103145338
train_mses at epoch49: [0.05466771 0.01224646 0.01353828 0.01430925 0.01131294 0.01039258
 0.00986158 0.01072279 0.01051205 0.00184202]
train_maes at epoch49: [0.16686619 0.06840583 0.07370762 0.07364068 0.05954093 0.06236676
 0.05995386 0.06538487 0.05657678 0.03073771]
test_loss at epoch49: 0.0011940399640576636
test_mses at epoch49: [0.15819216 0.01867433 0.02163445 0.10348043 0.00131736 0.00033161
 0.00156861 0.0083464  0.01659909 0.00419381]
test_maes at epoch49: [0.30438708 0.08079436 0.07733343 0.16577437 0.03299849 0.01449263
 0.03350526 0.05695287 0.07441077 0.05185604]

