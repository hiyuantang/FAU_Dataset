(torch-gpu) yuantang@Yuans-MBP code % python train_facegen_alt.py --seed 554 --dataset_root /Volumes/Yuan-T7/Datasets/face_gen --resume /Volumes/Yuan-T7/FAU_models/checkpoint_epoch_init.pth
PyTorch Version:  2.0.0
Torchvision Version:  0.15.1
Initializing Datasets and Dataloaders...
Train Sets: ['aw' 'e']
Test Sets: ['a' 'eb']
Total Number of Train Sets: 92
Total Number of Test Sets: 92

train_loss at epoch0: 0.039801953927330345
train_mses at epoch0: [4.18400034 1.1487829  1.16142976 1.15957289 1.17184701 1.00055364
 1.14648512 1.15694541 1.14732398 0.02631173]
train_maes at epoch0: [1.59180551 0.36618163 0.5382051  0.41769051 0.35911392 0.49352781
 0.36452875 0.3883174  0.38274674 0.07744244]
test_loss at epoch0: 0.03887109004932901
test_mses at epoch0: [3.83808082 1.12705993 1.15650536 1.09859154 1.16987303 0.97649323
 1.12394915 1.13030655 1.12538117 0.02384157]
test_maes at epoch0: [1.54725606 0.39713094 0.61590829 0.44220166 0.37671208 0.56472545
 0.39344858 0.42778299 0.39756752 0.0466053 ]

train_loss at epoch1: 0.03796539164107779
train_mses at epoch1: [3.70074675 1.11499822 1.16066499 1.10452058 1.15674402 0.94224293
 1.1011034  1.09314757 1.09420009 0.02546716]
train_maes at epoch1: [1.57659231 0.41805958 0.64953727 0.51304785 0.38230972 0.55128261
 0.41054836 0.44445044 0.42084113 0.07818005]
test_loss at epoch1: 0.037826234879701034
test_mses at epoch1: [3.36201058 1.09294656 1.16665741 1.06682552 1.14772796 0.95982541
 1.08720162 1.09311351 1.08543582 0.02407154]
test_maes at epoch1: [1.56265245 0.47058377 0.72034057 0.56203833 0.42005561 0.62677552
 0.44507474 0.50525356 0.45423336 0.04525674]

train_loss at epoch2: 0.03666652544685032
train_mses at epoch2: [3.23276362 1.06389455 1.12526433 1.0818874  1.12733589 0.87545245
 1.07281786 1.06756007 1.05585253 0.02867423]
train_maes at epoch2: [1.58263095 0.48379595 0.70986462 0.62711796 0.42317931 0.56749893
 0.4757675  0.5164883  0.50151941 0.08577934]
test_loss at epoch2: 0.03697051561397055
test_mses at epoch2: [3.20069018 1.06255574 1.1755353  1.0568729  1.12095298 0.90783104
 1.05778984 1.07291669 1.05341499 0.0253161 ]
test_maes at epoch2: [1.58811962 0.52247299 0.75589878 0.63342095 0.4754734  0.60774729
 0.50473521 0.58600148 0.5269178  0.04952222]

train_loss at epoch3: 0.03603658598402272
train_mses at epoch3: [3.15911671 1.01924599 1.10009565 1.08852296 1.10346452 0.8092674
 1.0381479  1.07075378 1.0541525  0.02691919]
train_maes at epoch3: [1.59873018 0.53077498 0.70099217 0.67275442 0.4780351  0.50334539
 0.51181894 0.57173021 0.57686308 0.07458258]
test_loss at epoch3: 0.0359616422134897
test_mses at epoch3: [3.10282317 1.01965572 1.1555364  1.01810358 1.10612356 0.83122916
 1.04097886 1.06591619 1.03951595 0.02575708]
test_maes at epoch3: [1.56791335 0.53732168 0.72150575 0.58901403 0.51690314 0.53829612
 0.52344935 0.61414848 0.55772992 0.0532664 ]

train_loss at epoch4: 0.03362147963565329
train_mses at epoch4: [3.07063881 0.95563256 1.0349616  0.99793624 1.08113896 0.62257798
 1.02282417 1.0056422  1.02687839 0.03144166]
train_maes at epoch4: [1.55493946 0.53948076 0.66347949 0.60992902 0.51717282 0.41351852
 0.54764121 0.58210913 0.60005672 0.09026108]
test_loss at epoch4: 0.03483486629050711
test_mses at epoch4: [2.98598717 0.96189107 1.15192234 0.96851926 1.09654334 0.74575178
 1.01879032 1.04488419 1.02713801 0.02704631]
test_maes at epoch4: [1.55026641 0.57305173 0.71377274 0.55631499 0.54945258 0.48049785
 0.53839424 0.60166624 0.55649448 0.0615109 ]

train_loss at epoch5: 0.03194329207358153
train_mses at epoch5: [3.02302092 0.87710257 1.06732413 0.93276317 1.07150163 0.51955974
 1.0030108  0.95198226 0.96268074 0.03357406]
train_maes at epoch5: [1.5700703  0.55492571 0.66521299 0.57955905 0.54214058 0.35738825
 0.56686764 0.5561885  0.5704758  0.10103507]
test_loss at epoch5: 0.03349259107009224
test_mses at epoch5: [2.88245397 0.8977423  1.1647221  0.90068653 1.08614028 0.64053704
 0.98886549 1.01409041 1.00813972 0.03053076]
test_maes at epoch5: [1.539419   0.59149693 0.74023611 0.53117373 0.57743693 0.46882855
 0.55521791 0.59466081 0.5462305  0.07878653]

train_loss at epoch6: 0.030061731519906418
train_mses at epoch6: [2.88804433 0.80010547 1.04732427 0.84841987 1.02551787 0.41658536
 0.94013454 0.9461511  0.95266266 0.04245106]
train_maes at epoch6: [1.5493926  0.53148591 0.67552073 0.54085472 0.54642375 0.35123657
 0.57733869 0.52777532 0.56109456 0.120072  ]
test_loss at epoch6: 0.031768072558485946
test_mses at epoch6: [2.73641988 0.8171452  1.17450583 0.81424934 1.07285398 0.52188179
 0.95139379 0.97098492 0.97599418 0.03549306]
test_maes at epoch6: [1.5091999  0.59693388 0.74761354 0.48956307 0.58839802 0.452907
 0.54716532 0.56541054 0.5287525  0.099182  ]

train_loss at epoch7: 0.02764227921548097
train_mses at epoch7: [2.37243116 0.67888197 0.90903245 0.74140834 1.01301329 0.27875547
 0.90628181 0.88543826 0.90054026 0.05159715]
train_maes at epoch7: [1.39505147 0.45905226 0.56809153 0.5196731  0.54892257 0.29659054
 0.56872664 0.50464541 0.55761192 0.14509586]
test_loss at epoch7: 0.029514576429906098
test_mses at epoch7: [2.41757727 0.70256435 1.16084766 0.69715498 1.05188502 0.4128118
 0.90680463 0.92097148 0.93367929 0.04209478]
test_maes at epoch7: [1.39530379 0.53478243 0.68793015 0.46049892 0.56664692 0.42656834
 0.50956242 0.53856276 0.49646099 0.12465154]

train_loss at epoch8: 0.02502125242482061
train_mses at epoch8: [2.32977746 0.49711642 0.88865065 0.62081278 0.9847125  0.27518679
 0.81522142 0.81953412 0.8397277  0.05846154]
train_maes at epoch8: [1.37421786 0.35896162 0.51116382 0.49588073 0.52625821 0.35561297
 0.5469575  0.48558866 0.53961768 0.16783684]
test_loss at epoch8: 0.027623970223509747
test_mses at epoch8: [2.31257879 0.62982882 1.17656016 0.57801811 1.01411432 0.38056
 0.84387539 0.84557437 0.87691232 0.0447558 ]
test_maes at epoch8: [1.36933184 0.54974951 0.72364728 0.38093276 0.55689858 0.469128
 0.42273502 0.51620957 0.48931281 0.12484393]

train_loss at epoch9: 0.022586905437967052
train_mses at epoch9: [2.05886212 0.38510353 0.89861449 0.54521248 0.88078437 0.19961886
 0.71753073 0.74688464 0.74666393 0.07561665]
train_maes at epoch9: [1.27424603 0.37183681 0.52381034 0.40731925 0.50042186 0.33440534
 0.46833647 0.47649731 0.48130495 0.1835434 ]
test_loss at epoch9: 0.02622624000777369
test_mses at epoch9: [2.10955405 0.59350123 1.15780239 0.49624577 0.97468333 0.36737333
 0.82338511 0.77729086 0.84021034 0.04512369]
test_maes at epoch9: [1.27719551 0.55044496 0.75482774 0.32056153 0.5239847  0.47125132
 0.35898962 0.51576188 0.44397585 0.12644393]

train_loss at epoch10: 0.019378875908644302
train_mses at epoch10: [1.87691851 0.29219631 0.75044181 0.34790732 0.86035295 0.29336336
 0.61110899 0.64821933 0.65104455 0.07873352]
train_maes at epoch10: [1.10580251 0.37379376 0.52240947 0.37496483 0.49327477 0.41480274
 0.43502656 0.47565918 0.45922126 0.20426404]
test_loss at epoch10: 0.023598858843679012
test_mses at epoch10: [1.79216014 0.47357371 1.07550922 0.37390311 0.92254117 0.38214887
 0.70458659 0.71080762 0.77871373 0.06196359]
test_maes at epoch10: [1.08528613 0.48077993 0.6896051  0.31916964 0.5127953  0.49058596
 0.35725166 0.52509201 0.44494751 0.19085948]

train_loss at epoch11: 0.01660189388886742
train_mses at epoch11: [1.75925954 0.27326539 0.64711255 0.25624108 0.73390911 0.28295722
 0.53052255 0.5375394  0.51261188 0.06063694]
train_maes at epoch11: [0.98543631 0.40804046 0.49388952 0.34087384 0.46430556 0.41079689
 0.45586859 0.45523968 0.42109617 0.18108031]
test_loss at epoch11: 0.022363048532734745
test_mses at epoch11: [1.66003738 0.52912134 0.9813774  0.37778898 0.88266755 0.28403954
 0.7106482  0.69373775 0.69349367 0.04723659]
test_maes at epoch11: [1.00996746 0.538216   0.61873706 0.30459971 0.53164461 0.3946111
 0.36736395 0.53463029 0.4368977  0.1581925 ]

train_loss at epoch12: 0.014735559730426125
train_mses at epoch12: [1.59330074 0.23399902 0.5465109  0.22276007 0.65064293 0.22144501
 0.49377093 0.51962756 0.47015363 0.0444843 ]
train_maes at epoch12: [0.92887629 0.37274624 0.44824824 0.30774156 0.45929577 0.37061117
 0.42498464 0.46567816 0.4087512  0.13822069]
test_loss at epoch12: 0.02216177206972371
test_mses at epoch12: [1.53273764 0.50954571 1.00926286 0.37120359 0.83874717 0.42614201
 0.63729107 0.68553969 0.65806197 0.03614453]
test_maes at epoch12: [0.97663247 0.53341444 0.76426495 0.31177015 0.4815015  0.52432595
 0.3688241  0.56842356 0.4507891  0.12739327]

train_loss at epoch13: 0.01327031170544417
train_mses at epoch13: [1.688888   0.19411474 0.47858708 0.26316465 0.6306448  0.20486771
 0.36884962 0.46633009 0.37070005 0.04805198]
train_maes at epoch13: [0.96341217 0.3319555  0.41162675 0.32895485 0.41932885 0.37156965
 0.37309872 0.45402272 0.37048584 0.15087481]
test_loss at epoch13: 0.01963252217873283
test_mses at epoch13: [1.48246914 0.41924539 0.8568457  0.32987992 0.80948271 0.27888513
 0.59234747 0.68656481 0.55643053 0.03769585]
test_maes at epoch13: [0.97744822 0.46678538 0.5458298  0.27472449 0.55014257 0.34593682
 0.35258234 0.57843026 0.40922239 0.13229646]

train_loss at epoch14: 0.011150833057320637
train_mses at epoch14: [1.30425633 0.16270456 0.44382817 0.25956904 0.55829083 0.15613914
 0.26967806 0.35896457 0.28931785 0.04503303]
train_maes at epoch14: [0.83884337 0.30646621 0.3537485  0.30270155 0.44671576 0.30134012
 0.34362655 0.4007542  0.36169595 0.15238614]
test_loss at epoch14: 0.018625841192577198
test_mses at epoch14: [1.42671726 0.40357809 0.81712811 0.32964888 0.7686209  0.284864
 0.53819089 0.60311926 0.55736006 0.03592846]
test_maes at epoch14: [0.96839301 0.45626617 0.52890753 0.26970195 0.52162994 0.36057115
 0.35589593 0.50758867 0.41965173 0.12425451]

train_loss at epoch15: 0.010001151620045952
train_mses at epoch15: [1.30024081 0.15265899 0.38446738 0.22661889 0.42002146 0.11810569
 0.30712444 0.3251895  0.30251517 0.05372905]
train_maes at epoch15: [0.88548607 0.28104515 0.40389486 0.28524048 0.38650409 0.25617946
 0.34296352 0.38418385 0.36270334 0.17318821]
test_loss at epoch15: 0.018431605204291965
test_mses at epoch15: [1.40537746 0.41266382 0.79547082 0.33019037 0.71256409 0.34450234
 0.55010387 0.57761347 0.53958452 0.03696427]
test_maes at epoch15: [1.01720662 0.4723507  0.61087054 0.26847943 0.50014194 0.44833177
 0.37526462 0.50502537 0.44483974 0.12819152]

train_loss at epoch16: 0.008290910040554792
train_mses at epoch16: [1.21309089 0.098334   0.31476193 0.17926776 0.3583888  0.11657108
 0.29198634 0.26315001 0.20375627 0.04837699]
train_maes at epoch16: [0.83114023 0.24348752 0.33819419 0.27432609 0.35138389 0.26537962
 0.35196173 0.3270446  0.26659598 0.16307272]
test_loss at epoch16: 0.01736446210871572
test_mses at epoch16: [1.38330137 0.32530547 0.7495055  0.33310808 0.6787732  0.28501209
 0.59259559 0.5619527  0.49094791 0.03700835]
test_maes at epoch16: [0.88457637 0.35827331 0.39271182 0.25969608 0.47061733 0.3744421
 0.39177884 0.48005742 0.41171493 0.1296718 ]

train_loss at epoch17: 0.00797428125920503
train_mses at epoch17: [1.09079948 0.10265814 0.32669089 0.23178985 0.27973637 0.08567931
 0.25547159 0.2627169  0.20101814 0.05142993]
train_maes at epoch17: [0.7669125  0.2431936  0.35201657 0.29880551 0.30953919 0.23047785
 0.2917594  0.33235766 0.28237147 0.1593931 ]
test_loss at epoch17: 0.017062738861726677
test_mses at epoch17: [1.28944584 0.3718984  0.69323871 0.34484831 0.64524393 0.26388018
 0.60354351 0.54625871 0.47678048 0.04086899]
test_maes at epoch17: [0.91168891 0.4390586  0.40837373 0.26157587 0.47934763 0.35322364
 0.40230184 0.45452556 0.40816982 0.14837112]

train_loss at epoch18: 0.008099030865275341
train_mses at epoch18: [1.24991873 0.11029137 0.24802362 0.27714941 0.28873214 0.09933716
 0.27485812 0.26213347 0.19304435 0.04874716]
train_maes at epoch18: [0.81664848 0.25696195 0.29972766 0.33251154 0.30218528 0.24740457
 0.32472526 0.34186765 0.27510641 0.15164232]
test_loss at epoch18: 0.01658618547346281
test_mses at epoch18: [1.25785057 0.39734749 0.66366505 0.2994577  0.60614249 0.35479039
 0.52221111 0.53204661 0.45746898 0.04475807]
test_maes at epoch18: [0.94115392 0.47451039 0.49321472 0.26155687 0.50792561 0.47132136
 0.36947012 0.45734101 0.40941174 0.16107631]

train_loss at epoch19: 0.007047204867653225
train_mses at epoch19: [0.83427942 0.09127525 0.2437728  0.21158778 0.18931156 0.07838309
 0.29222457 0.27707804 0.15238677 0.04581132]
train_maes at epoch19: [0.65972776 0.21432517 0.28551501 0.2765824  0.25488303 0.21720963
 0.31177363 0.28760047 0.23961038 0.1556914 ]
test_loss at epoch19: 0.01732629569976226
test_mses at epoch19: [1.50126155 0.28699657 0.73300235 0.39331856 0.63540485 0.27639341
 0.61958555 0.64060361 0.42522684 0.03023699]
test_maes at epoch19: [0.86192002 0.32009611 0.37181681 0.27166051 0.52876352 0.37372497
 0.36944781 0.57361711 0.34364853 0.10729058]

train_loss at epoch20: 0.006547523581463358
train_mses at epoch20: [0.8484372  0.07198096 0.24012046 0.2139188  0.17729272 0.06417635
 0.24689932 0.30504601 0.12827536 0.03926858]
train_maes at epoch20: [0.6461083  0.17894824 0.30557315 0.25854533 0.24067453 0.18959565
 0.25465452 0.33812078 0.23319641 0.13325367]
test_loss at epoch20: 0.015623874638391577
test_mses at epoch20: [1.23508599 0.30253444 0.69196555 0.30443497 0.56474719 0.2420724
 0.62346101 0.56979474 0.3281931  0.02702921]
test_maes at epoch20: [0.85289331 0.38877328 0.37214582 0.25528086 0.50033575 0.34452162
 0.40355559 0.51793016 0.32460988 0.09304068]

train_loss at epoch21: 0.00634154240074365
train_mses at epoch21: [0.72134869 0.07171481 0.28816701 0.15399412 0.14999245 0.04158433
 0.24179551 0.26578946 0.20377378 0.03720272]
train_maes at epoch21: [0.61619305 0.18273944 0.30054367 0.23912846 0.23929912 0.1559895
 0.28838651 0.30433658 0.27428809 0.13031353]
test_loss at epoch21: 0.015444426756838093
test_mses at epoch21: [1.1792309  0.29355944 0.65841472 0.30374283 0.52696184 0.40370962
 0.51461784 0.52134178 0.37797843 0.02685468]
test_maes at epoch21: [0.84380199 0.37833862 0.3908081  0.24990346 0.49439176 0.53688507
 0.36125409 0.47392855 0.34907133 0.09306101]

train_loss at epoch22: 0.006547478230103203
train_mses at epoch22: [0.89386447 0.05048892 0.21458947 0.23052899 0.14097907 0.07673587
 0.2375638  0.26744015 0.22098025 0.03590976]
train_maes at epoch22: [0.68957345 0.15335411 0.27550365 0.26029268 0.22527257 0.2163803
 0.26882881 0.29782003 0.2835855  0.13356835]
test_loss at epoch22: 0.016931070581726406
test_mses at epoch22: [1.37213768 0.25525132 0.7359244  0.35206855 0.52212873 0.36280626
 0.56526776 0.60801352 0.54185245 0.02526839]
test_maes at epoch22: [0.84111937 0.3134189  0.37052107 0.24973829 0.45012232 0.50572852
 0.35056079 0.56696151 0.4445004  0.08486207]

train_loss at epoch23: 0.005492209416368733
train_mses at epoch23: [0.84244571 0.05249161 0.17346449 0.1784056  0.12403611 0.0540388
 0.19340015 0.24090442 0.18510336 0.03240787]
train_maes at epoch23: [0.63922419 0.16026114 0.24140633 0.23233759 0.2155253  0.17817781
 0.23846824 0.29454962 0.25469618 0.12364696]
test_loss at epoch23: 0.015365108523679815
test_mses at epoch23: [1.22095114 0.243893   0.7393258  0.28463564 0.49426972 0.2401578
 0.55506401 0.62561068 0.39150957 0.02864497]
test_maes at epoch23: [0.86795559 0.31703436 0.37257142 0.26440527 0.46943768 0.35651703
 0.34996631 0.58586216 0.32999174 0.10173103]

train_loss at epoch24: 0.004691990416335023
train_mses at epoch24: [0.63936643 0.04194177 0.1852819  0.16164435 0.12682863 0.06212494
 0.15478256 0.16175931 0.13707017 0.02866857]
train_maes at epoch24: [0.5933123  0.15448708 0.24583652 0.2431971  0.21907934 0.18723779
 0.22967589 0.24789699 0.24195131 0.10789285]
test_loss at epoch24: 0.014854191114073214
test_mses at epoch24: [1.1557112  0.28946648 0.71223444 0.311945   0.51017125 0.30668477
 0.44761398 0.55978569 0.32922513 0.02769089]
test_maes at epoch24: [0.86599328 0.39199508 0.3813538  0.2638082  0.54415798 0.45641992
 0.33375392 0.50935459 0.30830931 0.09919637]

train_loss at epoch25: 0.004825353136529093
train_mses at epoch25: [0.58314396 0.04127591 0.15303067 0.15741155 0.09396597 0.07198432
 0.22825782 0.20386125 0.12032502 0.03041836]
train_maes at epoch25: [0.61136813 0.15981388 0.25004723 0.25729649 0.21238664 0.2113473
 0.27700303 0.27444979 0.22813404 0.12394973]
test_loss at epoch25: 0.015219041510768559
test_mses at epoch25: [1.19648545 0.28925706 0.73589886 0.31306993 0.46183065 0.28481903
 0.54253929 0.52114631 0.391734   0.02511613]
test_maes at epoch25: [0.86391925 0.3925886  0.40685802 0.25690045 0.42980561 0.43659907
 0.34463186 0.43962977 0.33178883 0.08530552]

train_loss at epoch26: 0.004266466621471488
train_mses at epoch26: [0.60510879 0.05378574 0.17150913 0.09200491 0.09258481 0.05076591
 0.16791394 0.2262037  0.07594329 0.02765943]
train_maes at epoch26: [0.58510043 0.16889443 0.25667683 0.21336131 0.18463454 0.16784869
 0.22310729 0.26395596 0.18783958 0.10696091]
test_loss at epoch26: 0.015410930890104046
test_mses at epoch26: [1.27475961 0.25415804 0.78548589 0.27238864 0.4758181  0.19770062
 0.65192329 0.55774668 0.37659752 0.02346982]
test_maes at epoch26: [0.84797728 0.331163   0.39482494 0.23666326 0.38493342 0.2987833
 0.39789427 0.45760771 0.31772582 0.07847558]

train_loss at epoch27: 0.0044662435741528225
train_mses at epoch27: [0.57267188 0.04584405 0.19091897 0.13440752 0.06258203 0.05539736
 0.18017122 0.2353401  0.08372238 0.03016733]
train_maes at epoch27: [0.55398419 0.1511045  0.23172185 0.22312993 0.16364682 0.176911
 0.25158847 0.26898587 0.1955334  0.1214101 ]
test_loss at epoch27: 0.01462637309146964
test_mses at epoch27: [1.17486972 0.25547752 0.74409528 0.26185603 0.453777   0.18910996
 0.58882841 0.55509838 0.34782839 0.02361515]
test_maes at epoch27: [0.83388892 0.33901913 0.38461597 0.23805961 0.43095605 0.30496545
 0.36427805 0.45633776 0.32082077 0.08333402]

train_loss at epoch28: 0.0036521025973817577
train_mses at epoch28: [0.57355654 0.03546633 0.16624066 0.16528252 0.05418927 0.0417422
 0.1133508  0.16172553 0.05117017 0.02789624]
train_maes at epoch28: [0.55518785 0.13972177 0.20469078 0.23906875 0.17030412 0.14990062
 0.21121118 0.24050978 0.15823194 0.11774822]
test_loss at epoch28: 0.013990997620250868
test_mses at epoch28: [1.03594866 0.26218818 0.66358147 0.26310932 0.4673389  0.23102737
 0.49953082 0.5914721  0.28755501 0.02691733]
test_maes at epoch28: [0.81056242 0.35297897 0.38015639 0.24763646 0.52917522 0.37476744
 0.33354958 0.5185959  0.30875336 0.10231227]

train_loss at epoch29: 0.003719236458773198
train_mses at epoch29: [0.49339306 0.03306432 0.15351784 0.12404183 0.0580453  0.04608583
 0.1416887  0.17494784 0.07683742 0.0260589 ]
train_maes at epoch29: [0.52644429 0.13516001 0.24178102 0.22066143 0.17783184 0.15894972
 0.23050316 0.25375813 0.20840771 0.10945676]
test_loss at epoch29: 0.014066948190979336
test_mses at epoch29: [1.01685128 0.26167699 0.66554715 0.2529687  0.42871518 0.25140207
 0.55352295 0.5739261  0.2946426  0.0272472 ]
test_maes at epoch29: [0.79713242 0.35024789 0.38049256 0.24185401 0.46934682 0.40125889
 0.34750168 0.50750402 0.30518314 0.10410018]

train_loss at epoch30: 0.0036907859308564143
train_mses at epoch30: [0.63774929 0.02700825 0.17928194 0.10286081 0.05141496 0.03648704
 0.20747007 0.12801087 0.05678326 0.0242382 ]
train_maes at epoch30: [0.58552044 0.12369624 0.23356219 0.20636251 0.14821359 0.14587887
 0.25024923 0.21025951 0.17156346 0.10233366]
test_loss at epoch30: 0.014313485959301825
test_mses at epoch30: [1.02352441 0.25880768 0.68709818 0.25232538 0.4233338  0.25414446
 0.62943632 0.53753005 0.29399342 0.0258643 ]
test_maes at epoch30: [0.7875025  0.34205853 0.38170039 0.23601796 0.42452051 0.40664981
 0.37322132 0.47807755 0.2911577  0.09796703]

train_loss at epoch31: 0.003693080466726552
train_mses at epoch31: [0.58691594 0.04724467 0.2085994  0.08549069 0.05845375 0.05387628
 0.11676286 0.15871197 0.06217142 0.0243855 ]
train_maes at epoch31: [0.58974385 0.14807291 0.24803842 0.1818357  0.15605473 0.16359427
 0.19062909 0.21320428 0.18662081 0.10207944]
test_loss at epoch31: 0.014181294195030046
test_mses at epoch31: [0.96045917 0.246512   0.65410323 0.30412101 0.44742349 0.29769532
 0.52670989 0.56850077 0.27689976 0.02407192]
test_maes at epoch31: [0.76313184 0.31680528 0.39967059 0.27300038 0.50745365 0.46123177
 0.32098353 0.50456158 0.28424975 0.09281478]

train_loss at epoch32: 0.003507276592047318
train_mses at epoch32: [0.40938437 0.03363825 0.17752156 0.09563611 0.05895462 0.03900241
 0.1701797  0.14951251 0.04928753 0.0281648 ]
train_maes at epoch32: [0.47772714 0.134189   0.23694198 0.19813498 0.16441799 0.14725484
 0.23024646 0.2097953  0.16497529 0.10731373]
test_loss at epoch32: 0.015176707959693411
test_mses at epoch32: [1.12182397 0.24226104 0.74840894 0.3212402  0.45584933 0.26384973
 0.62763505 0.59769013 0.29120084 0.02190929]
test_maes at epoch32: [0.74303611 0.28973403 0.38001198 0.27192281 0.49219493 0.42417399
 0.33960498 0.53066615 0.27865986 0.08356069]

train_loss at epoch33: 0.003117663704830667
train_mses at epoch33: [0.43982679 0.04186097 0.14179812 0.07176487 0.04479905 0.03193973
 0.15372654 0.13870934 0.04406661 0.02367342]
train_maes at epoch33: [0.4746426  0.14826458 0.21182319 0.17583899 0.15287402 0.12384196
 0.2076452  0.21510665 0.16034366 0.10502764]
test_loss at epoch33: 0.014756757280100946
test_mses at epoch33: [0.9958259  0.26384954 0.74345277 0.23841697 0.4161153  0.26370006
 0.68336594 0.55795867 0.28338788 0.02341815]
test_maes at epoch33: [0.7390145  0.3573998  0.39458037 0.22024313 0.42916632 0.428528
 0.37985727 0.49614468 0.27775609 0.08832365]

train_loss at epoch34: 0.002721249733282172
train_mses at epoch34: [0.43865216 0.04031456 0.11820126 0.08747613 0.04569664 0.02842886
 0.11095594 0.11439318 0.03631061 0.02887851]
train_maes at epoch34: [0.51357379 0.13976758 0.21885865 0.20123975 0.149579   0.13151281
 0.18821762 0.2151867  0.14095901 0.11905379]
test_loss at epoch34: 0.014175413743309353
test_mses at epoch34: [0.99820532 0.24388437 0.76273323 0.20474338 0.40853316 0.21707847
 0.67972624 0.50641204 0.2805435  0.02422855]
test_maes at epoch34: [0.74651194 0.32609032 0.39837862 0.21245061 0.42129189 0.37296767
 0.3705385  0.45212122 0.27679316 0.09184019]

train_loss at epoch35: 0.0029042164754608402
train_mses at epoch35: [0.42326923 0.05033526 0.13756671 0.08882752 0.03962857 0.02375852
 0.13649772 0.11312765 0.04369152 0.02507218]
train_maes at epoch35: [0.48514275 0.14270592 0.23006825 0.19054565 0.14394787 0.12402665
 0.19453591 0.23086872 0.15549333 0.10675261]
test_loss at epoch35: 0.013705412654773048
test_mses at epoch35: [1.00526307 0.22470317 0.76139099 0.21303865 0.42406634 0.1795212
 0.63050847 0.50141447 0.2595208  0.02290783]
test_maes at epoch35: [0.74100128 0.28329428 0.39335488 0.21702255 0.47108162 0.3201923
 0.34594425 0.44772128 0.26582056 0.08662053]

train_loss at epoch36: 0.0023056356155354042
train_mses at epoch36: [0.36672518 0.04173478 0.09525456 0.05415278 0.0432522  0.03534356
 0.10452399 0.07483737 0.04441456 0.02566552]
train_maes at epoch36: [0.44745843 0.15271423 0.20624263 0.17612606 0.16174042 0.14981661
 0.19703064 0.18742574 0.16223202 0.10744826]
test_loss at epoch36: 0.013840804929318636
test_mses at epoch36: [1.06173545 0.23613939 0.74764534 0.26950294 0.42923722 0.18276027
 0.58467689 0.50321125 0.26915233 0.02192625]
test_maes at epoch36: [0.73042035 0.30268679 0.39223939 0.27796315 0.44629655 0.32462354
 0.32915442 0.44914825 0.26650163 0.0816891 ]

train_loss at epoch37: 0.002581835683921109
train_mses at epoch37: [0.37047417 0.03869107 0.12584628 0.0515659  0.03350849 0.03106696
 0.08641921 0.14220356 0.0490873  0.02795832]
train_maes at epoch37: [0.44080412 0.14709613 0.20435805 0.17056862 0.14005611 0.13806252
 0.18214423 0.21308133 0.16277183 0.11188902]
test_loss at epoch37: 0.013194563596144966
test_mses at epoch37: [0.96400748 0.30290005 0.64363425 0.20207164 0.39602368 0.20380532
 0.55928071 0.51053544 0.25114164 0.02293743]
test_maes at epoch37: [0.78559972 0.40595269 0.44034224 0.2228317  0.39271642 0.35757506
 0.33955479 0.45958297 0.27234385 0.0839606 ]

train_loss at epoch38: 0.0026281303523675256
train_mses at epoch38: [0.47867039 0.05558059 0.14440259 0.08359898 0.03551532 0.05371911
 0.05759084 0.09629269 0.03717143 0.02565483]
train_maes at epoch38: [0.51642406 0.16500952 0.25591042 0.2012594  0.12983383 0.16832511
 0.15318177 0.20622608 0.15598917 0.109581  ]
test_loss at epoch38: 0.015191703062990437
test_mses at epoch38: [1.14709872 0.24242843 0.72518911 0.30126078 0.50141932 0.22868063
 0.63231759 0.50004118 0.39367738 0.02151314]
test_maes at epoch38: [0.78762521 0.31592384 0.38192648 0.25035299 0.46099985 0.39003807
 0.33627061 0.40107558 0.32090889 0.07792448]

train_loss at epoch39: 0.0030334510071122127
train_mses at epoch39: [0.40056022 0.05524403 0.1046002  0.09059608 0.05872688 0.05108366
 0.08273821 0.156785   0.07228291 0.02375216]
train_maes at epoch39: [0.48205483 0.13460914 0.19886145 0.176617   0.15706074 0.13859115
 0.17811326 0.21280513 0.172573   0.10247753]
test_loss at epoch39: 0.013755682045998781
test_mses at epoch39: [0.97869873 0.22226266 0.73214275 0.22418859 0.49228382 0.20770404
 0.57547995 0.54081015 0.21984153 0.02371987]
test_maes at epoch39: [0.76224936 0.29521174 0.37884935 0.20704347 0.55298397 0.35809094
 0.33581869 0.51020745 0.24244039 0.09362841]

train_loss at epoch40: 0.002142778719248979
train_mses at epoch40: [0.32981527 0.03554712 0.07491018 0.05952776 0.04179126 0.0386589
 0.06176329 0.097061   0.05584796 0.02161192]
train_maes at epoch40: [0.46836324 0.13038398 0.19477427 0.16242582 0.15725672 0.14777323
 0.16120137 0.20842767 0.17760576 0.09914805]
test_loss at epoch40: 0.012846234376015871
test_mses at epoch40: [0.96647407 0.26077695 0.72658924 0.1900881  0.43325721 0.21398096
 0.54250545 0.42587325 0.19538241 0.0218438 ]
test_maes at epoch40: [0.77203791 0.36610219 0.38437589 0.19641487 0.44998393 0.35938992
 0.32646698 0.39790046 0.2350473  0.08280344]

train_loss at epoch41: 0.002203184301438539
train_mses at epoch41: [0.33731949 0.0336616  0.05207394 0.0865726  0.03484749 0.04624508
 0.07171835 0.10227434 0.04040669 0.02592149]
train_maes at epoch41: [0.4545132  0.13811627 0.15338416 0.17278169 0.127723   0.1489525
 0.16909822 0.21774895 0.14759026 0.10395403]
test_loss at epoch41: 0.013567750220713408
test_mses at epoch41: [1.10234606 0.24157508 0.77363273 0.24623433 0.45528531 0.23717408
 0.50310467 0.42159314 0.26931536 0.02006139]
test_maes at epoch41: [0.75431749 0.32705973 0.37835742 0.21908732 0.41246153 0.39067266
 0.32439444 0.3617117  0.26065701 0.07472209]

train_loss at epoch42: 0.0022452137959392176
train_mses at epoch42: [0.33524809 0.03671959 0.14289555 0.05313211 0.03227606 0.04121242
 0.0605262  0.07570371 0.03565052 0.02284319]
train_maes at epoch42: [0.41764894 0.13599958 0.21016268 0.14329818 0.12443559 0.13391115
 0.16984182 0.15414103 0.14413981 0.10113688]
test_loss at epoch42: 0.012877667727677719
test_mses at epoch42: [0.99563206 0.22113544 0.74866155 0.22893941 0.416347   0.19815115
 0.50596241 0.45349063 0.23080012 0.0197729 ]
test_maes at epoch42: [0.72362235 0.29045978 0.37764302 0.22738812 0.42441634 0.32363785
 0.31203008 0.43963148 0.25538188 0.07487099]

train_loss at epoch43: 0.002159374887528627
train_mses at epoch43: [0.33197329 0.03107684 0.08500942 0.06411849 0.03959429 0.05345081
 0.07771585 0.06314451 0.04754538 0.02191786]
train_maes at epoch43: [0.42395588 0.11679466 0.17550991 0.16437132 0.13925235 0.16685052
 0.16344487 0.17132202 0.15370936 0.09992018]
test_loss at epoch43: 0.013366468898628069
test_mses at epoch43: [1.02021481 0.22143933 0.83320043 0.20121082 0.42601426 0.18695368
 0.58106315 0.41799459 0.24715211 0.01966154]
test_maes at epoch43: [0.73917098 0.29956461 0.38089983 0.20422899 0.44828441 0.29419383
 0.31694969 0.40305687 0.25768152 0.07334423]

train_loss at epoch44: 0.0017199039297259371
train_mses at epoch44: [0.23991214 0.02453953 0.06824094 0.05734867 0.03105204 0.02884042
 0.06112151 0.05770717 0.03571405 0.0225757 ]
train_maes at epoch44: [0.38489971 0.11923655 0.16602317 0.14738302 0.12110333 0.12456776
 0.16057003 0.15760702 0.14194077 0.09995022]
test_loss at epoch44: 0.012690581705259241
test_mses at epoch44: [0.93859347 0.25347041 0.7499436  0.17307434 0.39613323 0.21123495
 0.54718693 0.38880796 0.23732551 0.02109608]
test_maes at epoch44: [0.75965712 0.36291074 0.37404331 0.19823538 0.4485646  0.36925144
 0.32053689 0.37085816 0.25783643 0.0822475 ]

train_loss at epoch45: 0.0018867298798716586
train_mses at epoch45: [0.33393244 0.04590131 0.05971642 0.05361554 0.03793136 0.06446391
 0.04592993 0.05430962 0.03501932 0.02255609]
train_maes at epoch45: [0.43508465 0.13681465 0.17765083 0.16627811 0.13610599 0.15402272
 0.16110505 0.1772494  0.13962424 0.10221811]
test_loss at epoch45: 0.013585633557775745
test_mses at epoch45: [1.08748874 0.22757485 0.79578746 0.19946999 0.42272518 0.2361743
 0.6145466  0.41659684 0.2366691  0.02058753]
test_maes at epoch45: [0.72108631 0.30062059 0.37487891 0.2071136  0.40155513 0.40032047
 0.32683795 0.35883547 0.23726488 0.07749545]

train_loss at epoch46: 0.0019656374807591023
train_mses at epoch46: [0.31149865 0.05608711 0.08078723 0.0545153  0.051527   0.03641879
 0.03832165 0.07181361 0.03029888 0.01996518]
train_maes at epoch46: [0.43515506 0.14028643 0.16742862 0.1543828  0.13878181 0.14708968
 0.12668364 0.15883304 0.1179377  0.09521437]
test_loss at epoch46: 0.012843282326408054
test_mses at epoch46: [0.97849014 0.2162992  0.73667076 0.20743902 0.40373176 0.19683357
 0.63522003 0.42805669 0.16215659 0.02094902]
test_maes at epoch46: [0.70305249 0.28623513 0.37649795 0.22505038 0.43341553 0.32874825
 0.33480193 0.41338811 0.21803225 0.07999329]

train_loss at epoch47: 0.0019121738555638688
train_mses at epoch47: [0.26228913 0.03485555 0.09189418 0.04411225 0.02933618 0.0485335
 0.04156596 0.05965    0.05907331 0.02604073]
train_maes at epoch47: [0.39262385 0.1285004  0.18942759 0.15293824 0.13378529 0.15262342
 0.13728445 0.16304027 0.16938294 0.11366002]
test_loss at epoch47: 0.013082931547061256
test_mses at epoch47: [0.95631108 0.2296833  0.72928077 0.18157258 0.39581247 0.19371043
 0.63039029 0.49159982 0.19773646 0.02220187]
test_maes at epoch47: [0.72146341 0.31971076 0.35813661 0.20053416 0.44315978 0.31816185
 0.33027822 0.47230312 0.22482815 0.0887324 ]

train_loss at epoch48: 0.001533740524040616
train_mses at epoch48: [0.21654118 0.03101844 0.06142371 0.04337897 0.02912104 0.02775859
 0.0467799  0.06338558 0.02515968 0.02003594]
train_maes at epoch48: [0.34520529 0.11505997 0.15731074 0.14503777 0.12215929 0.11600679
 0.13325674 0.16348716 0.11330105 0.10153534]
test_loss at epoch48: 0.013729727138643679
test_mses at epoch48: [1.05385634 0.25558105 0.79825099 0.16020801 0.40999228 0.2256726
 0.62932574 0.4268126  0.28035792 0.02260588]
test_maes at epoch48: [0.74843497 0.36541099 0.36400929 0.18148378 0.40990461 0.37594207
 0.32593619 0.38263218 0.25624897 0.09066799]

train_loss at epoch49: 0.0013889326997425246
train_mses at epoch49: [0.30451487 0.03944441 0.04335753 0.04445639 0.02214318 0.03168282
 0.02879695 0.04015742 0.03252875 0.02067758]
train_maes at epoch49: [0.42213474 0.13857468 0.15274524 0.14703951 0.11098634 0.13528573
 0.11777628 0.13689226 0.13211912 0.08998657]
test_loss at epoch49: 0.012912963395533354
test_mses at epoch49: [1.01312135 0.26827225 0.75299284 0.14914493 0.39218109 0.2286466
 0.56358979 0.39594953 0.24560041 0.02217302]
test_maes at epoch49: [0.75897607 0.38912051 0.36428599 0.19042322 0.40439325 0.39412125
 0.3178808  0.36603117 0.24005774 0.09058558]

