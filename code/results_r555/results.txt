PyTorch Version:  2.0.0
Torchvision Version:  0.15.1
Initializing Datasets and Dataloaders...
Train Sets: ['a' 'eb']
Test Sets: ['aw' 'e']
Total Number of Train Sets: 92
Total Number of Test Sets: 92

train_loss at epoch0: 0.03944825478222059
train_mses at epoch0: [4.04129995 1.05072814 1.1659043  1.17349133 1.17730496 1.00843157
 1.1592938  1.18552638 1.12966473 0.02543581]
train_maes at epoch0: [1.52128828 0.35741466 0.51604923 0.38418126 0.36221264 0.47993676
 0.34206082 0.37428512 0.41959613 0.05415292]
test_loss at epoch0: 0.03812635981518289
test_mses at epoch0: [3.5463725  0.99127762 1.15907303 1.14202213 1.16877617 0.92510529
 1.10653911 1.17366364 1.08105309 0.02633854]
test_maes at epoch0: [1.48420202 0.38800553 0.62002346 0.42888007 0.37680704 0.54062091
 0.3441011  0.38579958 0.44779659 0.05664454]

train_loss at epoch1: 0.037497865117114525
train_mses at epoch1: [3.31347242 0.96122039 1.19872543 1.11457738 1.15983894 0.89592396
 1.07927429 1.16560329 1.07809091 0.02603678]
train_maes at epoch1: [1.510123   0.42740344 0.68642509 0.46856214 0.38704314 0.56510061
 0.35352281 0.39467966 0.47627836 0.06766652]
test_loss at epoch1: 0.035994917154312134
test_mses at epoch1: [2.90181921 0.83662203 1.17962791 1.08317014 1.14447619 0.82794439
 1.03525927 1.14139404 1.03190099 0.03319894]
test_maes at epoch1: [1.46985247 0.46492013 0.73198712 0.53287386 0.41869428 0.56165964
 0.36659812 0.41655985 0.51918378 0.07840587]

train_loss at epoch2: 0.03528083731298861
train_mses at epoch2: [2.92921615 0.81261938 1.22561597 1.05497027 1.12730391 0.82005223
 1.01487881 1.1189033  1.02284809 0.03793683]
train_maes at epoch2: [1.50438314 0.49216233 0.76529124 0.56790371 0.43305097 0.54913862
 0.39233113 0.42878913 0.54822707 0.10424474]
test_loss at epoch2: 0.03368766346703405
test_mses at epoch2: [2.70635246 0.69841319 1.14301259 1.02984456 1.11513602 0.70027624
 0.95039357 1.1050869  0.99592453 0.03839641]
test_maes at epoch2: [1.45289208 0.46188239 0.71044959 0.57420426 0.46568507 0.45569186
 0.41456335 0.45202239 0.57671087 0.0886351 ]

train_loss at epoch3: 0.03278792163600092
train_mses at epoch3: [2.77705517 0.64113189 1.17793135 1.01670466 1.11156363 0.6738226
 0.93388091 1.08123268 0.96756115 0.04009246]
train_maes at epoch3: [1.46450314 0.42202094 0.70260601 0.57266122 0.47815769 0.41253491
 0.44719323 0.46585679 0.58193619 0.1012558 ]
test_loss at epoch3: 0.03153386388135993
test_mses at epoch3: [2.57646296 0.57011806 1.09778881 0.96451118 1.09005734 0.59445524
 0.88514002 1.07287598 0.9636959  0.0396987 ]
test_maes at epoch3: [1.39707644 0.36444102 0.64135298 0.53374655 0.50656801 0.32815482
 0.47108699 0.50380627 0.598847   0.09123446]

train_loss at epoch4: 0.03075995069483052
train_mses at epoch4: [2.57199722 0.50775546 1.09705039 0.90512076 1.08619083 0.58152598
 0.86851464 1.07580724 0.96211414 0.0465366 ]
train_maes at epoch4: [1.40503571 0.34887939 0.66068025 0.53735953 0.51917061 0.33004899
 0.50606156 0.53235339 0.62285655 0.10851333]
test_loss at epoch4: 0.02903457892977673
test_mses at epoch4: [2.50516909 0.41878738 1.08385645 0.8661624  1.05973542 0.43935362
 0.81626431 1.0368226  0.91929162 0.04625894]
test_maes at epoch4: [1.38087194 0.29111707 0.65518442 0.53539959 0.53986365 0.27749187
 0.52601711 0.55706523 0.62135779 0.09937736]

train_loss at epoch5: 0.0284883386414984
train_mses at epoch5: [2.68399563 0.34613976 1.12765719 0.80202323 1.06799123 0.43989179
 0.82000533 1.02063233 0.93215633 0.04712651]
train_maes at epoch5: [1.46932663 0.27565232 0.69633632 0.55521853 0.54633895 0.31156485
 0.55442317 0.57177706 0.63497456 0.1133932 ]
test_loss at epoch5: 0.02638923150041829
test_mses at epoch5: [2.53233079 0.28281976 1.07516843 0.7316601  1.0203558  0.29672509
 0.73948272 0.99245468 0.85081499 0.05737007]
test_maes at epoch5: [1.38251368 0.2570724  0.66913733 0.50367327 0.54255615 0.27691066
 0.54104516 0.57258896 0.60494528 0.10656998]

train_loss at epoch6: 0.02580502434917118
train_mses at epoch6: [2.55436265 0.30154757 1.07155042 0.73412923 1.02088373 0.25264787
 0.7072958  0.95485488 0.85929813 0.06168818]
train_maes at epoch6: [1.36746323 0.29180617 0.65415988 0.52427332 0.52336873 0.2921706
 0.53382367 0.56228017 0.59202873 0.12703894]
test_loss at epoch6: 0.02378855127355327
test_mses at epoch6: [2.45422232 0.2123103  1.02388211 0.5715191  0.97919065 0.23970067
 0.6408923  0.94109634 0.75665039 0.06154918]
test_maes at epoch6: [1.30385701 0.26474087 0.58787437 0.43558041 0.50166504 0.29183867
 0.47084889 0.54551971 0.53374477 0.10722575]

train_loss at epoch7: 0.023293337096338688
train_mses at epoch7: [2.5871052  0.19689755 1.0317917  0.53787442 0.97095389 0.27918076
 0.59853637 0.93858147 0.74966216 0.05595693]
train_maes at epoch7: [1.33590705 0.27435281 0.58314692 0.42770317 0.49859796 0.32813862
 0.44939445 0.54839197 0.50218132 0.12281801]
test_loss at epoch7: 0.021230756588604138
test_mses at epoch7: [2.32555225 0.16913505 0.95714247 0.40875497 0.93063435 0.22613133
 0.54357064 0.8793662  0.66182979 0.04611202]
test_maes at epoch7: [1.19662138 0.27149122 0.51696787 0.36630484 0.46754144 0.32309145
 0.42741877 0.52174869 0.46135574 0.09351699]

train_loss at epoch8: 0.02082860081092171
train_mses at epoch8: [2.31934019 0.18756604 0.91366115 0.39472545 0.94236506 0.26617252
 0.48972625 0.8645625  0.6581512  0.04924641]
train_maes at epoch8: [1.19233887 0.30171172 0.52992065 0.38878672 0.48060989 0.37362861
 0.40979059 0.51288968 0.48241673 0.12237848]
test_loss at epoch8: 0.01865268887385078
test_mses at epoch8: [2.18166395 0.14764629 0.85968342 0.28872418 0.86756381 0.23749446
 0.42750349 0.79171433 0.56740989 0.02809436]
test_maes at epoch8: [1.12712185 0.28332199 0.47535368 0.35448598 0.46570443 0.3633537
 0.3929658  0.48861691 0.41757816 0.09638459]

train_loss at epoch9: 0.01938675959473071
train_mses at epoch9: [2.35626618 0.17783894 0.80480166 0.29834919 0.91483481 0.31216267
 0.41387881 0.79621154 0.61390728 0.04054206]
train_maes at epoch9: [1.14416094 0.31750337 0.50639712 0.36609658 0.50343617 0.39687879
 0.40608832 0.50435606 0.44709736 0.1336871 ]
test_loss at epoch9: 0.016535539018071217
test_mses at epoch9: [2.0669143  0.11463626 0.77671666 0.22480287 0.81731008 0.23552358
 0.34295854 0.72609174 0.46499172 0.02137561]
test_maes at epoch9: [1.02318456 0.2492425  0.4471194  0.2947072  0.49703569 0.39356647
 0.35010794 0.47893917 0.39877085 0.09281675]

train_loss at epoch10: 0.01669293382893438
train_mses at epoch10: [2.1464308  0.14724905 0.71744791 0.21499263 0.84343055 0.3115783
 0.33648391 0.73872367 0.4555728  0.03517965]
train_maes at epoch10: [1.06941058 0.2660824  0.45900102 0.3027874  0.52002943 0.42653619
 0.35443107 0.51626914 0.41078219 0.13670558]
test_loss at epoch10: 0.014338018453639486
test_mses at epoch10: [1.78663687 0.09379503 0.65315723 0.2089992  0.75603391 0.20277111
 0.27164883 0.64721233 0.37475542 0.01974636]
test_maes at epoch10: [0.93904998 0.20655501 0.41697551 0.30144914 0.49476622 0.32983217
 0.29434226 0.47538386 0.3656749  0.09227364]

train_loss at epoch11: 0.015056112214275028
train_mses at epoch11: [1.92167463 0.13859236 0.70181307 0.24857845 0.73537746 0.24485371
 0.26341651 0.68786232 0.37892278 0.04035486]
train_maes at epoch11: [1.04212888 0.24573996 0.48960955 0.34463367 0.47774161 0.34993774
 0.32179308 0.49441048 0.41311049 0.14847868]
test_loss at epoch11: 0.012427324186200682
test_mses at epoch11: [1.48470927 0.08295998 0.56352715 0.20084079 0.67988641 0.18620746
 0.23761381 0.54518523 0.2894919  0.01847574]
test_maes at epoch11: [0.90594904 0.18544864 0.43370005 0.29318849 0.4490338  0.28932556
 0.29075134 0.4051255  0.32843087 0.09115756]

train_loss at epoch12: 0.013162519944750744
train_mses at epoch12: [1.30734975 0.0926755  0.57025428 0.21806969 0.67563612 0.25109354
 0.24837958 0.5934757  0.35026056 0.03584505]
train_maes at epoch12: [0.86217748 0.20581155 0.46110536 0.32170698 0.45508521 0.36126819
 0.32675559 0.42978176 0.37114967 0.15510858]
test_loss at epoch12: 0.010727695796800696
test_mses at epoch12: [1.32628961 0.07239531 0.47093086 0.1894449  0.60758615 0.14815276
 0.2108079  0.47235275 0.23100043 0.01674927]
test_maes at epoch12: [0.85476595 0.16817721 0.36836549 0.28296133 0.41915099 0.24935496
 0.27756432 0.36535911 0.29513128 0.08975133]

train_loss at epoch13: 0.012117971220742102
train_mses at epoch13: [1.48236489 0.0886853  0.50964026 0.23941114 0.66378153 0.21008334
 0.23243866 0.49249102 0.31500772 0.02435479]
train_maes at epoch13: [0.94653331 0.20239078 0.39512885 0.29923057 0.43989319 0.30257216
 0.31003462 0.41180465 0.37816917 0.12368894]
test_loss at epoch13: 0.0093016491635986
test_mses at epoch13: [1.16191799 0.05680266 0.41166087 0.16567625 0.52364136 0.12307142
 0.19219582 0.40821764 0.20075211 0.01460845]
test_maes at epoch13: [0.79700825 0.15341512 0.33288026 0.261962   0.41709147 0.22851617
 0.26300607 0.3457741  0.28392731 0.08150997]

train_loss at epoch14: 0.010208009377769802
train_mses at epoch14: [1.27044147 0.13270292 0.41654164 0.15529741 0.50595485 0.15308088
 0.25705712 0.44397102 0.25364204 0.02989351]
train_maes at epoch14: [0.88112403 0.24304131 0.35137875 0.27857945 0.43141862 0.2708038
 0.3302106  0.40298003 0.35025613 0.12420614]
test_loss at epoch14: 0.007397493428510168
test_mses at epoch14: [0.90263437 0.04330217 0.30105761 0.14685158 0.41942519 0.07078391
 0.18085808 0.326079   0.16831155 0.01381634]
test_maes at epoch14: [0.75049712 0.1468789  0.29414495 0.2432371  0.3844978  0.19466927
 0.2554495  0.33095443 0.26010518 0.07446149]

train_loss at epoch15: 0.00928172080413155
train_mses at epoch15: [1.19540544 0.08383879 0.35063832 0.19261116 0.46472953 0.12623831
 0.26309197 0.40326723 0.19313982 0.03323969]
train_maes at epoch15: [0.8697975  0.21765048 0.36870058 0.30021517 0.40657594 0.25418149
 0.33183524 0.3979532  0.30738009 0.13246174]
test_loss at epoch15: 0.006524175405502319
test_mses at epoch15: [0.83892351 0.02845802 0.26739117 0.13791552 0.36033574 0.05088899
 0.15335701 0.29787003 0.16112934 0.01373473]
test_maes at epoch15: [0.66477899 0.11615943 0.26218251 0.21159461 0.32903003 0.15296717
 0.21739778 0.31981697 0.28224846 0.06675561]

train_loss at epoch16: 0.008345172942980476
train_mses at epoch16: [1.17217034 0.0461826  0.34736822 0.19778988 0.40697523 0.08019212
 0.24877664 0.39608341 0.13552736 0.02813614]
train_maes at epoch16: [0.81810856 0.15082485 0.34532089 0.25741386 0.35438674 0.21726928
 0.284085   0.37738649 0.27491511 0.11777734]
test_loss at epoch16: 0.005881806268640186
test_mses at epoch16: [0.68534427 0.02354878 0.22909621 0.12519646 0.3132088  0.04953302
 0.18417084 0.26859337 0.12721834 0.01268366]
test_maes at epoch16: [0.61364748 0.10235266 0.24928602 0.19914389 0.29184498 0.1511373
 0.21699089 0.29181743 0.22640095 0.05972438]

train_loss at epoch17: 0.006618612162444902
train_mses at epoch17: [0.78202414 0.04463336 0.23750166 0.20255488 0.27285812 0.08989283
 0.16292086 0.30770208 0.17755598 0.0287653 ]
train_maes at epoch17: [0.67857794 0.15408585 0.27932242 0.2802374  0.31073866 0.22820179
 0.25814634 0.35315105 0.27241708 0.13097199]
test_loss at epoch17: 0.004878968000411987
test_mses at epoch17: [0.56997214 0.01882272 0.20062184 0.10292551 0.22189935 0.03534984
 0.16470877 0.23458751 0.1190668  0.01135833]
test_maes at epoch17: [0.57871598 0.08602487 0.24400184 0.18775041 0.27765629 0.14403034
 0.2016287  0.28105822 0.23009047 0.0586331 ]

train_loss at epoch18: 0.005980046061070069
train_mses at epoch18: [0.77085444 0.05814889 0.21251557 0.1411342  0.27433417 0.07871255
 0.13525146 0.28775561 0.14699905 0.02706705]
train_maes at epoch18: [0.7053641  0.17094455 0.28345634 0.24492568 0.32938655 0.21016896
 0.24048035 0.36668495 0.29409038 0.13467236]
test_loss at epoch18: 0.00460185952808546
test_mses at epoch18: [0.54401507 0.01688144 0.18563932 0.10303819 0.18481896 0.02999749
 0.16944396 0.22530235 0.12173437 0.01089601]
test_maes at epoch18: [0.52710264 0.09063119 0.22342247 0.18694953 0.26410927 0.12734711
 0.19320923 0.2725865  0.22607043 0.06059887]

train_loss at epoch19: 0.006156823071448699
train_mses at epoch19: [0.83683859 0.07105334 0.24225963 0.13195261 0.23190193 0.07284546
 0.16822699 0.30171315 0.15749466 0.02943064]
train_maes at epoch19: [0.73214555 0.16705724 0.31387638 0.23616035 0.29572051 0.20249827
 0.23524574 0.32937152 0.26595587 0.12546911]
test_loss at epoch19: 0.003844717922418014
test_mses at epoch19: [0.50484137 0.01221056 0.17735988 0.0745049  0.12603657 0.02427098
 0.13969001 0.21357661 0.09456627 0.00954893]
test_maes at epoch19: [0.52179176 0.08069373 0.2184518  0.16404778 0.2083967  0.11507279
 0.18362958 0.25310786 0.18731561 0.06007738]

train_loss at epoch20: 0.004745221089409745
train_mses at epoch20: [0.69235568 0.04718992 0.24260474 0.08074986 0.12759518 0.07812447
 0.11791257 0.22535198 0.13036567 0.02721729]
train_maes at epoch20: [0.6173743  0.15273833 0.29111333 0.20253621 0.25345751 0.21816409
 0.21645364 0.311342   0.24540766 0.12821233]
test_loss at epoch20: 0.0036728238767903786
test_mses at epoch20: [0.58774295 0.02567745 0.17510647 0.07117077 0.09836118 0.02633185
 0.13271614 0.20278057 0.08222149 0.00967876]
test_maes at epoch20: [0.59122342 0.09476766 0.21499425 0.15925583 0.17401331 0.11307983
 0.17490279 0.23140422 0.16846038 0.06056326]

train_loss at epoch21: 0.004567860585191976
train_mses at epoch21: [0.79209754 0.07326943 0.24266292 0.11923226 0.09905504 0.04691235
 0.09771704 0.228257   0.09234222 0.02559192]
train_maes at epoch21: [0.71231279 0.17378032 0.28422422 0.23645033 0.21818585 0.16449628
 0.20744838 0.28965541 0.21553864 0.1283138 ]
test_loss at epoch21: 0.003179813937648483
test_mses at epoch21: [0.46711137 0.02112935 0.15764813 0.05794007 0.07316934 0.01558717
 0.11556813 0.19306592 0.0727754  0.01051875]
test_maes at epoch21: [0.50295135 0.10173229 0.19656083 0.14932942 0.1607055  0.09544155
 0.18313642 0.22424563 0.16039545 0.06769913]

train_loss at epoch22: 0.004598083784398826
train_mses at epoch22: [0.71171887 0.06658215 0.15963641 0.10492045 0.11164865 0.07358023
 0.11688855 0.27828868 0.10210227 0.02864018]
train_maes at epoch22: [0.63676342 0.18412226 0.26375333 0.21305225 0.23649713 0.20143742
 0.23193613 0.31053524 0.23599499 0.13518325]
test_loss at epoch22: 0.002823306085622829
test_mses at epoch22: [0.42590261 0.01698436 0.14923142 0.04775873 0.05369398 0.01937451
 0.09742874 0.18617007 0.05543527 0.01046934]
test_maes at epoch22: [0.4820124  0.10663361 0.18917337 0.13192121 0.14394177 0.10532409
 0.17863145 0.21663708 0.13733809 0.06572368]

train_loss at epoch23: 0.004057835301627283
train_mses at epoch23: [0.5571563  0.03713469 0.22232403 0.06350072 0.06941212 0.06585521
 0.11863577 0.22030504 0.0905841  0.02652056]
train_maes at epoch23: [0.55302266 0.14719623 0.28012681 0.17326402 0.19356293 0.20102449
 0.20849921 0.28243454 0.21736113 0.11771839]
test_loss at epoch23: 0.0031661881214898567
test_mses at epoch23: [0.45110341 0.02001672 0.17972541 0.06710569 0.04752625 0.04676867
 0.08478812 0.20014101 0.05932651 0.01159982]
test_maes at epoch23: [0.45207568 0.08272537 0.19136284 0.146453   0.12500921 0.13960402
 0.13894396 0.22326549 0.14494988 0.06653337]

train_loss at epoch24: 0.004274281470671944
train_mses at epoch24: [0.64301336 0.04009365 0.2135227  0.07950372 0.06858264 0.07078315
 0.11613541 0.23514634 0.10493456 0.02533169]
train_maes at epoch24: [0.57866935 0.13824344 0.26509832 0.20039273 0.18027677 0.18674872
 0.2307823  0.26346201 0.22209304 0.12523478]
test_loss at epoch24: 0.00253508604415085
test_mses at epoch24: [0.42232028 0.01575695 0.13677886 0.06187392 0.03344831 0.01612007
 0.07720715 0.18029252 0.03786449 0.01108222]
test_maes at epoch24: [0.50553742 0.08364227 0.18651223 0.14832931 0.11716523 0.08601804
 0.13886723 0.21883475 0.11247348 0.07091871]

train_loss at epoch25: 0.004255255279333695
train_mses at epoch25: [0.58716344 0.03888458 0.17045945 0.14657746 0.07351409 0.06165123
 0.13230728 0.24223142 0.06395761 0.03795205]
train_maes at epoch25: [0.59128467 0.14887969 0.24879282 0.22180508 0.1904461  0.1923842
 0.22058205 0.28326625 0.18084926 0.13837537]
test_loss at epoch25: 0.0024078362823828406
test_mses at epoch25: [0.37015842 0.01556477 0.13864559 0.07303499 0.03061077 0.01250364
 0.05453259 0.1794724  0.03100279 0.00976876]
test_maes at epoch25: [0.41759534 0.08119506 0.1649574  0.13018326 0.10882418 0.07291889
 0.11467073 0.20388296 0.10338283 0.05579789]

train_loss at epoch26: 0.0037775501284910283
train_mses at epoch26: [0.61017795 0.03340814 0.21021687 0.13875466 0.06823111 0.04268405
 0.09472306 0.17999348 0.05441483 0.01967091]
train_maes at epoch26: [0.57188296 0.14038722 0.24774152 0.20343361 0.18765939 0.15251034
 0.18368366 0.23644228 0.15090345 0.10646925]
test_loss at epoch26: 0.0022145740525877995
test_mses at epoch26: [0.33012307 0.01190491 0.12415664 0.059351   0.0244656  0.01948404
 0.05760071 0.16706133 0.02843714 0.00937866]
test_maes at epoch26: [0.39003408 0.06892897 0.15505584 0.11306306 0.09838346 0.08178135
 0.14384781 0.21607469 0.10999753 0.05376856]

train_loss at epoch27: 0.0032450510593860045
train_mses at epoch27: [0.47574074 0.02940151 0.12806593 0.09052128 0.05139957 0.0501901
 0.06499568 0.21535404 0.068707   0.02813216]
train_maes at epoch27: [0.53148978 0.12867302 0.22748009 0.21200848 0.16364063 0.15984004
 0.17649994 0.28099545 0.18979433 0.12242516]
test_loss at epoch27: 0.0019248608580749967
test_mses at epoch27: [0.29136112 0.01193144 0.10984701 0.04191471 0.01748544 0.00899202
 0.03834692 0.14837731 0.04989799 0.00930278]
test_maes at epoch27: [0.37907841 0.06960912 0.16406183 0.13390032 0.09228807 0.06664615
 0.10719455 0.18371387 0.14396178 0.05917839]

train_loss at epoch28: 0.0033592072356006374
train_mses at epoch28: [0.44820281 0.05109875 0.12398375 0.08022451 0.06028861 0.04998255
 0.08976924 0.18201161 0.09603897 0.03279158]
train_maes at epoch28: [0.52627543 0.16036882 0.22208638 0.19444657 0.17330379 0.1709054
 0.18148674 0.25275518 0.22292756 0.13836943]
test_loss at epoch28: 0.0018743293197906535
test_mses at epoch28: [0.32117207 0.04019999 0.10138565 0.02937762 0.01587    0.01252455
 0.04267388 0.15240648 0.01863884 0.0085257 ]
test_maes at epoch28: [0.38254984 0.1006458  0.15092589 0.09800803 0.08317786 0.07480937
 0.093347   0.18568649 0.09713264 0.04976484]

train_loss at epoch29: 0.0032153692420410075
train_mses at epoch29: [0.42357945 0.06289218 0.14888257 0.05625401 0.04801716 0.05325334
 0.10278718 0.17037679 0.06749402 0.02195011]
train_maes at epoch29: [0.5182073  0.15971622 0.23709602 0.17235928 0.15639417 0.16541369
 0.18634496 0.22882801 0.18285792 0.1102253 ]
test_loss at epoch29: 0.0016311316791435947
test_mses at epoch29: [0.27984748 0.0334578  0.09086053 0.02584689 0.01180237 0.00897692
 0.03230773 0.13992411 0.01511915 0.00887326]
test_maes at epoch29: [0.36181133 0.08819972 0.14495543 0.11170572 0.07487457 0.06741257
 0.08721285 0.18434458 0.0901417  0.06012969]

train_loss at epoch30: 0.002899684983751048
train_mses at epoch30: [0.44432754 0.05053085 0.13918967 0.06509092 0.0378245  0.05193703
 0.05188005 0.18013746 0.05896413 0.02217582]
train_maes at epoch30: [0.47681331 0.13888835 0.22965422 0.17549969 0.14798853 0.16443902
 0.15883474 0.23508746 0.17497009 0.10768269]
test_loss at epoch30: 0.0013404459409091783
test_mses at epoch30: [0.21669658 0.01103667 0.0795809  0.01989369 0.0105297  0.00821322
 0.0272399  0.12193321 0.01300079 0.00969419]
test_maes at epoch30: [0.32232947 0.07206979 0.13967959 0.10549167 0.07394031 0.06545399
 0.10702443 0.17547226 0.08288435 0.06949814]

train_loss at epoch31: 0.002702119314800138
train_mses at epoch31: [0.38326644 0.0511003  0.11061984 0.06889206 0.05100222 0.04058104
 0.06585808 0.15649646 0.03943797 0.02705879]
train_maes at epoch31: [0.45924396 0.15066884 0.20598595 0.17535663 0.15545999 0.14825346
 0.17748994 0.23215255 0.15147855 0.12623749]
test_loss at epoch31: 0.0015990083389308143
test_mses at epoch31: [0.21834757 0.01559997 0.08967387 0.01972138 0.01782192 0.04584409
 0.0271001  0.12667819 0.01317151 0.00940223]
test_maes at epoch31: [0.31075243 0.084209   0.13794053 0.09730874 0.08377492 0.10823947
 0.08109472 0.17351609 0.07350877 0.0609676 ]

train_loss at epoch32: 0.0024345072148286777
train_mses at epoch32: [0.41293137 0.03724082 0.12553358 0.03666988 0.04546343 0.06107793
 0.03298407 0.14994624 0.03757631 0.02198358]
train_maes at epoch32: [0.49906658 0.13428345 0.2172054  0.14082332 0.16303132 0.16825033
 0.13245274 0.23427261 0.15205398 0.10657637]
test_loss at epoch32: 0.0012973247136434784
test_mses at epoch32: [0.18466253 0.00819101 0.07219614 0.02015376 0.01317016 0.02353085
 0.02240204 0.11291879 0.01388032 0.0092754 ]
test_maes at epoch32: [0.2840264  0.05796266 0.12858325 0.09324005 0.07599888 0.09598812
 0.0755754  0.15947629 0.0717731  0.05659323]

train_loss at epoch33: 0.0022910036065656204
train_mses at epoch33: [0.37070259 0.03122559 0.08195612 0.07214562 0.03804381 0.04167722
 0.04632167 0.1345456  0.04817939 0.02061707]
train_maes at epoch33: [0.48884086 0.13440194 0.19156462 0.19080813 0.13964638 0.15545225
 0.15652989 0.22563063 0.15076956 0.09944114]
test_loss at epoch33: 0.001164550305870564
test_mses at epoch33: [0.16076548 0.00400309 0.07975212 0.02090913 0.01204785 0.01619997
 0.02019896 0.09521292 0.00913475 0.00852909]
test_maes at epoch33: [0.26516088 0.04422576 0.12804376 0.09915207 0.08212329 0.1059675
 0.07411558 0.14558212 0.06475335 0.05130257]

train_loss at epoch34: 0.0024369265884160995
train_mses at epoch34: [0.39514088 0.02862906 0.14048812 0.06453648 0.04695233 0.04147454
 0.04713771 0.10585876 0.04506997 0.02334165]
train_maes at epoch34: [0.48687584 0.12002862 0.22505231 0.17399667 0.16805017 0.16315189
 0.14195813 0.20364953 0.16061995 0.11128787]
test_loss at epoch34: 0.0012816107305495636
test_mses at epoch34: [0.19365435 0.00668102 0.07372131 0.05128679 0.01748866 0.01365256
 0.01924037 0.09016405 0.0104664  0.00830823]
test_maes at epoch34: [0.30592768 0.06349427 0.12561212 0.12966726 0.09122928 0.07707124
 0.07429037 0.14047476 0.06956099 0.04967924]

train_loss at epoch35: 0.002527830150464307
train_mses at epoch35: [0.40315667 0.05263983 0.09929291 0.06616863 0.04794094 0.05264489
 0.04925964 0.1197734  0.04991257 0.0249513 ]
train_maes at epoch35: [0.49392061 0.14051223 0.20794314 0.17694737 0.15793305 0.16367328
 0.15087372 0.21492721 0.15664779 0.12004753]
test_loss at epoch35: 0.0012799905937003052
test_mses at epoch35: [0.18517608 0.01378589 0.07397857 0.06925618 0.00998513 0.01428656
 0.02161114 0.07247659 0.00885823 0.0073461 ]
test_maes at epoch35: [0.31605122 0.08652229 0.16482729 0.15956297 0.0650646  0.0888656
 0.08298938 0.12912717 0.0674231  0.05321032]

train_loss at epoch36: 0.002359261290858621
train_mses at epoch36: [0.31102304 0.03211464 0.12096327 0.09278695 0.03407522 0.05799573
 0.02788464 0.11465059 0.033913   0.02473433]
train_maes at epoch36: [0.42314655 0.12800636 0.21960516 0.19221322 0.13821117 0.17150241
 0.12876738 0.20448746 0.13904991 0.11861501]
test_loss at epoch36: 0.001091564722035242
test_mses at epoch36: [0.22100124 0.01478725 0.0667119  0.02605838 0.01402201 0.02093192
 0.01046672 0.0710474  0.01000853 0.00732705]
test_maes at epoch36: [0.33287671 0.07915631 0.12495991 0.0873356  0.07074548 0.0888925
 0.06590901 0.15864    0.07455366 0.04278877]

train_loss at epoch37: 0.002082398080307504
train_mses at epoch37: [0.35112466 0.04463291 0.0905546  0.05472588 0.04548087 0.04780874
 0.04140921 0.08325877 0.04578982 0.01572463]
train_maes at epoch37: [0.4367085  0.14296751 0.18320886 0.14415143 0.14288151 0.15025705
 0.14522787 0.18849713 0.15167803 0.0909738 ]
test_loss at epoch37: 0.0009320919078005397
test_mses at epoch37: [0.20988785 0.00873671 0.06713725 0.03412719 0.01079769 0.00635673
 0.00887981 0.054029   0.00759839 0.00781444]
test_maes at epoch37: [0.32523794 0.05901941 0.13503395 0.09995542 0.06230747 0.05782939
 0.05365663 0.11318598 0.06538581 0.04451148]

train_loss at epoch38: 0.002368695345585761
train_mses at epoch38: [0.39034662 0.03640256 0.09712332 0.05267134 0.03549115 0.05551006
 0.05051105 0.10968638 0.0680036  0.01936687]
train_maes at epoch38: [0.46839868 0.13479874 0.20832257 0.15057179 0.1354869  0.16203208
 0.1377444  0.20360274 0.1762155  0.10291323]
test_loss at epoch38: 0.0010277928541535916
test_mses at epoch38: [0.17569595 0.0253081  0.0475924  0.0566601  0.00658904 0.00681718
 0.01098779 0.05984635 0.01116281 0.00809352]
test_maes at epoch38: [0.31477363 0.1129005  0.11869917 0.12947751 0.06094041 0.06276913
 0.07170711 0.16094297 0.07003321 0.0510873 ]

train_loss at epoch39: 0.002408380377227845
train_mses at epoch39: [0.42640046 0.06268795 0.1052582  0.0724797  0.03707917 0.04788466
 0.05943716 0.0951499  0.03959036 0.02000803]
train_maes at epoch39: [0.48764779 0.1773408  0.20953863 0.17889934 0.14769259 0.16027699
 0.16348898 0.21761685 0.15021129 0.10589768]
test_loss at epoch39: 0.0013721538100229657
test_mses at epoch39: [0.23888929 0.02206043 0.08516183 0.05660341 0.01256255 0.01962907
 0.03239854 0.0525625  0.0200295  0.00932811]
test_maes at epoch39: [0.32778736 0.07506351 0.12470254 0.11961522 0.06453026 0.07951588
 0.08301639 0.10744587 0.09579127 0.04868021]

train_loss at epoch40: 0.002401633026159328
train_mses at epoch40: [0.38698895 0.06002888 0.06682202 0.10891706 0.03210134 0.03625879
 0.07077397 0.08720898 0.05095619 0.02639319]
train_maes at epoch40: [0.44165573 0.13492455 0.1620956  0.20586044 0.12371252 0.1367129
 0.16169976 0.17656995 0.15909544 0.1068162 ]
test_loss at epoch40: 0.0008996939084128193
test_mses at epoch40: [0.18863134 0.01816861 0.05292284 0.03645405 0.00568653 0.0106305
 0.01113866 0.04292014 0.01429039 0.00810504]
test_maes at epoch40: [0.29228036 0.07389304 0.1022432  0.10542562 0.05759616 0.06207301
 0.07992942 0.13443314 0.09248046 0.04906356]

train_loss at epoch41: 0.0018211966785399811
train_mses at epoch41: [0.22797439 0.02968513 0.07192077 0.03460839 0.0274928  0.05272812
 0.0554969  0.06276988 0.05768254 0.01971224]
train_maes at epoch41: [0.36288313 0.11963676 0.17065189 0.14197274 0.13097708 0.15769863
 0.16664882 0.17057676 0.17137448 0.10418333]
test_loss at epoch41: 0.0005721742003832175
test_mses at epoch41: [0.1070505  0.00518606 0.03462332 0.0151749  0.00654023 0.00824281
 0.0079248  0.03506199 0.00760035 0.00753555]
test_maes at epoch41: [0.24464157 0.04707085 0.08851369 0.07591973 0.05829473 0.05786549
 0.06101318 0.09569727 0.065037   0.04929606]

train_loss at epoch42: 0.001835635174875674
train_mses at epoch42: [0.28904546 0.03712763 0.06366511 0.05777143 0.03536066 0.03977628
 0.04973306 0.08505161 0.02826047 0.01680159]
train_maes at epoch42: [0.4526314  0.12451365 0.17174887 0.16100187 0.13206543 0.13763587
 0.14521937 0.18143133 0.12097566 0.09515223]
test_loss at epoch42: 0.000939423218369484
test_mses at epoch42: [0.1321662  0.00541128 0.07005711 0.01357993 0.01184204 0.01948407
 0.01965193 0.05929896 0.00638161 0.00799959]
test_maes at epoch42: [0.24135199 0.04569214 0.12211615 0.06839794 0.06606425 0.07680189
 0.0655363  0.11506978 0.0553651  0.04623922]

train_loss at epoch43: 0.001889948733150959
train_mses at epoch43: [0.23195948 0.037005   0.09291734 0.03035964 0.05136849 0.02633354
 0.04259302 0.09556575 0.03507479 0.0167066 ]
train_maes at epoch43: [0.38713196 0.13257344 0.17300995 0.12026163 0.1477288  0.11976897
 0.13385765 0.19237732 0.12859642 0.09185135]
test_loss at epoch43: 0.0008126631601835074
test_mses at epoch43: [0.1622804  0.00557018 0.09010993 0.02090671 0.00657085 0.00601038
 0.00765818 0.03249271 0.00605228 0.00712148]
test_maes at epoch43: [0.27156841 0.04856645 0.14351793 0.08190322 0.06090608 0.05759298
 0.04653355 0.10078334 0.05577253 0.04137707]

train_loss at epoch44: 0.001849447134072366
train_mses at epoch44: [0.25924198 0.02721112 0.07397567 0.05755904 0.05014473 0.04395474
 0.02894625 0.0636061  0.04923936 0.01874762]
train_maes at epoch44: [0.38447061 0.1202373  0.16914828 0.1633872  0.15605857 0.14358162
 0.12075424 0.16632352 0.14250411 0.09806996]
test_loss at epoch44: 0.0004902153032953324
test_mses at epoch44: [0.09773167 0.0157615  0.02628605 0.0100534  0.01169837 0.00574863
 0.00536175 0.02008105 0.00818901 0.00585514]
test_maes at epoch44: [0.23066583 0.06723756 0.09815102 0.07292663 0.06869488 0.05306163
 0.04717045 0.08527707 0.06509616 0.04014848]

train_loss at epoch45: 0.0013519008036540902
train_mses at epoch45: [0.22379621 0.02920811 0.0592662  0.02858473 0.03205771 0.03530832
 0.03629797 0.03488711 0.03055295 0.01522236]
train_maes at epoch45: [0.37230013 0.111844   0.16562194 0.12589162 0.12701066 0.13873015
 0.12668009 0.13168765 0.1255377  0.09106114]
test_loss at epoch45: 0.0006067507859805356
test_mses at epoch45: [0.0982151  0.00410632 0.02763053 0.00858246 0.01300283 0.02286683
 0.01461867 0.02383057 0.01411635 0.00593612]
test_maes at epoch45: [0.21793775 0.04020067 0.10227103 0.0634327  0.07699427 0.08321963
 0.06243807 0.0966971  0.08315947 0.03605893]

train_loss at epoch46: 0.0016237737044044163
train_mses at epoch46: [0.21064643 0.01284801 0.05485826 0.03649479 0.0420528  0.06904561
 0.02102944 0.06776495 0.04456637 0.02122947]
train_maes at epoch46: [0.35585598 0.09120233 0.16216944 0.12960378 0.13742846 0.15920711
 0.10362666 0.17611569 0.13023444 0.10356452]
test_loss at epoch46: 0.0004729834061277949
test_mses at epoch46: [0.10705321 0.00689909 0.02405477 0.00913893 0.00880608 0.01139616
 0.01504863 0.01697531 0.00530285 0.00536839]
test_maes at epoch46: [0.25460761 0.06217108 0.08337531 0.06660936 0.0662534  0.06994221
 0.06186473 0.07730286 0.05184547 0.0360054 ]

train_loss at epoch47: 0.0013818011983581212
train_mses at epoch47: [0.18484872 0.03173952 0.03040236 0.03098602 0.02335136 0.05453739
 0.02151888 0.06570095 0.0334732  0.01835757]
train_maes at epoch47: [0.35213374 0.12341692 0.1299132  0.11571022 0.109977   0.15334281
 0.11190401 0.18195865 0.13618668 0.09519756]
test_loss at epoch47: 0.000524961288370516
test_mses at epoch47: [0.08854768 0.00679002 0.03314355 0.00837771 0.00532205 0.00755648
 0.01452383 0.02364082 0.01293068 0.00499802]
test_maes at epoch47: [0.23544133 0.05559565 0.10299533 0.06764183 0.05032602 0.06291274
 0.06095941 0.08302855 0.07498503 0.03568566]

train_loss at epoch48: 0.0016645549027168233
train_mses at epoch48: [0.21658656 0.01955867 0.08272514 0.03895458 0.03742931 0.04702185
 0.03451796 0.05232732 0.0503101  0.01594966]
train_maes at epoch48: [0.36934564 0.10661006 0.18732793 0.13559173 0.13398809 0.1555309
 0.1243109  0.15359554 0.13914763 0.0908823 ]
test_loss at epoch48: 0.0007129249810848547
test_mses at epoch48: [0.07492081 0.00437744 0.02867269 0.00816007 0.01191073 0.03272423
 0.01683052 0.04113249 0.01451958 0.00580555]
test_maes at epoch48: [0.18359501 0.04266455 0.08493757 0.06387418 0.0636107  0.10327101
 0.06166125 0.12671301 0.08017672 0.03855708]

train_loss at epoch49: 0.001592290952153828
train_mses at epoch49: [0.21006947 0.01368585 0.0352426  0.03823695 0.03925317 0.06461335
 0.04201147 0.0667115  0.04508114 0.01417193]
train_maes at epoch49: [0.36443791 0.08453966 0.12370321 0.13870507 0.12755327 0.14417614
 0.1279104  0.16690014 0.13844868 0.08209009]
test_loss at epoch49: 0.00042383809088040954
test_mses at epoch49: [0.08757954 0.00414811 0.01949014 0.00761039 0.00821543 0.00504508
 0.0089774  0.02303796 0.0117405  0.00553031]
test_maes at epoch49: [0.20590088 0.04456359 0.07474406 0.06222646 0.05597495 0.04986246
 0.05680439 0.09023939 0.0801794  0.04236157]

