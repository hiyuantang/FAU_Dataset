PyTorch Version:  2.0.0
Torchvision Version:  0.15.1
Initializing Datasets and Dataloaders...
Train Sets: ['aw' 'e']
Test Sets: ['a' 'eb']
Total Number of Train Sets: 92
Total Number of Test Sets: 92

train_loss at epoch0: 0.0392163195039915
train_mses at epoch0: [4.14014519 1.07216085 1.16576558 1.17794534 1.17647063 1.01868759
 1.14364356 1.17939926 1.12021482 0.02477309]
train_maes at epoch0: [1.52361666 0.3653277  0.52221551 0.39421355 0.36427421 0.48922072
 0.3455399  0.37753492 0.4253539  0.05182871]
test_loss at epoch0: 0.03805929940679799
test_mses at epoch0: [3.48350994 0.98263416 1.1745192  1.13959633 1.16727549 0.91640534
 1.11649551 1.15904525 1.08198544 0.02608129]
test_maes at epoch0: [1.48163972 0.39321419 0.64131097 0.44207036 0.37834957 0.55781641
 0.3415825  0.40293418 0.44715696 0.05694566]

train_loss at epoch1: 0.03708950268185657
train_mses at epoch1: [3.11458654 0.92663312 1.22039205 1.10517851 1.16361949 0.92691503
 1.10438512 1.13256206 1.04936419 0.02912003]
train_maes at epoch1: [1.4381277  0.41945388 0.70101991 0.47242721 0.393328   0.58783502
 0.35639199 0.40987151 0.46256868 0.07296526]
test_loss at epoch1: 0.03598298067631929
test_mses at epoch1: [2.92554376 0.84305486 1.17895589 1.09038093 1.13848542 0.82523416
 1.04355541 1.12768355 1.03380971 0.03039142]
test_maes at epoch1: [1.4633073  0.44711396 0.72794474 0.52507896 0.42514196 0.56512981
 0.36476423 0.43517159 0.51138162 0.07153286]

train_loss at epoch2: 0.03541323607382567
train_mses at epoch2: [2.7233932  0.77361137 1.19660282 1.05270179 1.14042821 0.80244976
 1.03091418 1.11367245 1.02677518 0.03896721]
train_maes at epoch2: [1.43931659 0.44943295 0.73549139 0.55650922 0.44155089 0.52694731
 0.38639502 0.45320936 0.53499211 0.09557347]
test_loss at epoch2: 0.0337537591872008
test_mses at epoch2: [2.69871374 0.70729915 1.13068811 1.02640461 1.10992905 0.71615086
 0.97209986 1.1002406  0.99927858 0.03327216]
test_maes at epoch2: [1.44049856 0.44232106 0.66810658 0.57583542 0.473356   0.4205338
 0.39893901 0.45949319 0.57778147 0.07577115]

train_loss at epoch3: 0.03302332499752874
train_mses at epoch3: [2.73110804 0.66725253 1.13986708 0.96940377 1.11560397 0.70474352
 0.94391446 1.0959864  0.95590971 0.03631331]
train_maes at epoch3: [1.46202754 0.4548905  0.68309536 0.58732209 0.4890393  0.39392148
 0.42086601 0.48604293 0.58954725 0.09452553]
test_loss at epoch3: 0.0313590240219365
test_mses at epoch3: [2.63103503 0.54109377 1.11711969 0.94740226 1.08004729 0.58540114
 0.88353419 1.06013145 0.96935671 0.04001225]
test_maes at epoch3: [1.44664329 0.40976295 0.67548268 0.61484016 0.52418903 0.32072966
 0.45515431 0.50082619 0.63329027 0.08892627]

train_loss at epoch4: 0.03108811702417291
train_mses at epoch4: [2.97806081 0.50138796 1.20067213 0.92534993 1.09553359 0.52849509
 0.83651054 1.04376727 0.95901739 0.04947967]
train_maes at epoch4: [1.54680395 0.39903282 0.71904324 0.63332643 0.5432425  0.31578835
 0.47975549 0.51995466 0.65528408 0.11011436]
test_loss at epoch4: 0.028761738020440807
test_mses at epoch4: [2.50054094 0.38728473 1.09502482 0.83254753 1.05363005 0.43620946
 0.82217166 1.02415681 0.92213274 0.04472931]
test_maes at epoch4: [1.38169512 0.28111396 0.65776555 0.54481518 0.5302644  0.28310104
 0.50982005 0.54456659 0.64074731 0.09483468]

train_loss at epoch5: 0.028378910344579945
train_mses at epoch5: [2.73990264 0.34227285 1.12965477 0.84408613 1.04443748 0.41911314
 0.78548487 1.01705825 0.91290215 0.04875166]
train_maes at epoch5: [1.43858355 0.27647455 0.68285099 0.5564174  0.52449103 0.30577009
 0.51543808 0.55456235 0.64683363 0.11275265]
test_loss at epoch5: 0.02618659514447917
test_mses at epoch5: [2.420629   0.28149673 1.06316893 0.682311   1.02070079 0.31686892
 0.75951586 0.98551601 0.84775198 0.04966872]
test_maes at epoch5: [1.31788523 0.24020168 0.64159962 0.46289429 0.51318992 0.28361676
 0.5215589  0.55064182 0.58512201 0.09263165]

train_loss at epoch6: 0.026473121798556785
train_mses at epoch6: [2.60199049 0.28386085 1.08435993 0.64071733 1.02792621 0.30656648
 0.73929571 1.0035611  0.8914581  0.05749537]
train_maes at epoch6: [1.37024768 0.26361648 0.64373613 0.45720619 0.50186895 0.31740988
 0.5242331  0.56644143 0.59191276 0.12500228]
test_loss at epoch6: 0.023680730358414028
test_mses at epoch6: [2.39537526 0.20152694 1.01022122 0.5248971  0.98592763 0.25256066
 0.67589688 0.93921906 0.7638427  0.05120779]
test_maes at epoch6: [1.25171638 0.24325667 0.57205508 0.37929463 0.48698609 0.29107694
 0.49408762 0.5372359  0.5059314  0.09101563]

train_loss at epoch7: 0.02309267028518345
train_mses at epoch7: [2.63304916 0.19363233 1.06988239 0.42554    1.00516439 0.23545857
 0.62903776 0.96151806 0.75936498 0.0552488 ]
train_maes at epoch7: [1.29306973 0.2627695  0.59853309 0.37831482 0.47664317 0.32224923
 0.48622028 0.53889599 0.5070369  0.12984296]
test_loss at epoch7: 0.020899345693380936
test_mses at epoch7: [2.33796656 0.16717106 0.93364165 0.36130842 0.92897993 0.23260996
 0.54749739 0.85728399 0.66759463 0.04198515]
test_maes at epoch7: [1.19866481 0.26612755 0.51139136 0.36454836 0.46857729 0.32807765
 0.43675566 0.503204   0.45767039 0.08781216]

train_loss at epoch8: 0.021197048866230507
train_mses at epoch8: [2.21996714 0.16187194 0.94370775 0.38754661 0.9295342  0.26782626
 0.5429289  0.8739007  0.66293214 0.04922275]
train_maes at epoch8: [1.18668388 0.26904063 0.53404532 0.41195249 0.48161398 0.38214009
 0.42701169 0.51210904 0.46481305 0.13468891]
test_loss at epoch8: 0.01849060602810072
test_mses at epoch8: [2.18448035 0.15266698 0.84061481 0.26625155 0.87139643 0.24324827
 0.43173017 0.77589886 0.56972798 0.02519471]
test_maes at epoch8: [1.1265144  0.27450553 0.46509297 0.35108025 0.4772823  0.37571829
 0.37038749 0.48457422 0.41926648 0.08449698]

train_loss at epoch9: 0.01865384824897932
train_mses at epoch9: [2.20143794 0.18853136 0.86598961 0.28893071 0.84911984 0.26415614
 0.39191895 0.80067171 0.59495567 0.03268836]
train_maes at epoch9: [1.12860905 0.28780067 0.47979396 0.35503913 0.49698082 0.39408593
 0.38352318 0.52303247 0.42587166 0.11668615]
test_loss at epoch9: 0.016230764920296875
test_mses at epoch9: [1.9646642  0.11056026 0.72701863 0.20333414 0.81885259 0.24425931
 0.35230067 0.70289792 0.47967403 0.01956809]
test_maes at epoch9: [1.02711308 0.23512357 0.44507882 0.28484563 0.5003963  0.36032207
 0.32387548 0.47332054 0.38565228 0.08431606]

train_loss at epoch10: 0.01639156626618427
train_mses at epoch10: [2.1239051  0.11495891 0.70550722 0.22274019 0.81908728 0.29043709
 0.35242867 0.73241326 0.44334798 0.03944466]
train_maes at epoch10: [1.09868634 0.25519785 0.47726902 0.29994893 0.53108757 0.38582671
 0.36510951 0.4958168  0.41511805 0.14556321]
test_loss at epoch10: 0.014141619205474854
test_mses at epoch10: [1.75595129 0.08754112 0.62839553 0.21004633 0.75326077 0.21963273
 0.27732591 0.61680037 0.37147456 0.01815971]
test_maes at epoch10: [0.95151116 0.20372583 0.42573062 0.32012042 0.5056438  0.33946205
 0.30878712 0.4508968  0.36385042 0.08631629]

train_loss at epoch11: 0.014825321410013281
train_mses at epoch11: [1.75702955 0.13071276 0.62201076 0.24489738 0.73062425 0.24240656
 0.30078933 0.6747611  0.37588846 0.03466303]
train_maes at epoch11: [0.97842559 0.24305221 0.44615695 0.35822334 0.50827279 0.33578901
 0.33373305 0.46123778 0.39782244 0.13440382]
test_loss at epoch11: 0.01224495988825093
test_mses at epoch11: [1.51769022 0.07256815 0.54245199 0.20056948 0.67475319 0.18135126
 0.24267169 0.53060537 0.29466866 0.01698185]
test_maes at epoch11: [0.90419062 0.17607311 0.40454106 0.31922524 0.47875692 0.29102037
 0.30071412 0.38784879 0.33401186 0.08731444]

train_loss at epoch12: 0.013787323689979055
train_mses at epoch12: [1.70772001 0.10785895 0.63075092 0.26703273 0.72203319 0.23048037
 0.2646885  0.52790802 0.33786871 0.03079291]
train_maes at epoch12: [1.00502529 0.20853363 0.4716848  0.34751529 0.49914931 0.33888048
 0.333115   0.4084603  0.39331095 0.12893033]
test_loss at epoch12: 0.010472045320531597
test_mses at epoch12: [1.2961978  0.06154279 0.46229102 0.17938654 0.58491403 0.1537363
 0.21919086 0.4508257  0.23426035 0.01507895]
test_maes at epoch12: [0.85316417 0.15444739 0.36589374 0.2856541  0.43161983 0.25651857
 0.28896142 0.36650605 0.29908676 0.08296595]

train_loss at epoch13: 0.012384710752445719
train_mses at epoch13: [1.54423151 0.1072994  0.55677852 0.2221746  0.64650401 0.20848005
 0.24408268 0.49247766 0.27941404 0.03630269]
train_maes at epoch13: [0.95622398 0.23989285 0.41402396 0.31840225 0.4425554  0.31740244
 0.34730795 0.42068114 0.352893   0.13928956]
test_loss at epoch13: 0.008943024698806845
test_mses at epoch13: [1.13792825 0.04924505 0.38433938 0.16158238 0.50143155 0.12155949
 0.21132373 0.38212452 0.1900215  0.01427674]
test_maes at epoch13: [0.78428449 0.13646742 0.33507281 0.26423679 0.38509538 0.24183215
 0.28166303 0.35147251 0.26287601 0.0788111 ]

train_loss at epoch14: 0.009563561043013697
train_mses at epoch14: [1.0523023  0.07995463 0.37963404 0.18044188 0.51616803 0.14737397
 0.22313563 0.37810596 0.23315017 0.02684935]
train_maes at epoch14: [0.76571339 0.20352558 0.37741525 0.28839897 0.41733864 0.27902497
 0.31471376 0.39395227 0.3454466  0.11816594]
test_loss at epoch14: 0.007654583810464195
test_mses at epoch14: [0.91850385 0.03917238 0.33081868 0.14746077 0.41061044 0.07594972
 0.19404024 0.33009062 0.19386493 0.01270038]
test_maes at epoch14: [0.73296395 0.12679936 0.3051692  0.24887737 0.38224726 0.20725584
 0.28141288 0.37031787 0.28493232 0.06412717]

train_loss at epoch15: 0.009040955616080242
train_mses at epoch15: [1.18116485 0.07803712 0.32950508 0.2004009  0.50235638 0.10794961
 0.21424243 0.35005168 0.2350786  0.03592712]
train_maes at epoch15: [0.89080084 0.19275889 0.34351046 0.31276528 0.45686402 0.26120368
 0.30769574 0.39920429 0.34918432 0.14146503]
test_loss at epoch15: 0.006248303405616594
test_mses at epoch15: [0.75388368 0.02706234 0.26151933 0.13671164 0.3310488  0.0509019
 0.17804356 0.27508557 0.1433416  0.01238561]
test_maes at epoch15: [0.6824405  0.09762192 0.26330969 0.22662232 0.35290394 0.17394982
 0.2342037  0.30204994 0.24259346 0.05904542]

train_loss at epoch16: 0.007852423774159473
train_mses at epoch16: [0.97241748 0.07556606 0.24863095 0.14238817 0.41458356 0.13099947
 0.18650782 0.33802357 0.22866446 0.03297255]
train_maes at epoch16: [0.82129567 0.21225081 0.31140748 0.27778967 0.39146482 0.26645064
 0.2793299  0.36808151 0.35076987 0.13378498]
test_loss at epoch16: 0.005500532524741214
test_mses at epoch16: [0.67320075 0.02809178 0.24063173 0.14399382 0.25754812 0.04676933
 0.14055161 0.24940409 0.12693678 0.01454998]
test_maes at epoch16: [0.59116439 0.11327732 0.25228208 0.21788588 0.28375921 0.1469284
 0.2028517  0.28734678 0.23646721 0.06321508]

train_loss at epoch17: 0.007434500300365946
train_mses at epoch17: [1.07120547 0.04996197 0.25579172 0.23005488 0.30855033 0.11368805
 0.23868079 0.25141744 0.16540954 0.03067604]
train_maes at epoch17: [0.82547279 0.17561122 0.30469782 0.30102521 0.31562516 0.23696689
 0.27498197 0.3032785  0.27677386 0.12248271]
test_loss at epoch17: 0.004944794689831527
test_mses at epoch17: [0.72594199 0.02233035 0.22586347 0.12833816 0.20157205 0.03605531
 0.13112608 0.24183705 0.11295341 0.0133676 ]
test_maes at epoch17: [0.62456151 0.09163003 0.25610125 0.20118748 0.24373996 0.14107034
 0.18405262 0.29521161 0.217201   0.06187032]

train_loss at epoch18: 0.006736357574877532
train_mses at epoch18: [0.82366397 0.06077526 0.250388   0.20016472 0.26999245 0.07845982
 0.16766368 0.25605462 0.20140395 0.03914604]
train_maes at epoch18: [0.70815367 0.17273212 0.31495425 0.27923546 0.32785453 0.20753201
 0.25595737 0.33667112 0.2953128  0.1491446 ]
test_loss at epoch18: 0.004230289922460266
test_mses at epoch18: [0.56755889 0.01245752 0.20718478 0.09836048 0.15525432 0.0242991
 0.13697568 0.21766536 0.09462096 0.01000911]
test_maes at epoch18: [0.53912329 0.09477032 0.25902234 0.1879766  0.22366498 0.11767455
 0.21104984 0.24271045 0.19218155 0.05387966]

train_loss at epoch19: 0.006014537066221237
train_mses at epoch19: [0.82609541 0.05674672 0.27050248 0.18943167 0.21952467 0.08140021
 0.14880715 0.25549772 0.1076502  0.03220359]
train_maes at epoch19: [0.72204656 0.16691817 0.3320051  0.28426791 0.2991062  0.19589851
 0.23593391 0.31574451 0.23498154 0.13373779]
test_loss at epoch19: 0.0037901516517867212
test_mses at epoch19: [0.49999136 0.01115653 0.18681778 0.0850061  0.11961063 0.03057002
 0.12743452 0.20828588 0.08140526 0.00938478]
test_maes at epoch19: [0.49641405 0.08494147 0.22942038 0.18613795 0.20256461 0.11976067
 0.20651125 0.22990396 0.16856706 0.05304519]

train_loss at epoch20: 0.005623981194651645
train_mses at epoch20: [0.79722388 0.04681809 0.22339354 0.10249102 0.17193729 0.05718653
 0.19157306 0.27968654 0.15774516 0.03134313]
train_maes at epoch20: [0.66494617 0.15316254 0.29775006 0.2394909  0.27963478 0.18303282
 0.25913551 0.30924702 0.27401147 0.13170872]
test_loss at epoch20: 0.0036410335451364517
test_mses at epoch20: [0.51124821 0.01186966 0.19357659 0.0757949  0.09330877 0.03055515
 0.11307599 0.20225997 0.09343468 0.0101    ]
test_maes at epoch20: [0.50228749 0.07626929 0.24140931 0.16608086 0.17240332 0.12108311
 0.1771295  0.22281081 0.17334458 0.05608079]

train_loss at epoch21: 0.004889363018066987
train_mses at epoch21: [0.56981888 0.03549125 0.21305252 0.10569594 0.136085   0.08542375
 0.12099382 0.2712137  0.11012575 0.02904778]
train_maes at epoch21: [0.56286809 0.14276479 0.27817194 0.22189518 0.2421811  0.22157454
 0.22245032 0.28898825 0.20463229 0.13112855]
test_loss at epoch21: 0.003059264758358831
test_mses at epoch21: [0.47497807 0.02547915 0.16891553 0.05817969 0.06371954 0.01603687
 0.0908084  0.19312805 0.06311126 0.01009605]
test_maes at epoch21: [0.50454146 0.10162812 0.20815265 0.14611251 0.15382863 0.08986944
 0.15750696 0.22274813 0.15549308 0.06040963]

train_loss at epoch22: 0.004855307062035022
train_mses at epoch22: [0.8021888  0.05626406 0.22995709 0.08997547 0.11172385 0.07858142
 0.1374547  0.17722029 0.1617848  0.03225783]
train_maes at epoch22: [0.7011129  0.15432669 0.30638552 0.22781378 0.23050452 0.1960865
 0.21424956 0.27201761 0.25807515 0.13139449]
test_loss at epoch22: 0.002951030578950177
test_mses at epoch22: [0.45848182 0.0156058  0.16165374 0.04868285 0.04924792 0.02338413
 0.07979363 0.20225575 0.07460883 0.00922791]
test_maes at epoch22: [0.47456839 0.08758454 0.21040894 0.13017038 0.13685334 0.11554469
 0.15335343 0.26610116 0.18885541 0.05622204]

train_loss at epoch23: 0.004760139986224796
train_mses at epoch23: [0.73305683 0.04508913 0.23689704 0.09769763 0.10221259 0.04551828
 0.13232355 0.28265233 0.10601664 0.02346918]
train_maes at epoch23: [0.6860911  0.16328695 0.27636758 0.22506527 0.22491023 0.15827975
 0.22333557 0.31157771 0.22950222 0.11006002]
test_loss at epoch23: 0.0028751104584206705
test_mses at epoch23: [0.52494327 0.02361128 0.17047269 0.05909805 0.03585538 0.02538276
 0.07157009 0.20098829 0.04559916 0.00942201]
test_maes at epoch23: [0.48614882 0.095457   0.20220876 0.15638183 0.1179829  0.10458636
 0.13866721 0.25515312 0.13023583 0.04987606]

train_loss at epoch24: 0.003972828388214111
train_mses at epoch24: [0.62092278 0.05165307 0.20056025 0.08100576 0.08814471 0.05731239
 0.10992269 0.23351338 0.06499827 0.0204807 ]
train_maes at epoch24: [0.59471155 0.14681499 0.25496144 0.19086206 0.19057803 0.17712949
 0.1997116  0.26879956 0.19920659 0.10180693]
test_loss at epoch24: 0.0023611092373080874
test_mses at epoch24: [0.41098291 0.01875023 0.14877532 0.03603161 0.02888401 0.01376054
 0.0647467  0.17306448 0.03413203 0.00994949]
test_maes at epoch24: [0.45562143 0.09908675 0.18613906 0.1115766  0.11400257 0.08777454
 0.13177528 0.21860916 0.12526198 0.05742279]

train_loss at epoch25: 0.0042730958565421725
train_mses at epoch25: [0.65834831 0.05238399 0.210122   0.08998972 0.07903733 0.07781697
 0.09891998 0.25075204 0.06356012 0.02973882]
train_maes at epoch25: [0.62781154 0.17313056 0.26908054 0.20330125 0.20115603 0.18110606
 0.18205359 0.27342261 0.17922469 0.12212331]
test_loss at epoch25: 0.002576317354712797
test_mses at epoch25: [0.46981215 0.00772496 0.18640587 0.06946505 0.01995981 0.02268698
 0.05419886 0.18043277 0.02744964 0.00972735]
test_maes at epoch25: [0.42684322 0.06212662 0.21170793 0.13324947 0.0954405  0.10216874
 0.11631351 0.21309889 0.10118668 0.05046623]

train_loss at epoch26: 0.004278527086843615
train_mses at epoch26: [0.7216403  0.03706761 0.23754055 0.11842161 0.05484881 0.05261203
 0.11208318 0.23777692 0.08042966 0.0212217 ]
train_maes at epoch26: [0.61996501 0.13192389 0.25120759 0.19870257 0.17850376 0.17050578
 0.19201753 0.27731108 0.18838519 0.11035336]
test_loss at epoch26: 0.002458176535108815
test_mses at epoch26: [0.4134699  0.0206354  0.15032255 0.06518143 0.01629944 0.05555694
 0.05256777 0.16084609 0.02288001 0.00964779]
test_maes at epoch26: [0.39798458 0.09739112 0.17487335 0.12412092 0.08519419 0.14793223
 0.11215078 0.20779197 0.09449191 0.057043  ]

train_loss at epoch27: 0.004139247147933296
train_mses at epoch27: [0.58812972 0.05900584 0.20384242 0.09676439 0.05302299 0.04528572
 0.16036287 0.20057254 0.07592159 0.03032898]
train_maes at epoch27: [0.55576147 0.17319706 0.26577588 0.23068698 0.16765129 0.15363884
 0.21029678 0.28330273 0.18364162 0.12411031]
test_loss at epoch27: 0.0021619367534699645
test_mses at epoch27: [0.35266994 0.00956147 0.15188652 0.07272302 0.01678582 0.0132376
 0.04629807 0.14462903 0.02331578 0.00868097]
test_maes at epoch27: [0.41909686 0.07030624 0.23952385 0.16574899 0.09771802 0.09403202
 0.12129685 0.19822871 0.09901529 0.06255259]

train_loss at epoch28: 0.0037689048634923024
train_mses at epoch28: [0.56336185 0.04906576 0.19069507 0.0765374  0.03871445 0.07582622
 0.05656467 0.23118636 0.10884681 0.01928238]
train_maes at epoch28: [0.55298351 0.14718287 0.29046267 0.19951651 0.15726053 0.19996437
 0.16348058 0.27327989 0.21673763 0.10208574]
test_loss at epoch28: 0.0023702843438671985
test_mses at epoch28: [0.39270016 0.01506353 0.13253753 0.07274861 0.0197818  0.02768128
 0.06306384 0.14242947 0.05160681 0.00905653]
test_maes at epoch28: [0.40318248 0.06975785 0.1602341  0.13799003 0.08866319 0.11738446
 0.12031798 0.19726487 0.14330332 0.04845078]

train_loss at epoch29: 0.0035558298392140346
train_mses at epoch29: [0.59007883 0.0467476  0.1948354  0.13843829 0.05359281 0.05003771
 0.0534105  0.16633245 0.07024728 0.01954668]
train_maes at epoch29: [0.52936472 0.1467413  0.24295447 0.20130552 0.16120526 0.15947382
 0.14188394 0.25069691 0.17331521 0.09883252]
test_loss at epoch29: 0.0018508060146933017
test_mses at epoch29: [0.2948557  0.01001611 0.13623354 0.03457517 0.01348856 0.01120988
 0.05038323 0.12955628 0.02145884 0.01103892]
test_maes at epoch29: [0.3441455  0.06704127 0.18946354 0.10943974 0.08386218 0.07328055
 0.10514017 0.1801059  0.08711104 0.06290819]

train_loss at epoch30: 0.0033562202816424165
train_mses at epoch30: [0.42954859 0.04826034 0.20113652 0.05901856 0.03916099 0.07183909
 0.09389777 0.1828136  0.04553753 0.02258119]
train_maes at epoch30: [0.4826643  0.15488171 0.25636005 0.17019049 0.1592803  0.18118717
 0.19520279 0.24895148 0.15623307 0.11361973]
test_loss at epoch30: 0.001471647061407566
test_mses at epoch30: [0.23082319 0.00592514 0.10659555 0.0339051  0.00914176 0.0049111
 0.02730733 0.12298439 0.01205179 0.00985862]
test_maes at epoch30: [0.33241917 0.05105753 0.17319152 0.11034745 0.06984372 0.05514812
 0.07795878 0.17243484 0.06826151 0.05951897]

train_loss at epoch31: 0.0028303549827440925
train_mses at epoch31: [0.46506095 0.03492415 0.19252352 0.0613216  0.04586002 0.04459516
 0.04154124 0.14400464 0.04512943 0.02354997]
train_maes at epoch31: [0.50379913 0.13367993 0.25317437 0.16394211 0.1424762  0.14811456
 0.13965113 0.2276423  0.15922836 0.11752614]
test_loss at epoch31: 0.0020465358848805013
test_mses at epoch31: [0.30908238 0.0268546  0.1301528  0.03804349 0.02045418 0.03572116
 0.04393446 0.1328504  0.02749282 0.0078541 ]
test_maes at epoch31: [0.37870231 0.09116398 0.16575754 0.10684015 0.0822051  0.12876591
 0.09567441 0.15860853 0.09419896 0.0410768 ]

train_loss at epoch32: 0.002912417216145474
train_mses at epoch32: [0.42839786 0.0609286  0.12113253 0.06100797 0.06250996 0.04010173
 0.08682826 0.14300158 0.06986025 0.01788464]
train_maes at epoch32: [0.50832547 0.14984492 0.21659303 0.16532117 0.16377162 0.1520615
 0.16448528 0.22466095 0.18443903 0.1010238 ]
test_loss at epoch32: 0.001254631449346957
test_mses at epoch32: [0.23017056 0.01291055 0.08844711 0.01823324 0.01106998 0.00708467
 0.01856924 0.10536058 0.01119741 0.00674229]
test_maes at epoch32: [0.3641172  0.07048707 0.15878207 0.09272013 0.07796212 0.06537143
 0.07155711 0.15461194 0.07669022 0.04390959]

train_loss at epoch33: 0.003265850650875465
train_mses at epoch33: [0.5583357  0.06872219 0.20787    0.0503069  0.0384844  0.06273662
 0.06674885 0.17278348 0.04463489 0.02144162]
train_maes at epoch33: [0.55575751 0.15414741 0.28069025 0.1583263  0.14770724 0.16478608
 0.16887905 0.23256342 0.15853328 0.10921574]
test_loss at epoch33: 0.0012570327228826025
test_mses at epoch33: [0.1917171  0.00544786 0.07780427 0.03526098 0.00835743 0.00762694
 0.01784732 0.0967841  0.02667054 0.00746919]
test_maes at epoch33: [0.31187862 0.04964993 0.13479641 0.12171625 0.0676408  0.06539699
 0.07083144 0.14384687 0.11080733 0.0458714 ]

train_loss at epoch34: 0.002505460268129473
train_mses at epoch34: [0.3932993  0.04349552 0.11201288 0.04769857 0.04515656 0.05051159
 0.04665861 0.14986354 0.04307351 0.01994717]
train_maes at epoch34: [0.47505066 0.13785268 0.19726251 0.14704968 0.14184489 0.14766737
 0.14623587 0.2381241  0.15342252 0.1033612 ]
test_loss at epoch34: 0.0011737750357259874
test_mses at epoch34: [0.19259694 0.0049563  0.07314231 0.01990831 0.01121821 0.01003466
 0.01768739 0.0910462  0.02618196 0.00730512]
test_maes at epoch34: [0.29453424 0.04523064 0.11718529 0.08759223 0.07610236 0.06439441
 0.07623144 0.15526956 0.0961316  0.03971915]

train_loss at epoch35: 0.0020159939708917036
train_mses at epoch35: [0.3162714  0.04278467 0.07374255 0.04996965 0.02746694 0.03418185
 0.06839221 0.10510379 0.03154357 0.01625879]
train_maes at epoch35: [0.42154855 0.12747038 0.19120586 0.16106859 0.12920673 0.13262154
 0.17034843 0.20291538 0.12775808 0.09699653]
test_loss at epoch35: 0.0009291262527846772
test_mses at epoch35: [0.16905727 0.00738533 0.06025595 0.01450721 0.01133006 0.00647889
 0.0112176  0.07415301 0.01407317 0.00698933]
test_maes at epoch35: [0.28757271 0.04699953 0.11587767 0.07614892 0.0833686  0.05708406
 0.06875819 0.1372922  0.07680141 0.04113899]

train_loss at epoch36: 0.0020609487495992494
train_mses at epoch36: [0.27374832 0.05610772 0.08611741 0.04196648 0.03859091 0.04348872
 0.04763226 0.0945373  0.0349241  0.01971702]
train_maes at epoch36: [0.41670728 0.13861747 0.18273012 0.13987282 0.14950512 0.14954733
 0.14706963 0.20494269 0.13434801 0.10195537]
test_loss at epoch36: 0.0008176215114476888
test_mses at epoch36: [0.13075347 0.00498111 0.05430705 0.01126101 0.00811165 0.00635771
 0.01502747 0.06550021 0.01128125 0.00710758]
test_maes at epoch36: [0.24607179 0.04378145 0.11165544 0.07123937 0.06620726 0.06159582
 0.09575605 0.1262136  0.07399454 0.04306652]

train_loss at epoch37: 0.0020053639765019
train_mses at epoch37: [0.28946422 0.04443223 0.10905656 0.04067159 0.0326651  0.03226299
 0.0385617  0.09369918 0.03491247 0.02121079]
train_maes at epoch37: [0.41984695 0.14093839 0.21894695 0.15738134 0.1413365  0.13893244
 0.14234064 0.19943869 0.13846716 0.10924427]
test_loss at epoch37: 0.0008911107383344485
test_mses at epoch37: [0.13007955 0.00673524 0.05012537 0.01634338 0.00717938 0.01043078
 0.02441903 0.07005896 0.00983172 0.00731905]
test_maes at epoch37: [0.26175789 0.05558984 0.11174485 0.08909461 0.06452582 0.07409767
 0.12331042 0.13893243 0.0680739  0.04718602]

train_loss at epoch38: 0.00212612671210714
train_mses at epoch38: [0.33833304 0.02585854 0.10033161 0.0545564  0.02492376 0.03623303
 0.05099437 0.10925551 0.04845392 0.02146149]
train_maes at epoch38: [0.46106368 0.12334228 0.19058426 0.16918826 0.12435045 0.14759273
 0.1687029  0.20570285 0.15677728 0.09796213]
test_loss at epoch38: 0.0008527457349650238
test_mses at epoch38: [0.15009584 0.00733151 0.04683953 0.02925076 0.00629266 0.01293667
 0.01128501 0.05978383 0.01052142 0.0075138 ]
test_maes at epoch38: [0.25681767 0.05183419 0.10270641 0.11484103 0.06091575 0.07674641
 0.06089799 0.12018221 0.06912719 0.04931731]

train_loss at epoch39: 0.002006400255081446
train_mses at epoch39: [0.23088364 0.03182689 0.0723467  0.05573884 0.0521645  0.0512841
 0.07203591 0.06132909 0.04468032 0.01772437]
train_maes at epoch39: [0.37010039 0.13058818 0.17978278 0.16379802 0.13531523 0.1699186
 0.17916736 0.16921325 0.15304211 0.09510955]
test_loss at epoch39: 0.0007453266045321589
test_mses at epoch39: [0.13602836 0.0067802  0.0382703  0.01693343 0.01534381 0.00693798
 0.0187769  0.04545838 0.01045341 0.0064414 ]
test_maes at epoch39: [0.25713463 0.05684346 0.10119976 0.08650227 0.09850342 0.06353664
 0.10664083 0.11047264 0.0726199  0.04796961]

train_loss at epoch40: 0.001929309502567934
train_mses at epoch40: [0.24265549 0.04621233 0.05349174 0.05104734 0.03681234 0.03709014
 0.04732952 0.10031787 0.04601654 0.02051282]
train_maes at epoch40: [0.36693673 0.12009457 0.15284848 0.15980518 0.14197937 0.14524771
 0.16848284 0.1863701  0.15618336 0.1087689 ]
test_loss at epoch40: 0.0009353903689138267
test_mses at epoch40: [0.25753378 0.00957499 0.05543047 0.03206334 0.0269641  0.0070514
 0.00744214 0.04185263 0.01255057 0.00673837]
test_maes at epoch40: [0.37483832 0.05967149 0.1082674  0.11839884 0.10069161 0.06295093
 0.06652926 0.1241687  0.07308046 0.04451916]

train_loss at epoch41: 0.00201499008614084
train_mses at epoch41: [0.283525   0.03803254 0.08146748 0.06359847 0.0423491  0.04111381
 0.0390803  0.08584103 0.04492083 0.01910897]
train_maes at epoch41: [0.42444515 0.12005044 0.17943712 0.17108106 0.14629669 0.15036121
 0.13407033 0.2002725  0.13584358 0.10040968]
test_loss at epoch41: 0.0006812374593447084
test_mses at epoch41: [0.14120829 0.0043265  0.0424385  0.01089548 0.01521    0.00543563
 0.01424618 0.04305554 0.00718074 0.00634372]
test_maes at epoch41: [0.25790185 0.04573377 0.09784438 0.07569668 0.07284577 0.05315385
 0.07241366 0.12138351 0.05924922 0.04422499]

train_loss at epoch42: 0.0020129159092903137
train_mses at epoch42: [0.32571463 0.08260258 0.08065147 0.07286645 0.02520766 0.02748416
 0.0236377  0.08860589 0.02829598 0.01794679]
train_maes at epoch42: [0.4218688  0.15410379 0.19472903 0.17425611 0.11872753 0.12221154
 0.12093518 0.18906615 0.12467425 0.09876323]
test_loss at epoch42: 0.0006270130043444426
test_mses at epoch42: [0.14249866 0.00467821 0.03611035 0.02020365 0.01624722 0.00499198
 0.01080089 0.03179764 0.00659656 0.00590098]
test_maes at epoch42: [0.27298992 0.04672978 0.10853585 0.08521912 0.07256823 0.05228746
 0.07024788 0.10090714 0.0613287  0.04522141]

train_loss at epoch43: 0.0016379488389129224
train_mses at epoch43: [0.21413042 0.02792613 0.05673055 0.05853571 0.0417597  0.03961482
 0.04102053 0.05865318 0.02981731 0.01804686]
train_maes at epoch43: [0.36372822 0.11508869 0.1544369  0.16663893 0.13993642 0.14524375
 0.12828137 0.16130693 0.13170242 0.10159752]
test_loss at epoch43: 0.0010264855404586895
test_mses at epoch43: [0.1856982  0.03977125 0.04101593 0.06781509 0.01716628 0.00557839
 0.00595158 0.03711608 0.01150472 0.0063168 ]
test_maes at epoch43: [0.30643443 0.08931509 0.09449155 0.13248845 0.10103498 0.0547968
 0.04787105 0.10591705 0.0737515  0.04605669]

train_loss at epoch44: 0.0016479727447680805
train_mses at epoch44: [0.28284077 0.06426475 0.05768252 0.08050426 0.03052069 0.03221264
 0.02040071 0.03046084 0.0328219  0.01775941]
train_maes at epoch44: [0.3940155  0.15985708 0.16214303 0.16631602 0.13485076 0.14104795
 0.09972756 0.13071978 0.13044334 0.09338204]
test_loss at epoch44: 0.0006402727095005305
test_mses at epoch44: [0.11759074 0.02526003 0.0286812  0.03299426 0.01104449 0.00918628
 0.00380641 0.0221922  0.00567724 0.00547349]
test_maes at epoch44: [0.24140347 0.06561645 0.08655395 0.0943225  0.08587284 0.06946004
 0.04014045 0.08837059 0.05646194 0.03949638]

train_loss at epoch45: 0.0017090507418565128
train_mses at epoch45: [0.27864428 0.05468667 0.05844092 0.04357484 0.03533784 0.03752386
 0.03641561 0.05080051 0.04900601 0.01607977]
train_maes at epoch45: [0.41046089 0.14128174 0.15600184 0.15291736 0.12765091 0.14824065
 0.14030191 0.16002438 0.15324872 0.09563495]
test_loss at epoch45: 0.0004252255691782288
test_mses at epoch45: [0.07998959 0.01099168 0.02258248 0.01362535 0.00498087 0.00394921
 0.00444768 0.01838226 0.01053955 0.005262  ]
test_maes at epoch45: [0.19749952 0.05575594 0.08880861 0.07775358 0.05154135 0.04925496
 0.04224671 0.08651785 0.06973482 0.04282016]

train_loss at epoch46: 0.0014557885410992997
train_mses at epoch46: [0.26404798 0.02238086 0.07112057 0.02991476 0.01699294 0.03299912
 0.0341739  0.05205072 0.03920317 0.01970186]
train_maes at epoch46: [0.39278903 0.10816613 0.17517077 0.13583853 0.10321158 0.1346212
 0.11729382 0.15741659 0.14775768 0.102965  ]
test_loss at epoch46: 0.0007673123163049636
test_mses at epoch46: [0.1546842  0.0089596  0.05121051 0.02163434 0.02012308 0.01641091
 0.01474635 0.02643473 0.00535752 0.00545788]
test_maes at epoch46: [0.27538972 0.05885631 0.12743557 0.08314221 0.08849054 0.08061729
 0.06294618 0.09259937 0.04920321 0.03726242]

train_loss at epoch47: 0.0014844658258168595
train_mses at epoch47: [0.25384117 0.03224493 0.06719918 0.04370181 0.03195435 0.03009197
 0.02696116 0.04492765 0.03893099 0.01711599]
train_maes at epoch47: [0.39141808 0.12994646 0.16994288 0.14758181 0.12355714 0.13379183
 0.12126158 0.15472068 0.13700982 0.08793639]
test_loss at epoch47: 0.00039244560605805856
test_mses at epoch47: [0.08576502 0.00471481 0.02363517 0.01040041 0.00979922 0.00953416
 0.00515772 0.01476127 0.00439097 0.00413503]
test_maes at epoch47: [0.20196083 0.03903444 0.08580631 0.06342121 0.06957476 0.07375218
 0.04338947 0.07164568 0.04829584 0.03256773]

train_loss at epoch48: 0.001484354188584763
train_mses at epoch48: [0.28144813 0.02870067 0.05168973 0.0505465  0.0277028  0.04888454
 0.01936774 0.05288265 0.03122763 0.0183517 ]
train_maes at epoch48: [0.40213928 0.11118417 0.15879929 0.1607215  0.11923865 0.14444822
 0.09826077 0.16255884 0.13027149 0.09816513]
test_loss at epoch48: 0.00037463002271302366
test_mses at epoch48: [0.06927076 0.00743168 0.01953204 0.00763694 0.00534801 0.00898001
 0.01155315 0.01336945 0.00519731 0.00443388]
test_maes at epoch48: [0.18330334 0.05756066 0.07463469 0.05618449 0.04974451 0.06437159
 0.05813894 0.06608886 0.05267209 0.03935896]

train_loss at epoch49: 0.0016187256930962853
train_mses at epoch49: [0.27948989 0.03026742 0.06200701 0.03649778 0.04148348 0.04509095
 0.03842163 0.05258523 0.03565458 0.01349745]
train_maes at epoch49: [0.40793584 0.11386768 0.1761844  0.13855209 0.13834297 0.14618322
 0.12881549 0.15887035 0.12965313 0.08565682]
test_loss at epoch49: 0.0004834983416873476
test_mses at epoch49: [0.08254916 0.00845848 0.01742642 0.01095359 0.00521368 0.00601658
 0.01231337 0.03453413 0.00844937 0.00448742]
test_maes at epoch49: [0.22482004 0.06053376 0.07840219 0.06464077 0.04897117 0.05394847
 0.05567077 0.11780233 0.07015    0.04134863]

