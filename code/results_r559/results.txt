Train on FAU 5 light, test on FAU 5 dark, alt
train_loss at epoch0: 0.035187790241647275
train_mses at epoch0: [3.27272461 1.06990012 1.08029312 1.06755613 1.06614596 0.7662899
 1.07873555 1.07134708 1.07165529 0.01963286]
train_maes at epoch0: [1.52835882 0.48382135 0.63307504 0.55305169 0.45576934 0.44653704
 0.46281444 0.50638385 0.50164549 0.05803916]
test_loss at epoch0: 0.0318659523700146
test_mses at epoch0: [2.98457793 1.00055052 1.09291388 0.98362922 0.99257281 0.40305889
 1.00550545 1.02044634 1.00990525 0.01843483]
test_maes at epoch0: [1.55383289 0.53825674 0.64449534 0.59192569 0.53391615 0.30436927
 0.54715597 0.55916765 0.56693177 0.04792841]

train_loss at epoch1: 0.02861622483172315
train_mses at epoch1: [2.57484983 0.86084935 1.12808681 0.90423997 0.89080767 0.21486185
 0.91943029 0.95010992 0.90117664 0.01858266]
train_maes at epoch1: [1.42445149 0.51899382 0.64184361 0.57080144 0.5404242  0.27266755
 0.54676605 0.51821333 0.52072618 0.07896333]
test_loss at epoch1: 0.02469038151680155
test_mses at epoch1: [1.82432769 0.72501976 1.03338154 0.77460099 0.7196289  0.22990411
 0.75952014 0.82544632 0.78558089 0.014405  ]
test_maes at epoch1: [1.1752618  0.45044299 0.6056297  0.44613319 0.43719588 0.35840803
 0.44254474 0.44414179 0.48964598 0.07184884]

train_loss at epoch2: 0.019438232132729064
train_mses at epoch2: [1.11377912 0.49682093 0.96057101 0.62844421 0.51693819 0.1507528
 0.51890445 0.76448184 0.61663039 0.03172434]
train_maes at epoch2: [0.83277393 0.45676149 0.56649954 0.49231745 0.42542024 0.30085472
 0.4198841  0.51854834 0.46425096 0.13342565]
test_loss at epoch2: 0.01823244627485884
test_mses at epoch2: [1.95163085 0.59066359 0.79002013 0.57823235 0.44111221 0.12932396
 0.37691244 0.62460123 0.69892951 0.01206881]
test_maes at epoch2: [1.08592047 0.50959781 0.52686743 0.45284267 0.41624688 0.23507042
 0.34130676 0.44151833 0.66034561 0.06916496]

train_loss at epoch3: 0.011576815868945833
train_mses at epoch3: [0.70113156 0.19721082 0.71844737 0.36312135 0.29595024 0.09498964
 0.18891611 0.55531929 0.33756628 0.02621889]
train_maes at epoch3: [0.64859313 0.30864701 0.49071221 0.40600355 0.38613008 0.22219378
 0.2876142  0.48905726 0.37290104 0.12471846]
test_loss at epoch3: 0.010492949187755585
test_mses at epoch3: [0.6699719  0.20115968 0.58271496 0.48069007 0.23515136 0.17178365
 0.09755652 0.35909493 0.33652284 0.01006141]
test_maes at epoch3: [0.61316439 0.26642832 0.57167541 0.5037319  0.32188617 0.29497742
 0.21691261 0.37675777 0.40108867 0.06984676]

train_loss at epoch4: 0.006631882235090783
train_mses at epoch4: [0.57815978 0.0988057  0.41077435 0.18870125 0.15367188 0.09332234
 0.08805214 0.34901533 0.17297647 0.02052562]
train_maes at epoch4: [0.5564126  0.22996934 0.4352388  0.31007412 0.27786978 0.22096298
 0.20764342 0.40947534 0.28730582 0.10826546]
test_loss at epoch4: 0.006694210954803102
test_mses at epoch4: [0.64759364 0.22624434 0.30540946 0.32937219 0.16030825 0.08155331
 0.07008764 0.14752466 0.21136814 0.00673668]
test_maes at epoch4: [0.55262838 0.30497404 0.35767204 0.36311582 0.26863987 0.18248948
 0.18685797 0.26664144 0.37667729 0.05976689]

train_loss at epoch5: 0.0035224981606006623
train_mses at epoch5: [0.45236905 0.06576913 0.16985215 0.1159311  0.11302825 0.06774341
 0.06296649 0.12248039 0.08559813 0.02001982]
train_maes at epoch5: [0.50004142 0.18535411 0.2864028  0.23865576 0.23791636 0.18328885
 0.18840569 0.26062714 0.2160285  0.11157978]
test_loss at epoch5: 0.004641072459994479
test_mses at epoch5: [0.4883492  0.1686701  0.16965894 0.30485754 0.13223579 0.05108511
 0.06998482 0.09104788 0.06973026 0.00447246]
test_maes at epoch5: [0.50475267 0.23904492 0.29436504 0.33257225 0.2131168  0.14961558
 0.18363078 0.20357625 0.19708431 0.0475816 ]

train_loss at epoch6: 0.0025074539507957214
train_mses at epoch6: [0.38892814 0.06845616 0.08614557 0.07867861 0.06172169 0.05560656
 0.05179019 0.09196337 0.06735552 0.0157406 ]
train_maes at epoch6: [0.4792848  0.1844633  0.21675545 0.20052751 0.17579672 0.16969763
 0.16762227 0.22016114 0.1827981  0.09322721]
test_loss at epoch6: 0.004246434965666304
test_mses at epoch6: [0.55709284 0.22180068 0.13399222 0.21712243 0.12094143 0.03653583
 0.07168814 0.07753483 0.07467518 0.006453  ]
test_maes at epoch6: [0.51298431 0.2459958  0.25188738 0.25894794 0.19503561 0.12220454
 0.15993874 0.16905064 0.19445751 0.04944336]

train_loss at epoch7: 0.001868808332909929
train_mses at epoch7: [0.34139137 0.05259646 0.06127266 0.05541901 0.04309214 0.04044701
 0.05206748 0.06448402 0.04359744 0.01279237]
train_maes at epoch7: [0.42490037 0.16500299 0.17967459 0.16907714 0.14818557 0.13450467
 0.16113178 0.18915602 0.15102827 0.08581782]
test_loss at epoch7: 0.004299966071514373
test_mses at epoch7: [0.56779651 0.1848293  0.12817956 0.31689996 0.10840902 0.04675806
 0.05874037 0.05990962 0.05892942 0.00572858]
test_maes at epoch7: [0.50380323 0.25319902 0.24520327 0.33330754 0.18518564 0.13810727
 0.15658552 0.16414929 0.1857155  0.04655294]

train_loss at epoch8: 0.0016763477804178886
train_mses at epoch8: [0.28209046 0.04956208 0.05946042 0.06743614 0.03741339 0.03713844
 0.03286998 0.0478597  0.0405396  0.01137632]
train_maes at epoch8: [0.39376617 0.16043012 0.16484179 0.18482859 0.13631391 0.13499574
 0.13543655 0.15816021 0.1413937  0.07974034]
test_loss at epoch8: 0.0034426658592642623
test_mses at epoch8: [0.48983041 0.15644008 0.10335457 0.16586669 0.11222285 0.0343224
 0.05092245 0.06023811 0.08836375 0.00460355]
test_maes at epoch8: [0.4630213  0.21257926 0.20774067 0.20298646 0.17695078 0.09580623
 0.13785648 0.13746225 0.23464633 0.03676289]

train_loss at epoch9: 0.0016630678100788847
train_mses at epoch9: [0.23080657 0.04105206 0.0558639  0.07086612 0.03275598 0.03811487
 0.03441003 0.06183471 0.03774469 0.01388611]
train_maes at epoch9: [0.36325799 0.14257125 0.17045627 0.19071364 0.12107244 0.13580102
 0.13038392 0.16818073 0.13332005 0.08639731]
test_loss at epoch9: 0.004343035151349737
test_mses at epoch9: [0.44836945 0.18996389 0.17975541 0.31296582 0.10570217 0.02958045
 0.05226709 0.03794128 0.07662291 0.00440131]
test_maes at epoch9: [0.45602553 0.25003629 0.3427175  0.31351068 0.17921711 0.11355317
 0.14680599 0.1273066  0.22375658 0.037846  ]

train_loss at epoch10: 0.001384298154648314
train_mses at epoch10: [0.21094853 0.04535817 0.05025946 0.04489167 0.02956242 0.03044826
 0.03354612 0.03913759 0.03413154 0.01219126]
train_maes at epoch10: [0.35291608 0.14461399 0.16909222 0.15396871 0.11842665 0.12659546
 0.13195157 0.14154342 0.13062454 0.07839307]
test_loss at epoch10: 0.0033142442597036666
test_mses at epoch10: [0.47719005 0.13764299 0.10015717 0.18200355 0.09488357 0.0501967
 0.04368623 0.05011577 0.08420647 0.00318568]
test_maes at epoch10: [0.47054217 0.21342693 0.22297266 0.19678642 0.16445662 0.15841172
 0.12715709 0.1293897  0.23562259 0.03382617]

train_loss at epoch11: 0.0012882890457168539
train_mses at epoch11: [0.1738726  0.03476833 0.0420987  0.04159396 0.02987703 0.0282629
 0.03527374 0.04783418 0.02906869 0.00955023]
train_maes at epoch11: [0.31238548 0.12844707 0.14513033 0.14390015 0.11790094 0.11527613
 0.12739454 0.1458146  0.11600338 0.07166754]
test_loss at epoch11: 0.003554189419175716
test_mses at epoch11: [0.40717548 0.13540144 0.12413503 0.24545402 0.11402673 0.03775209
 0.0465561  0.04333976 0.05876092 0.0031652 ]
test_maes at epoch11: [0.42469741 0.19516958 0.24512126 0.24120106 0.1844344  0.12645096
 0.12900074 0.11737248 0.17693312 0.03320063]

train_loss at epoch12: 0.0011582486252201365
train_mses at epoch12: [0.15884703 0.03960413 0.03462158 0.03623403 0.02623398 0.02972248
 0.02313991 0.03692234 0.03356417 0.00961417]
train_maes at epoch12: [0.31037864 0.13499119 0.13596258 0.14042625 0.11409686 0.1091737
 0.11350422 0.1314536  0.1298723  0.07218952]
test_loss at epoch12: 0.003104264982678789
test_mses at epoch12: [0.42309348 0.10092253 0.11066198 0.21633253 0.08593924 0.04322884
 0.05237433 0.03496974 0.0525581  0.00240534]
test_maes at epoch12: [0.42607218 0.15379173 0.23190004 0.22804204 0.15512567 0.14447999
 0.13072111 0.11382854 0.17348838 0.0279195 ]

train_loss at epoch13: 0.0011077462794615866
train_mses at epoch13: [0.18106016 0.03104665 0.0368083  0.03357665 0.03166574 0.02923817
 0.02779464 0.03186918 0.02453617 0.00778152]
train_maes at epoch13: [0.3220857  0.12747724 0.13406929 0.13493262 0.11195237 0.11470128
 0.11304631 0.12441836 0.10849251 0.06428597]
test_loss at epoch13: 0.003253234158012461
test_mses at epoch13: [0.42509027 0.12275609 0.10423006 0.18724637 0.13847337 0.02835839
 0.04493023 0.05681785 0.05047406 0.00407157]
test_maes at epoch13: [0.43156628 0.1762473  0.21128762 0.19428895 0.20249993 0.11207791
 0.1253889  0.12712021 0.16987336 0.03634936]

train_loss at epoch14: 0.0010595405434674404
train_mses at epoch14: [0.15486869 0.02944435 0.03749994 0.0272571  0.02647468 0.02896568
 0.02564903 0.03467149 0.02507832 0.0092758 ]
train_maes at epoch14: [0.29958518 0.11694283 0.1387443  0.11934474 0.1101073  0.1154813
 0.11316372 0.12924868 0.10817463 0.06782877]
test_loss at epoch14: 0.0031975871665363615
test_mses at epoch14: [0.4181667  0.14205121 0.12116767 0.20648438 0.08102398 0.03505994
 0.0428676  0.03758282 0.05001181 0.00286027]
test_maes at epoch14: [0.42220129 0.203727   0.25541307 0.20231264 0.14982005 0.12868834
 0.12564642 0.1094774  0.17490693 0.02939974]

train_loss at epoch15: 0.000976351981467389
train_mses at epoch15: [0.15442689 0.0240987  0.03266124 0.03394251 0.02265    0.02371795
 0.02234939 0.02785719 0.02656912 0.0084393 ]
train_maes at epoch15: [0.29574665 0.11121448 0.1268976  0.1300142  0.10341226 0.10394424
 0.10498592 0.12257818 0.11281179 0.06749966]
test_loss at epoch15: 0.0038340952168119716
test_mses at epoch15: [0.61419605 0.16762247 0.12337022 0.28049602 0.1006648  0.04573663
 0.03887445 0.03898746 0.05215933 0.00324365]
test_maes at epoch15: [0.5485289  0.19688179 0.22390511 0.26846236 0.1611239  0.15034184
 0.10548825 0.11725637 0.18239844 0.03042878]

train_loss at epoch16: 0.0009552213700210794
train_mses at epoch16: [0.15735626 0.02549774 0.03871361 0.03060174 0.01993486 0.02593351
 0.02376104 0.024146   0.02361397 0.00700366]
train_maes at epoch16: [0.30354455 0.11312655 0.13399279 0.12904969 0.09582549 0.10375471
 0.10483633 0.1136253  0.10707749 0.06193621]
test_loss at epoch16: 0.0034475130048838066
test_mses at epoch16: [0.52813016 0.16658923 0.10230937 0.23534236 0.09522962 0.02997553
 0.0473282  0.04287587 0.04807844 0.00339022]
test_maes at epoch16: [0.50494298 0.2109393  0.18021505 0.22592411 0.14911262 0.11804178
 0.11725456 0.114341   0.16614323 0.02996765]

train_loss at epoch17: 0.0009173207064258292
train_mses at epoch17: [0.15363867 0.02714681 0.02796905 0.03620975 0.01717152 0.02292266
 0.02517338 0.02470962 0.02208514 0.00684987]
train_maes at epoch17: [0.29296834 0.10719963 0.1189913  0.12458766 0.08555044 0.10149658
 0.10314274 0.11720202 0.1062161  0.06075078]
test_loss at epoch17: 0.0034599178371594306
test_mses at epoch17: [0.44481378 0.15477865 0.10825102 0.26420641 0.09585679 0.02396969
 0.0401041  0.04339849 0.04607693 0.00330815]
test_maes at epoch17: [0.45068323 0.19803343 0.2124802  0.244136   0.14697261 0.10014812
 0.11019029 0.11102098 0.16484529 0.0287991 ]

train_loss at epoch18: 0.0009909794883842164
train_mses at epoch18: [0.14299382 0.02815313 0.0406427  0.03853258 0.02126773 0.01946779
 0.02650118 0.02840545 0.01899429 0.00787257]
train_maes at epoch18: [0.28504341 0.109823   0.13966327 0.13669084 0.09230302 0.09213863
 0.11293506 0.1200125  0.09428623 0.06185553]
test_loss at epoch18: 0.003143937794610541
test_mses at epoch18: [0.33385761 0.12392504 0.14472013 0.22673106 0.07239086 0.02196535
 0.04451961 0.0336468  0.04199599 0.00421244]
test_maes at epoch18: [0.37761758 0.1894388  0.28067476 0.24623119 0.13986582 0.09073491
 0.13313726 0.11036563 0.15484286 0.03291075]

train_loss at epoch19: 0.0009157680609124772
train_mses at epoch19: [0.1406705  0.0220826  0.03828672 0.02817122 0.02065097 0.02848019
 0.02142216 0.02470006 0.01989172 0.00593358]
train_maes at epoch19: [0.27643408 0.10529246 0.13367701 0.11451238 0.09423626 0.1039043
 0.09915725 0.1173195  0.09633397 0.05679171]
test_loss at epoch19: 0.00302549606149501
test_mses at epoch19: [0.36042926 0.11313859 0.11444254 0.2052657  0.11344363 0.02182037
 0.03307308 0.04112618 0.04204931 0.00238581]
test_maes at epoch19: [0.40008176 0.16196345 0.23740995 0.20647045 0.17331516 0.09150105
 0.10250385 0.10910746 0.15153287 0.02607982]

train_loss at epoch20: 0.0009039082584228922
train_mses at epoch20: [0.11502926 0.02396493 0.03379918 0.03034848 0.01925912 0.01876083
 0.02646751 0.02368123 0.02755245 0.00598546]
train_maes at epoch20: [0.25879235 0.10509163 0.12731568 0.12027956 0.09157395 0.09737751
 0.10593091 0.10805919 0.10919759 0.05666986]
test_loss at epoch20: 0.0030824125724587034
test_mses at epoch20: [0.51263678 0.1368906  0.09253906 0.21709867 0.0975731  0.03927768
 0.03583855 0.02653539 0.03357553 0.00343232]
test_maes at epoch20: [0.48738159 0.18200085 0.20018976 0.18461959 0.15240588 0.14434853
 0.11400701 0.0943899  0.13342071 0.02967103]

train_loss at epoch21: 0.0008534880117216009
train_mses at epoch21: [0.10833874 0.02760877 0.02669236 0.02937267 0.02099611 0.02590348
 0.02123429 0.02157002 0.01892163 0.00556893]
train_maes at epoch21: [0.25406057 0.1089665  0.12066796 0.11748296 0.08967317 0.10325412
 0.10127162 0.10614715 0.09697017 0.05407442]
test_loss at epoch21: 0.0034506850063483765
test_mses at epoch21: [0.49321936 0.14876488 0.09853799 0.25212135 0.12308417 0.03170254
 0.0393147  0.045086   0.03413023 0.00323073]
test_maes at epoch21: [0.48625918 0.195937   0.19525298 0.23159611 0.16692109 0.11154907
 0.0988654  0.10748766 0.12799945 0.02397297]

train_loss at epoch22: 0.0008127795591791894
train_mses at epoch22: [0.11773671 0.01974525 0.03100866 0.02950286 0.01684004 0.02020578
 0.02087889 0.02101024 0.02303553 0.00623699]
train_maes at epoch22: [0.25509332 0.09639044 0.12100266 0.10477753 0.08865389 0.09803056
 0.09688516 0.09795908 0.09892534 0.0537588 ]
test_loss at epoch22: 0.002648688288048861
test_mses at epoch22: [0.4397009  0.1112073  0.08418131 0.17965204 0.09134096 0.03233721
 0.02914252 0.02538038 0.03022594 0.003725  ]
test_maes at epoch22: [0.4433431  0.16000469 0.1664804  0.17103861 0.14332687 0.12889601
 0.10553704 0.0881605  0.1267745  0.02314583]

train_loss at epoch23: 0.0007961936096878762
train_mses at epoch23: [0.12434435 0.02042727 0.02815348 0.02113172 0.02272286 0.01999912
 0.01985768 0.02209818 0.02245649 0.00550595]
train_maes at epoch23: [0.26584193 0.09707548 0.11614568 0.09880178 0.0915153  0.09299927
 0.09563579 0.10081101 0.09784441 0.05369316]
test_loss at epoch23: 0.002669161780083433
test_mses at epoch23: [0.31614535 0.08885494 0.12094194 0.20546104 0.07676441 0.02542661
 0.02661846 0.02647248 0.02777465 0.0035698 ]
test_maes at epoch23: [0.36226597 0.14168751 0.25123723 0.20725255 0.14336256 0.10409447
 0.09654009 0.09245685 0.12126916 0.03546186]

train_loss at epoch24: 0.0007482751213172649
train_mses at epoch24: [0.1120328  0.0172038  0.02179413 0.01972958 0.01585876 0.022489
 0.02616635 0.02049974 0.02296498 0.00517092]
train_maes at epoch24: [0.25603623 0.09109595 0.10903612 0.09360049 0.08216592 0.09843603
 0.10132416 0.09610495 0.09722154 0.05363449]
test_loss at epoch24: 0.0026664710445131394
test_mses at epoch24: [0.47547062 0.13696987 0.07335234 0.17981145 0.07167401 0.02078809
 0.02846355 0.03613178 0.036564   0.0025451 ]
test_maes at epoch24: [0.47097702 0.18124758 0.15160026 0.1598222  0.13029518 0.08667781
 0.0981665  0.10986723 0.1413535  0.01992306]

train_loss at epoch25: 0.0006975440110297914
train_mses at epoch25: [0.10073469 0.01778754 0.0227507  0.0195315  0.02052998 0.01581908
 0.01684966 0.02254162 0.01965355 0.00524638]
train_maes at epoch25: [0.23776581 0.09077304 0.10569958 0.09623375 0.08950009 0.08655164
 0.08649306 0.1053749  0.09793509 0.05227774]
test_loss at epoch25: 0.0028296104969179376
test_mses at epoch25: [0.36661731 0.12706527 0.08138543 0.20611673 0.07185547 0.02564957
 0.04313858 0.04109192 0.03852721 0.00306599]
test_maes at epoch25: [0.39858869 0.16792795 0.1784883  0.18219046 0.12513314 0.10390438
 0.10705556 0.10314319 0.14420046 0.02693868]

train_loss at epoch26: 0.0007144445137299122
train_mses at epoch26: [0.09351417 0.02209043 0.02545578 0.02332793 0.01458204 0.01566886
 0.01881408 0.01991395 0.02072658 0.00545543]
train_maes at epoch26: [0.22962331 0.09749403 0.11273122 0.10459548 0.081613   0.08618854
 0.09431557 0.09950375 0.09344726 0.0524238 ]
test_loss at epoch26: 0.0027932980395060903
test_mses at epoch26: [0.29726092 0.12026437 0.11688395 0.20464727 0.06262821 0.02444269
 0.03870929 0.03228651 0.03214853 0.00232147]
test_maes at epoch26: [0.35470799 0.18444264 0.23656641 0.18711643 0.11934037 0.08965185
 0.09608394 0.09513179 0.13362846 0.02070251]

train_loss at epoch27: 0.0007418946818785465
train_mses at epoch27: [0.12297737 0.02585842 0.02677222 0.02291732 0.01770272 0.02063209
 0.01619677 0.01804184 0.01456005 0.00506062]
train_maes at epoch27: [0.26760499 0.09824191 0.11397316 0.10005684 0.08557365 0.09148284
 0.08412174 0.09168767 0.08490961 0.05287487]
test_loss at epoch27: 0.0028086921378494578
test_mses at epoch27: [0.31580096 0.09627526 0.12731047 0.23711898 0.05825442 0.02849055
 0.02768482 0.02491113 0.03175709 0.00222483]
test_maes at epoch27: [0.3499626  0.14412986 0.26050158 0.22381713 0.12582204 0.11254828
 0.08470301 0.09019824 0.11295022 0.0250798 ]

train_loss at epoch28: 0.0007340715563994773
train_mses at epoch28: [0.12038938 0.01700394 0.02665459 0.01881487 0.01991384 0.01977633
 0.02071112 0.02216588 0.01752256 0.00541113]
train_maes at epoch28: [0.26160626 0.09243914 0.1082892  0.09643344 0.08426971 0.08703923
 0.09334653 0.10620007 0.08635953 0.05201496]
test_loss at epoch28: 0.002769287112862506
test_mses at epoch28: [0.31492028 0.14187431 0.11296753 0.19333338 0.06566756 0.02448577
 0.03683767 0.02430819 0.02462951 0.00225748]
test_maes at epoch28: [0.36430991 0.1905629  0.24515857 0.1831182  0.12301031 0.09640806
 0.09932417 0.08446289 0.10942372 0.02377449]

train_loss at epoch29: 0.0007065034570529105
train_mses at epoch29: [0.10828368 0.02205654 0.02459519 0.02105083 0.02035552 0.016102
 0.01419928 0.0186903  0.02052313 0.00554083]
train_maes at epoch29: [0.25463476 0.09845419 0.10343778 0.10101828 0.08509847 0.08444053
 0.08184938 0.09678335 0.08837014 0.05342435]
test_loss at epoch29: 0.0025201568934828675
test_mses at epoch29: [0.26468471 0.11260722 0.10699428 0.18308522 0.05517336 0.02973108
 0.03386726 0.02279809 0.02533667 0.0026915 ]
test_maes at epoch29: [0.32834333 0.18022558 0.2298661  0.16479476 0.11644911 0.11325226
 0.09210798 0.08315283 0.11627993 0.03223541]

train_loss at epoch30: 0.0006613258926316778
train_mses at epoch30: [0.10077642 0.0182124  0.02100817 0.02431005 0.01520322 0.01649356
 0.01676172 0.01879197 0.01625808 0.00503378]
train_maes at epoch30: [0.23794303 0.08997896 0.09937199 0.10343497 0.07804699 0.08762182
 0.08410179 0.09581586 0.08742245 0.05115211]
test_loss at epoch30: 0.0026940982352862968
test_mses at epoch30: [0.40329694 0.12782772 0.08424586 0.18951049 0.08151268 0.02946446
 0.02801171 0.03312417 0.02652858 0.00248609]
test_maes at epoch30: [0.41780079 0.16630193 0.15275735 0.17809589 0.12867979 0.10683961
 0.08761086 0.090688   0.11188715 0.01867529]

train_loss at epoch31: 0.0006558061735902695
train_mses at epoch31: [0.09319265 0.01345796 0.01923395 0.02235586 0.01528701 0.01621289
 0.01777914 0.02078767 0.02258415 0.00465528]
train_maes at epoch31: [0.22794765 0.08018943 0.09252452 0.10235007 0.0829502  0.08301183
 0.08331801 0.0958303  0.08712161 0.05000815]
test_loss at epoch31: 0.0030217264759096695
test_mses at epoch31: [0.45988962 0.13341296 0.08961409 0.21547043 0.09423786 0.03484224
 0.04042928 0.03282234 0.02992312 0.00332251]
test_maes at epoch31: [0.44730286 0.16557235 0.19657078 0.17923366 0.15926996 0.13357791
 0.09800048 0.09290989 0.1248245  0.02231414]

train_loss at epoch32: 0.0006583355426629808
train_mses at epoch32: [0.11327622 0.02386617 0.01903688 0.02057063 0.01514395 0.01513901
 0.01654297 0.01431682 0.01959512 0.00461137]
train_maes at epoch32: [0.25192874 0.10133494 0.10078169 0.09220562 0.07850857 0.08239331
 0.08472655 0.08459946 0.0850801  0.04810097]
test_loss at epoch32: 0.0029907439775923466
test_mses at epoch32: [0.45479567 0.11801483 0.0861454  0.25836001 0.08932276 0.01992559
 0.03333416 0.03637911 0.02262184 0.0019077 ]
test_maes at epoch32: [0.4474835  0.16554028 0.18073432 0.22017691 0.13095572 0.09144523
 0.08707323 0.09456136 0.10219216 0.01958716]

train_loss at epoch33: 0.0006794179690645096
train_mses at epoch33: [0.10215554 0.0250175  0.02375835 0.02206379 0.01791778 0.01619389
 0.01577527 0.01937333 0.01404549 0.00359981]
train_maes at epoch33: [0.23711008 0.09603102 0.10073436 0.09747424 0.08241023 0.07904633
 0.08232302 0.0894216  0.07709024 0.04286495]
test_loss at epoch33: 0.0027920136584880504
test_mses at epoch33: [0.39191216 0.13702794 0.08662369 0.23635308 0.05462977 0.02630939
 0.03592564 0.02125936 0.0210421  0.00224716]
test_maes at epoch33: [0.40434226 0.16617139 0.19001302 0.19485171 0.1161389  0.09783482
 0.09003696 0.0809492  0.0926686  0.01936326]

train_loss at epoch34: 0.0006336360099784871
train_mses at epoch34: [0.09870913 0.01849217 0.02014055 0.01785267 0.01762933 0.0167297
 0.01750656 0.01604318 0.01768631 0.00379438]
train_maes at epoch34: [0.22811129 0.08853762 0.09706632 0.0860104  0.08098925 0.08327475
 0.08365686 0.08724192 0.08587271 0.04385868]
test_loss at epoch34: 0.0025943880504433144
test_mses at epoch34: [0.3541979  0.10514285 0.08252568 0.19905721 0.0587681  0.02590098
 0.03502533 0.03642419 0.03613255 0.00307996]
test_maes at epoch34: [0.37938471 0.14602982 0.17795394 0.16795677 0.11188193 0.0785289
 0.08673574 0.09273715 0.13963436 0.02184373]

train_loss at epoch35: 0.0006094148699590501
train_mses at epoch35: [0.09444908 0.01880998 0.01863946 0.01856453 0.01504929 0.01541787
 0.01279814 0.02042182 0.01597242 0.00358666]
train_maes at epoch35: [0.2264667  0.08516961 0.09194542 0.08786857 0.07646873 0.07751068
 0.07827005 0.09551024 0.08120336 0.04251944]
test_loss at epoch35: 0.002497423103356615
test_mses at epoch35: [0.33229274 0.10214795 0.08400511 0.19470528 0.0692853  0.02853065
 0.03123973 0.0277653  0.02044473 0.00252494]
test_maes at epoch35: [0.37000125 0.13788137 0.17447611 0.16122483 0.11937808 0.1166697
 0.08187601 0.08160345 0.09615457 0.01973236]

train_loss at epoch36: 0.0005437772760682918
train_mses at epoch36: [0.07759144 0.01886105 0.01786566 0.01839432 0.01097779 0.0155427
 0.01295525 0.01479456 0.01218691 0.0034716 ]
train_maes at epoch36: [0.21312772 0.08503614 0.09491876 0.09179656 0.07108016 0.08140515
 0.07272793 0.08444678 0.07767271 0.04189467]
test_loss at epoch36: 0.002844450287917193
test_mses at epoch36: [0.47735076 0.11220101 0.09119704 0.23134998 0.08286688 0.02788549
 0.03393709 0.02487889 0.02238388 0.00228169]
test_maes at epoch36: [0.47437944 0.1459244  0.16201764 0.19937721 0.13202062 0.11611392
 0.08777109 0.08089572 0.10356449 0.02070135]

train_loss at epoch37: 0.0005292329580542889
train_mses at epoch37: [0.08388189 0.01341268 0.0208132  0.01521007 0.01345398 0.01125667
 0.01350002 0.01682341 0.01325404 0.00363975]
train_maes at epoch37: [0.21880417 0.07664616 0.0979982  0.08923828 0.06997933 0.07518451
 0.07757861 0.08565471 0.0739567  0.04468887]
test_loss at epoch37: 0.002848403910769427
test_mses at epoch37: [0.47406528 0.08889231 0.0898125  0.25405639 0.08175061 0.02087142
 0.03256682 0.02797876 0.03106855 0.00191802]
test_maes at epoch37: [0.45568639 0.11971058 0.16119806 0.21201336 0.12740577 0.08590638
 0.08404552 0.08631727 0.1317308  0.01893866]

train_loss at epoch38: 0.0005746646586092228
train_mses at epoch38: [0.09986954 0.01387658 0.02122149 0.015576   0.01742302 0.01210067
 0.01808989 0.01309203 0.01570784 0.00390879]
train_maes at epoch38: [0.24391654 0.07502621 0.0967709  0.08915974 0.07646067 0.07020639
 0.0850579  0.0780667  0.08034658 0.0447667 ]
test_loss at epoch38: 0.002656590365903809
test_mses at epoch38: [0.38788203 0.10773577 0.08677046 0.21315111 0.06630887 0.02506441
 0.04091184 0.02804498 0.02188448 0.00186128]
test_maes at epoch38: [0.40283528 0.14277763 0.17650243 0.17591572 0.11558201 0.10637958
 0.08936255 0.08290537 0.1033566  0.01930464]

train_loss at epoch39: 0.0005881110879969089
train_mses at epoch39: [0.08259396 0.02353162 0.02020614 0.01903664 0.01395518 0.01259667
 0.0119027  0.01569977 0.01608388 0.00363167]
train_maes at epoch39: [0.21685562 0.09180365 0.09782804 0.08783233 0.07255596 0.07232616
 0.07728415 0.08756378 0.08164643 0.04364392]
test_loss at epoch39: 0.0029053543317825235
test_mses at epoch39: [0.30812778 0.09770517 0.11017616 0.27407274 0.05541673 0.01966186
 0.04637711 0.03066162 0.02008642 0.00186249]
test_maes at epoch39: [0.35336732 0.12733245 0.2260216  0.21658779 0.10371658 0.09477784
 0.11106784 0.09543315 0.09330221 0.0197013 ]

train_loss at epoch40: 0.0005650428421002753
train_mses at epoch40: [0.08412307 0.01822057 0.02152633 0.0231688  0.0109699  0.01239482
 0.01710373 0.01171292 0.01134717 0.00394393]
train_maes at epoch40: [0.21385832 0.0849632  0.09787962 0.09291506 0.07104142 0.07294677
 0.08387615 0.07572087 0.0724263  0.04455082]
test_loss at epoch40: 0.002623212123487858
test_mses at epoch40: [0.31979332 0.09801838 0.07827133 0.22237115 0.06101386 0.02573922
 0.04632312 0.03033    0.02611976 0.00231714]
test_maes at epoch40: [0.35450583 0.13924953 0.17689208 0.16964259 0.10915231 0.1048885
 0.09731089 0.08904574 0.08977612 0.01899648]

train_loss at epoch41: 0.0005718251511930151
train_mses at epoch41: [0.08847821 0.02072525 0.0221132  0.01501058 0.01246857 0.01158196
 0.01577236 0.01176354 0.01902784 0.00315781]
train_maes at epoch41: [0.21350885 0.08559732 0.09927197 0.08461024 0.0685712  0.0748971
 0.07948682 0.07514453 0.08202131 0.04086648]
test_loss at epoch41: 0.002355090221588282
test_mses at epoch41: [0.24090798 0.08020233 0.11354253 0.21099857 0.04560702 0.01859245
 0.02525932 0.02090611 0.01643882 0.00175837]
test_maes at epoch41: [0.30260125 0.11305729 0.22990829 0.19047464 0.10567527 0.08663793
 0.0798519  0.07465896 0.08495564 0.01546067]

train_loss at epoch42: 0.000567220474415003
train_mses at epoch42: [0.07013638 0.01801109 0.02134551 0.0154307  0.01181352 0.01640488
 0.01270292 0.01500427 0.01851408 0.00295467]
train_maes at epoch42: [0.19441733 0.08440884 0.09984309 0.08160075 0.06799367 0.08035912
 0.07454662 0.08045748 0.08432911 0.0393174 ]
test_loss at epoch42: 0.0025922766113851935
test_mses at epoch42: [0.31018789 0.1091765  0.08053526 0.21132552 0.05576738 0.02986914
 0.04305954 0.02978926 0.02331228 0.0023904 ]
test_maes at epoch42: [0.35504237 0.15581006 0.18791428 0.16741642 0.10513901 0.11253348
 0.09244369 0.08090381 0.08647616 0.02034503]

train_loss at epoch43: 0.0005213633060772369
train_mses at epoch43: [0.08738116 0.0115244  0.0169013  0.02156269 0.01309637 0.01290194
 0.00957004 0.01540442 0.0155009  0.0030684 ]
train_maes at epoch43: [0.21301283 0.07205417 0.08796742 0.09486251 0.07052005 0.07490637
 0.06700777 0.07991832 0.07491169 0.04013678]
test_loss at epoch43: 0.002631254109771962
test_mses at epoch43: [0.30898943 0.1003183  0.08726647 0.25609065 0.03752763 0.02067683
 0.03506104 0.03271725 0.0186296  0.00171497]
test_maes at epoch43: [0.34409288 0.13035516 0.1921264  0.21081271 0.09234558 0.08998777
 0.08594709 0.08561331 0.0871361  0.02007211]

train_loss at epoch44: 0.0005241976456439242
train_mses at epoch44: [0.07901589 0.0116539  0.01579137 0.02053152 0.0127373  0.01281599
 0.0139713  0.01551322 0.01445764 0.00341461]
train_maes at epoch44: [0.21836064 0.07226281 0.08838818 0.09196793 0.07226921 0.07259231
 0.07579245 0.08359921 0.07945423 0.04154516]
test_loss at epoch44: 0.0026656831991165243
test_mses at epoch44: [0.34335798 0.09523548 0.08065738 0.21909341 0.08967393 0.02161858
 0.03161249 0.03538707 0.02707715 0.00157256]
test_maes at epoch44: [0.37263727 0.12917725 0.17569825 0.18829751 0.13608649 0.09144658
 0.07382919 0.08815615 0.12051363 0.0178927 ]

train_loss at epoch45: 0.0005077274040655887
train_mses at epoch45: [0.07900476 0.0117261  0.01277272 0.01482053 0.01658055 0.01134273
 0.01239463 0.01480483 0.01894624 0.00314651]
train_maes at epoch45: [0.20880516 0.07249806 0.07928908 0.07740045 0.07597962 0.07002724
 0.07087766 0.0776875  0.07914665 0.04001991]
test_loss at epoch45: 0.0025042313349215276
test_mses at epoch45: [0.35532779 0.09925661 0.07367451 0.18956679 0.06357833 0.02694196
 0.04617983 0.03429759 0.02642724 0.00191896]
test_maes at epoch45: [0.38364281 0.13992362 0.14687659 0.16016099 0.11246046 0.10797809
 0.087278   0.08584015 0.12102129 0.01770307]

train_loss at epoch46: 0.0005281341559392341
train_mses at epoch46: [0.07658112 0.01109044 0.01360543 0.01971143 0.01644472 0.01255463
 0.01677007 0.01386447 0.01557768 0.00269324]
train_maes at epoch46: [0.19996573 0.07114429 0.0799264  0.0901939  0.07482096 0.07142936
 0.0781534  0.07804442 0.0782401  0.03621473]
test_loss at epoch46: 0.002495223376899958
test_mses at epoch46: [0.38198431 0.09789084 0.0757274  0.20642264 0.08229718 0.02448703
 0.02636992 0.02482033 0.01580713 0.002329  ]
test_maes at epoch46: [0.40759336 0.12590494 0.15431834 0.15478639 0.11999681 0.08626973
 0.07415868 0.07751082 0.08268876 0.01654127]

train_loss at epoch47: 0.0005514183695963089
train_mses at epoch47: [0.07537921 0.01577621 0.02138306 0.01554405 0.01397854 0.01698586
 0.01249937 0.01315869 0.01608771 0.00277915]
train_maes at epoch47: [0.20111623 0.07364976 0.09687983 0.07708496 0.07531517 0.07804053
 0.07212528 0.08092877 0.07695834 0.03795   ]
test_loss at epoch47: 0.002650223535663904
test_mses at epoch47: [0.28640053 0.08580103 0.12123496 0.21404963 0.06134925 0.03225067
 0.03695302 0.0298675  0.01768229 0.00229589]
test_maes at epoch47: [0.33943671 0.12115873 0.23943982 0.1857337  0.10746286 0.1285655
 0.08278862 0.08241145 0.0704557  0.01754465]

train_loss at epoch48: 0.0004976013061055478
train_mses at epoch48: [0.06996323 0.01426389 0.01937308 0.01266335 0.01005927 0.01723566
 0.0123897  0.01344315 0.01183979 0.00238438]
train_maes at epoch48: [0.1979715  0.0750494  0.0897698  0.07566111 0.06315495 0.08059185
 0.07202687 0.07983112 0.07016109 0.03712317]
test_loss at epoch48: 0.00225887361200566
test_mses at epoch48: [0.23141744 0.09430636 0.08802786 0.18158344 0.05330742 0.01629211
 0.03582469 0.02550393 0.01595974 0.00204811]
test_maes at epoch48: [0.30184359 0.13390902 0.20314173 0.14504603 0.10232464 0.07434392
 0.09033091 0.07704946 0.07950871 0.01767599]

train_loss at epoch49: 0.00048654730372289394
train_mses at epoch49: [0.07808581 0.01160748 0.01579551 0.01873392 0.01320354 0.01492189
 0.01073727 0.01214318 0.01095203 0.00296521]
train_maes at epoch49: [0.21408603 0.07218624 0.08140383 0.08538164 0.0724591  0.07713722
 0.06830506 0.07483097 0.06599477 0.03771401]
test_loss at epoch49: 0.002538287368147297
test_mses at epoch49: [0.33204831 0.12004465 0.06658989 0.20941738 0.05997024 0.02455787
 0.04331531 0.02545058 0.01690772 0.00229034]
test_maes at epoch49: [0.36686085 0.14619706 0.16157033 0.16404346 0.10576253 0.10660444
 0.08771893 0.07654673 0.07278332 0.01480492]

