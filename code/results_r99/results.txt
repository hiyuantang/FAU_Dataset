train_loss at epoch0: 0.03909468974756158
train_mses at epoch0: [3.76420938 1.15223548 1.15251571 1.14306795 1.1639     1.00479932
 1.15090104 1.14780472 1.13964759 0.02799618]
train_maes at epoch0: [1.5237897  0.3923099  0.62795137 0.37999182 0.37373481 0.59041266
 0.37216868 0.38110879 0.38408138 0.11043977]
test_loss at epoch0: 0.03878617934558703
test_mses at epoch0: [3.5968166  1.13802695 1.14656895 1.12067684 1.13847662 0.98829775
 1.12920836 1.12838579 1.12852923 0.02227367]
test_maes at epoch0: [1.52881471 0.39756458 0.66673472 0.43469511 0.42227997 0.63534046
 0.41011897 0.46370892 0.41125972 0.05292015]

train_loss at epoch1: 0.03797642303549725
train_mses at epoch1: [3.3876647  1.12246031 1.1513033  1.08170844 1.14023781 0.89909171
 1.12196512 1.10280354 1.11871992 0.02550514]
train_maes at epoch1: [1.51489307 0.44371703 0.6667704  0.46196492 0.426886   0.55928347
 0.40964289 0.44509534 0.42196822 0.08980207]
test_loss at epoch1: 0.03751620909442072
test_mses at epoch1: [3.20741968 1.09283594 1.12957755 1.06772482 1.10684092 0.95497041
 1.09924842 1.10471089 1.09245428 0.02208169]
test_maes at epoch1: [1.54106892 0.4588128  0.67575056 0.53667307 0.51156881 0.56348747
 0.46156852 0.55013906 0.47007056 0.03928851]

train_loss at epoch2: 0.0367623807295509
train_mses at epoch2: [3.04670103 1.06911855 1.08821234 1.01106231 1.10562046 0.84249596
 1.07043855 1.06257663 1.07854627 0.02451661]
train_maes at epoch2: [1.51732893 0.49783148 0.66282635 0.54817154 0.49289558 0.50043376
 0.44868837 0.51509447 0.4801005  0.07408385]
test_loss at epoch2: 0.0367644506952037
test_mses at epoch2: [3.08868941 1.05174909 1.14180713 1.04800502 1.09496438 0.90340934
 1.0674641  1.10238992 1.06711449 0.02399387]
test_maes at epoch2: [1.56943429 0.52395512 0.73912471 0.63535768 0.60310504 0.5461093
 0.52291172 0.65168124 0.52714945 0.05060466]

train_loss at epoch3: 0.03456626832485199
train_mses at epoch3: [2.7468726  1.00734092 1.04056291 0.99514641 1.08053374 0.72351943
 1.04623936 1.02175186 1.04021356 0.02455279]
train_maes at epoch3: [1.50209887 0.55190728 0.69057711 0.6539714  0.56920949 0.46538857
 0.49098836 0.58054766 0.5507544  0.07647322]
test_loss at epoch3: 0.03590078846268032
test_mses at epoch3: [3.01653267 1.00358468 1.132057   1.02352917 1.09867572 0.81971117
 1.04631044 1.10971844 1.04446139 0.02801729]
test_maes at epoch3: [1.57184771 0.53038698 0.73848045 0.67399972 0.64973088 0.51248499
 0.57879504 0.69865307 0.58275364 0.07984782]

train_loss at epoch4: 0.03263725275578706
train_mses at epoch4: [2.57154933 0.95767115 0.97974876 0.90854107 1.0681698  0.6266232
 1.00202671 0.98342681 1.00630761 0.02444304]
train_maes at epoch4: [1.44763129 0.52943068 0.66142371 0.63655424 0.58274475 0.40331116
 0.52930727 0.59308447 0.59785227 0.07911045]
test_loss at epoch4: 0.03445129226083341
test_mses at epoch4: [2.69219571 0.94007937 1.10753211 0.95715817 1.09409253 0.70816002
 1.03081045 1.11349737 1.01987806 0.0273023 ]
test_maes at epoch4: [1.46166936 0.46991565 0.70205743 0.57145294 0.63629202 0.48225595
 0.59509946 0.70518554 0.59871407 0.07386637]

train_loss at epoch5: 0.03023762417876202
train_mses at epoch5: [2.32287771 0.8159724  1.00808429 0.8100253  1.06811501 0.42253057
 0.96940984 0.96847482 0.97749591 0.0306975 ]
train_maes at epoch5: [1.33905545 0.46945142 0.64338989 0.53899703 0.57970376 0.36567437
 0.54371868 0.58726506 0.60798355 0.09690398]
test_loss at epoch5: 0.032946772549463356
test_mses at epoch5: [2.50029068 0.85521808 1.11376375 0.89305441 1.08121389 0.61886548
 0.99350595 1.1101984  0.97068996 0.02731622]
test_maes at epoch5: [1.39346202 0.42657557 0.71539243 0.4863686  0.62788266 0.49710658
 0.61779566 0.72291068 0.5821273  0.07161469]

train_loss at epoch6: 0.02658631425836812
train_mses at epoch6: [2.04863185 0.77622719 0.86120689 0.50742791 1.03708063 0.36776667
 0.89115488 0.85780681 0.85218813 0.03827932]
train_maes at epoch6: [1.24598824 0.47434312 0.60380837 0.44988641 0.56464438 0.37946816
 0.52869166 0.52614962 0.54289233 0.12835207]
test_loss at epoch6: 0.030611568171045055
test_mses at epoch6: [2.24648825 0.74179914 1.08294907 0.80276988 1.05562132 0.53577502
 0.92916919 1.05356461 0.90191492 0.02829393]
test_maes at epoch6: [1.29756889 0.40454111 0.63430015 0.43597229 0.60504698 0.48455359
 0.58836917 0.67532033 0.53227423 0.08021672]

train_loss at epoch7: 0.025045591851939327
train_mses at epoch7: [1.89998947 0.65193303 0.8781179  0.57894805 1.00795004 0.31714685
 0.75398578 0.81048112 0.75603878 0.04260078]
train_maes at epoch7: [1.19643441 0.44880408 0.56242255 0.51466528 0.5235318  0.37658944
 0.48974834 0.47370733 0.47955721 0.14320889]
test_loss at epoch7: 0.028166504657786827
test_mses at epoch7: [2.10241017 0.62536131 1.09069639 0.65972046 1.02470898 0.52418054
 0.81571782 0.97726139 0.81542655 0.03276546]
test_maes at epoch7: [1.22419455 0.39538741 0.64273802 0.41349214 0.56438665 0.5425541
 0.53028772 0.65075897 0.45437607 0.10005164]

train_loss at epoch8: 0.021266208070775738
train_mses at epoch8: [1.68383484 0.44918455 0.77250768 0.38378518 0.95179258 0.38174292
 0.70317891 0.68800084 0.61292567 0.05696106]
train_maes at epoch8: [1.03910071 0.41672861 0.54240243 0.43962346 0.491019   0.45600405
 0.48959285 0.4567611  0.43405068 0.16598964]
test_loss at epoch8: 0.026695927847986637
test_mses at epoch8: [1.88868324 0.5518239  1.07634    0.63858179 0.99983408 0.53122819
 0.71561718 0.9663692  0.73129906 0.03456663]
test_maes at epoch8: [1.05438329 0.3875236  0.6279613  0.41924238 0.56481533 0.5774848
 0.47166668 0.70947483 0.4336297  0.1064133 ]

train_loss at epoch9: 0.019974722162536953
train_mses at epoch9: [1.6478914  0.40655364 0.76589531 0.30927663 0.87041494 0.40090191
 0.60983542 0.63810073 0.57425057 0.0628838 ]
train_maes at epoch9: [0.97040531 0.3846717  0.52052712 0.35872739 0.46054023 0.49470642
 0.46834943 0.48232683 0.44687537 0.17752475]
test_loss at epoch9: 0.025165679014247398
test_mses at epoch9: [1.79295554 0.45517519 1.04212137 0.66906279 0.97089018 0.50685388
 0.61627102 0.94016596 0.66020509 0.03725889]
test_maes at epoch9: [1.0064228  0.40098771 0.63632682 0.49223826 0.59995862 0.57079537
 0.4229867  0.75410361 0.40924632 0.11706828]

train_loss at epoch10: 0.01683281588813533
train_mses at epoch10: [1.54336252 0.25440234 0.63168107 0.2917962  0.79670321 0.28924872
 0.4364859  0.54645747 0.54579435 0.06157531]
train_maes at epoch10: [0.93788031 0.3651442  0.51423135 0.38253183 0.48267612 0.43053292
 0.41369425 0.47562331 0.43361163 0.17891341]
test_loss at epoch10: 0.022342240680818973
test_mses at epoch10: [1.68832927 0.40938114 0.98359207 0.46718757 0.92970052 0.44943165
 0.54979569 0.79778397 0.61205244 0.03592426]
test_maes at epoch10: [0.96409917 0.40022417 0.61681093 0.37797572 0.60330934 0.52552075
 0.36556636 0.67755838 0.40823446 0.1154257 ]

train_loss at epoch11: 0.014766145335591358
train_mses at epoch11: [1.47788159 0.24469785 0.60553694 0.21879912 0.80170845 0.18920313
 0.40427575 0.42226105 0.4287296  0.06733304]
train_maes at epoch11: [0.9403957  0.36936164 0.48877867 0.2944102  0.50381998 0.36560683
 0.40442502 0.4086479  0.42309152 0.19669495]
test_loss at epoch11: 0.021815292213274086
test_mses at epoch11: [2.19564099 0.45287632 0.92822994 0.52067286 0.89526426 0.41457159
 0.50859902 0.79839793 0.52074275 0.02808585]
test_maes at epoch11: [1.10159517 0.40214507 0.54500745 0.37825808 0.57749378 0.48934115
 0.38763098 0.69496008 0.43710123 0.09142239]

train_loss at epoch12: 0.011953267066375069
train_mses at epoch12: [1.39814889 0.2291537  0.44774077 0.1933066  0.68666217 0.21388776
 0.30653985 0.30390838 0.29647681 0.04935887]
train_maes at epoch12: [0.8945753  0.36584984 0.43605875 0.2601784  0.45197966 0.35485055
 0.39576146 0.36705271 0.37926664 0.14824789]
test_loss at epoch12: 0.020301829213681427
test_mses at epoch12: [1.67161603 0.36064173 0.85505462 0.50275182 0.83791527 0.53595128
 0.47474091 0.67917686 0.48991816 0.02762455]
test_maes at epoch12: [0.91748151 0.34727266 0.52224838 0.35688133 0.53667414 0.56215258
 0.33107909 0.59879654 0.39041638 0.09296304]

train_loss at epoch13: 0.011457454899083013
train_mses at epoch13: [1.12741325 0.27048221 0.40894665 0.23326625 0.56849106 0.22227837
 0.2871375  0.28785168 0.32655366 0.04493579]
train_maes at epoch13: [0.80955508 0.35592722 0.39326947 0.26565273 0.41232069 0.35900766
 0.34863011 0.32887749 0.36949371 0.15177682]
test_loss at epoch13: 0.01895621904860372
test_mses at epoch13: [1.39775896 0.33608614 0.86505818 0.53520812 0.77825102 0.34260072
 0.44254819 0.71601858 0.42759753 0.02575847]
test_maes at epoch13: [0.82970622 0.35495543 0.47241781 0.38463277 0.56455958 0.42993729
 0.33847722 0.64835975 0.38236148 0.0919754 ]

train_loss at epoch14: 0.010319576963134434
train_mses at epoch14: [1.15163152 0.15915213 0.32075614 0.20997844 0.50575188 0.16961366
 0.26408511 0.34146809 0.34828558 0.03623677]
train_maes at epoch14: [0.82416048 0.30617658 0.36055043 0.28235742 0.40291606 0.31150084
 0.345429   0.34842677 0.41004841 0.14426635]
test_loss at epoch14: 0.01899765626243923
test_mses at epoch14: [1.4656795  0.33914018 0.75165428 0.6722257  0.74575445 0.33518302
 0.45167119 0.70365603 0.4606909  0.0242498 ]
test_maes at epoch14: [0.84141743 0.31860465 0.46205227 0.46953312 0.56432847 0.42958288
 0.33601255 0.632733   0.37803114 0.08925249]

train_loss at epoch15: 0.008948666408010151
train_mses at epoch15: [1.04832802 0.11872865 0.32540575 0.1585205  0.44896719 0.1074943
 0.27546361 0.27196265 0.26166586 0.03276709]
train_maes at epoch15: [0.74901076 0.22600461 0.3734901  0.25314269 0.34019687 0.25307384
 0.31003704 0.33621939 0.35851379 0.1318058 ]
test_loss at epoch15: 0.01777107462934826
test_mses at epoch15: [1.39373322 0.28326037 0.73993152 0.63826494 0.68335377 0.31063068
 0.45737506 0.5873455  0.46107136 0.02546197]
test_maes at epoch15: [0.82756074 0.28410147 0.43671675 0.4189097  0.52276389 0.41115363
 0.32719924 0.49240486 0.37785785 0.08787527]

train_loss at epoch16: 0.007833102960949358
train_mses at epoch16: [0.81791622 0.08830679 0.21934341 0.21680169 0.40673938 0.13018992
 0.20892589 0.23219658 0.25661565 0.04616388]
train_maes at epoch16: [0.66764606 0.20050942 0.30805861 0.26934384 0.34264066 0.27823889
 0.2700355  0.33145481 0.33876673 0.15614437]
test_loss at epoch16: 0.01790314945189849
test_mses at epoch16: [1.19408541 0.25779684 0.76179543 0.52522578 0.67977283 0.30129956
 0.42917962 0.85024874 0.41282207 0.03068507]
test_maes at epoch16: [0.83114444 0.30946628 0.43554344 0.36697724 0.617103   0.41208086
 0.33424515 0.77126326 0.34771222 0.10557483]

train_loss at epoch17: 0.006977886447440023
train_mses at epoch17: [0.77299048 0.10001687 0.23927244 0.18844757 0.31672702 0.06923887
 0.17253038 0.23328472 0.24506047 0.03298972]
train_maes at epoch17: [0.66196424 0.25348279 0.31382797 0.263835   0.33374899 0.20089492
 0.26310314 0.29204705 0.33959341 0.13222677]
test_loss at epoch17: 0.017400944686454277
test_mses at epoch17: [1.20916775 0.23242587 0.6795411  0.63350637 0.65585836 0.30364501
 0.42470109 0.69610031 0.47484787 0.02636756]
test_maes at epoch17: [0.80441089 0.26422075 0.41083703 0.45333522 0.60571103 0.4213783
 0.34647823 0.64103518 0.37335289 0.09170176]

train_loss at epoch18: 0.006396143818679063
train_mses at epoch18: [0.80759504 0.05179938 0.23597363 0.16214557 0.30094792 0.06611714
 0.18221781 0.21244068 0.22624088 0.03108867]
train_maes at epoch18: [0.65947359 0.16185032 0.30548363 0.25557247 0.30722379 0.19803809
 0.24818981 0.2876586  0.3109332  0.13484827]
test_loss at epoch18: 0.015838702411755272
test_mses at epoch18: [1.26110631 0.21459821 0.69795847 0.47778476 0.58228866 0.2833668
 0.45742228 0.56133238 0.4264261  0.02556218]
test_maes at epoch18: [0.81074798 0.25534707 0.39050078 0.3357779  0.54757168 0.39837614
 0.33247488 0.43340045 0.34681673 0.0821863 ]

train_loss at epoch19: 0.005900404537501542
train_mses at epoch19: [0.63893432 0.06207816 0.18940072 0.15127505 0.25790511 0.06459447
 0.1402795  0.25932355 0.21795424 0.02601576]
train_maes at epoch19: [0.59130089 0.18484796 0.27097811 0.24437981 0.28555949 0.19369362
 0.19774163 0.31430216 0.28463351 0.11736555]
test_loss at epoch19: 0.015357357004414434
test_mses at epoch19: [1.33383443 0.20722587 0.64079009 0.52124457 0.53677168 0.24418351
 0.41038843 0.62513105 0.40120121 0.02290169]
test_maes at epoch19: [0.82374067 0.26480267 0.36960054 0.37190232 0.5308196  0.36564989
 0.33525081 0.55870453 0.33882688 0.07707844]

train_loss at epoch20: 0.005540083607901697
train_mses at epoch20: [0.74833834 0.05141706 0.17535945 0.1935603  0.17965003 0.04386206
 0.14643946 0.23640393 0.19393263 0.02759803]
train_maes at epoch20: [0.6048498  0.17125823 0.26687609 0.25294375 0.24733831 0.16518848
 0.21083764 0.28788943 0.27558476 0.13123354]
test_loss at epoch20: 0.01604566658320634
test_mses at epoch20: [1.33130463 0.18147637 0.67299701 0.61135129 0.54985795 0.23530588
 0.40215862 0.73844812 0.36938557 0.02235043]
test_maes at epoch20: [0.82432219 0.24103343 0.370226   0.43268129 0.56928319 0.35770015
 0.32374029 0.68867781 0.34590276 0.07641862]

train_loss at epoch21: 0.004860192699276883
train_mses at epoch21: [0.58288712 0.0879347  0.14527932 0.14686452 0.17487456 0.04844042
 0.1545125  0.18999946 0.13089985 0.03110901]
train_maes at epoch21: [0.55335171 0.21011521 0.25654484 0.24736488 0.27563916 0.16228311
 0.22226339 0.27196531 0.26792068 0.1294564 ]
test_loss at epoch21: 0.01670025418634
test_mses at epoch21: [1.32149022 0.19122124 0.81663539 0.63105219 0.59307051 0.24895636
 0.42299951 0.66406449 0.35430191 0.0235869 ]
test_maes at epoch21: [0.81475665 0.23250855 0.43433204 0.42135378 0.62194795 0.3703041
 0.32083283 0.61786506 0.3557163  0.07844419]

train_loss at epoch22: 0.004628729723069979
train_mses at epoch22: [0.54971538 0.06959367 0.1981739  0.1375121  0.15137345 0.04142302
 0.10698398 0.19356129 0.13913062 0.01923079]
train_maes at epoch22: [0.53918839 0.18409608 0.27232691 0.22896411 0.2461419  0.1485792
 0.19511524 0.27776713 0.26359366 0.10532851]
test_loss at epoch22: 0.01398785460902297
test_mses at epoch22: [1.15354854 0.20992125 0.71471485 0.40694    0.44913593 0.24128591
 0.36253268 0.53642811 0.35348549 0.02860417]
test_maes at epoch22: [0.79534293 0.23547663 0.37275687 0.29940409 0.51012809 0.35859197
 0.32112153 0.49716652 0.31593166 0.09759588]

train_loss at epoch23: 0.004669539792382199
train_mses at epoch23: [0.6038452  0.05898788 0.10923251 0.15238073 0.12313661 0.05811391
 0.1299883  0.2261546  0.1732686  0.02914871]
train_maes at epoch23: [0.58949713 0.17057437 0.22614143 0.22727729 0.21390544 0.17717864
 0.21641303 0.26351399 0.26997221 0.13368784]
test_loss at epoch23: 0.014525051674117212
test_mses at epoch23: [1.15116777 0.26514416 0.66485969 0.47098112 0.46514281 0.22246007
 0.4126258  0.57153396 0.33533373 0.02562324]
test_maes at epoch23: [0.78933026 0.26213433 0.34629465 0.31830374 0.5322735  0.34499292
 0.32155514 0.5459688  0.2992411  0.08723772]

train_loss at epoch24: 0.004638264927527179
train_mses at epoch24: [0.53599471 0.05357888 0.16484856 0.15709551 0.10719564 0.03115075
 0.17402943 0.20744576 0.12266653 0.02678467]
train_maes at epoch24: [0.54524597 0.15919771 0.25542654 0.24848147 0.21768859 0.13825282
 0.22622573 0.26000474 0.23387405 0.11945441]
test_loss at epoch24: 0.014256423096293989
test_mses at epoch24: [1.08820237 0.21514967 0.6513798  0.47679976 0.53756262 0.21513957
 0.39847814 0.58079895 0.27590048 0.02450448]
test_maes at epoch24: [0.77448848 0.24620375 0.34770722 0.34517146 0.60646229 0.34914859
 0.32325237 0.56167993 0.28277766 0.08323214]

train_loss at epoch25: 0.0038342006180597387
train_mses at epoch25: [0.45935304 0.05562094 0.13644714 0.11724204 0.08101377 0.07320017
 0.0792902  0.15942824 0.12567656 0.02718153]
train_maes at epoch25: [0.50756971 0.1597149  0.22636806 0.19903331 0.19301527 0.17536144
 0.17950906 0.25716799 0.24970814 0.11902491]
test_loss at epoch25: 0.01384757232406865
test_mses at epoch25: [1.10785749 0.20131661 0.77286169 0.42197886 0.39630024 0.20427703
 0.35039591 0.54058209 0.3575933  0.02684885]
test_maes at epoch25: [0.77830509 0.23772245 0.39998868 0.31706108 0.49068712 0.33063167
 0.31249342 0.44457858 0.31187421 0.08867649]

train_loss at epoch26: 0.004334850398742635
train_mses at epoch26: [0.53722724 0.0814049  0.13886379 0.13263959 0.09837622 0.0531626
 0.16494636 0.14994208 0.13892285 0.02455079]
train_maes at epoch26: [0.55163529 0.19359903 0.25045371 0.24280111 0.21451053 0.17978352
 0.22866147 0.21689976 0.23107389 0.11998467]
test_loss at epoch26: 0.014677091461160908
test_mses at epoch26: [1.139961   0.24767638 0.73239762 0.40350145 0.33947805 0.22346104
 0.40898436 0.6560525  0.43348979 0.02467204]
test_maes at epoch26: [0.77702928 0.22649146 0.36538504 0.30239922 0.41508425 0.33915804
 0.31582027 0.65775409 0.33395606 0.08289309]

train_loss at epoch27: 0.003132969869867615
train_mses at epoch27: [0.40448448 0.05110072 0.10698512 0.08075054 0.07909877 0.04798354
 0.10665912 0.14692857 0.06550592 0.02371259]
train_maes at epoch27: [0.46763378 0.14506491 0.20671089 0.18134743 0.19247748 0.14727706
 0.19316256 0.21994567 0.19213255 0.10280737]
test_loss at epoch27: 0.01457311925680741
test_mses at epoch27: [1.04300796 0.21131786 0.6207288  0.52170784 0.44002552 0.23619509
 0.41143661 0.66840548 0.32497232 0.02260702]
test_maes at epoch27: [0.7492859  0.22161278 0.34466177 0.37823217 0.53014155 0.35306573
 0.32158657 0.67616476 0.27466954 0.07629326]

train_loss at epoch28: 0.0032790920332722044
train_mses at epoch28: [0.41855678 0.05521351 0.08569878 0.08865786 0.13521856 0.07392571
 0.09429759 0.0808456  0.1125026  0.02142339]
train_maes at epoch28: [0.4797284  0.16617909 0.21114972 0.21947973 0.22391995 0.17765155
 0.19821217 0.1968939  0.20780359 0.10862375]
test_loss at epoch28: 0.014809537031080412
test_mses at epoch28: [1.13800391 0.19046828 0.7161405  0.60211911 0.50203209 0.2320946
 0.408649   0.58097521 0.25122217 0.02300911]
test_maes at epoch28: [0.76346058 0.2060432  0.35515163 0.40059051 0.594809   0.35109259
 0.31760994 0.56117704 0.26590196 0.07663296]

train_loss at epoch29: 0.0029613876958256183
train_mses at epoch29: [0.36975514 0.0456336  0.05378291 0.07203301 0.07384851 0.0598566
 0.12950223 0.13978664 0.07923192 0.02634173]
train_maes at epoch29: [0.45015079 0.14597142 0.16553964 0.16035507 0.18519518 0.17481622
 0.20300054 0.20885335 0.19405284 0.11968334]
test_loss at epoch29: 0.013760636196188305
test_mses at epoch29: [1.2116918  0.19036142 0.78578612 0.48173654 0.36946067 0.24125
 0.34879903 0.52770086 0.2760939  0.0230944 ]
test_maes at epoch29: [0.79057088 0.20351471 0.38545175 0.32323215 0.45423232 0.36055771
 0.32211996 0.49321467 0.25988077 0.07930967]

train_loss at epoch30: 0.003172844162453776
train_mses at epoch30: [0.39127359 0.03657152 0.08778976 0.1303573  0.08876363 0.05051877
 0.10543402 0.12503549 0.06622672 0.0291826 ]
train_maes at epoch30: [0.46430199 0.14235237 0.19972961 0.21839078 0.1986925  0.17353692
 0.18436767 0.20768501 0.17740612 0.11993092]
test_loss at epoch30: 0.015468452935633452
test_mses at epoch30: [1.39296982 0.2551715  0.85617923 0.6169007  0.39617744 0.22946248
 0.35323168 0.56197104 0.35476734 0.02212757]
test_maes at epoch30: [0.82309366 0.21409982 0.41181253 0.36351327 0.48368442 0.35258868
 0.31102915 0.53622274 0.27239317 0.07304327]

train_loss at epoch31: 0.003205246818454369
train_mses at epoch31: [0.49121012 0.05702272 0.13566296 0.16888355 0.06497171 0.03107815
 0.08392079 0.07416519 0.0931563  0.02249687]
train_maes at epoch31: [0.55014417 0.1493409  0.20316136 0.20225346 0.1510981  0.14453929
 0.18657636 0.17511908 0.19102764 0.10516184]
test_loss at epoch31: 0.011768489590157633
test_mses at epoch31: [1.07836691 0.18198655 0.59364843 0.24717273 0.30439647 0.20472294
 0.36680802 0.55300994 0.29095997 0.02319976]
test_maes at epoch31: [0.80959879 0.22963685 0.35509194 0.2534158  0.41321191 0.34709967
 0.33475756 0.58449084 0.25873648 0.08702357]

train_loss at epoch32: 0.0028069819445195403
train_mses at epoch32: [0.57557134 0.0535034  0.11903292 0.11272003 0.05949253 0.05689066
 0.06673215 0.08760211 0.0483602  0.01991664]
train_maes at epoch32: [0.56797894 0.16626551 0.22818127 0.23226476 0.17635259 0.17287363
 0.20170642 0.21669195 0.15824452 0.10949542]
test_loss at epoch32: 0.013433065427386242
test_mses at epoch32: [1.050798   0.21653977 0.78781837 0.40645128 0.38515192 0.19299733
 0.35451567 0.55579226 0.26117746 0.02246163]
test_maes at epoch32: [0.73768479 0.22253181 0.38722625 0.30428122 0.50380017 0.31371384
 0.31636503 0.57334496 0.24724569 0.08013696]

train_loss at epoch33: 0.0024405216555232587
train_mses at epoch33: [0.38139755 0.04243212 0.10514438 0.07940491 0.0756719  0.05830862
 0.04117249 0.07512539 0.04777672 0.02182998]
train_maes at epoch33: [0.44481324 0.140136   0.19807082 0.17995465 0.18291325 0.16877701
 0.13795867 0.17653549 0.15388293 0.10685749]
test_loss at epoch33: 0.013450881871192352
test_mses at epoch33: [1.11678521 0.18383456 0.76450939 0.48437096 0.3350832  0.25833897
 0.32622802 0.5275108  0.28493465 0.02257002]
test_maes at epoch33: [0.7460607  0.21427387 0.38078611 0.36052188 0.4498288  0.37073608
 0.32909688 0.56101234 0.25270504 0.08343802]

train_loss at epoch34: 0.0022907406901535783
train_mses at epoch34: [0.26261915 0.06468887 0.07129601 0.046532   0.05980834 0.06194029
 0.07721803 0.0748548  0.04449349 0.01908596]
train_maes at epoch34: [0.41295262 0.15502868 0.1825254  0.15342749 0.18496232 0.17721886
 0.17922751 0.1837926  0.15305872 0.10561612]
test_loss at epoch34: 0.013953282619300096
test_mses at epoch34: [1.26963716 0.19052603 0.83938248 0.49268712 0.29400431 0.28552613
 0.36406472 0.49105038 0.3078293  0.02241399]
test_maes at epoch34: [0.78534118 0.20583754 0.4026635  0.33258442 0.38403962 0.38991191
 0.31448534 0.50328376 0.2501936  0.08020485]

train_loss at epoch35: 0.002291802316904068
train_mses at epoch35: [0.46474909 0.04011225 0.12864825 0.06693767 0.04010824 0.06907653
 0.06313448 0.04432874 0.0298533  0.02022073]
train_maes at epoch35: [0.46118524 0.14297638 0.20947683 0.15342371 0.14217679 0.18170749
 0.16532526 0.14375494 0.13150726 0.10083128]
test_loss at epoch35: 0.013393057105333908
test_mses at epoch35: [1.09553522 0.21544344 0.80542397 0.41848409 0.32065752 0.23321092
 0.37137845 0.50543176 0.27408775 0.02217173]
test_maes at epoch35: [0.74905805 0.21295952 0.3962487  0.29105121 0.43303097 0.33892208
 0.30968607 0.53053234 0.24627702 0.07876441]

train_loss at epoch36: 0.0018826474071196888
train_mses at epoch36: [0.24524451 0.03374429 0.0432008  0.09143497 0.0333451  0.05109515
 0.0580351  0.06266983 0.03710747 0.01916003]
train_maes at epoch36: [0.37328508 0.13119876 0.15630124 0.20476015 0.1368534  0.15693442
 0.15062303 0.16342183 0.15253938 0.10128649]
test_loss at epoch36: 0.01323266748500907
test_mses at epoch36: [1.00156108 0.2035223  0.66471614 0.41014548 0.48973619 0.24451834
 0.31323292 0.54479907 0.25039626 0.0224354 ]
test_maes at epoch36: [0.75967776 0.23829417 0.35465725 0.31952079 0.60101757 0.36183074
 0.30976521 0.58985346 0.23650521 0.08338122]

train_loss at epoch37: 0.002130546485600264
train_mses at epoch37: [0.27172907 0.03789392 0.07105414 0.07766624 0.0739892  0.03894953
 0.0672806  0.06194879 0.03134136 0.01665286]
train_maes at epoch37: [0.4121744  0.13563781 0.14600668 0.17782496 0.18375465 0.14872992
 0.16175734 0.15043371 0.12560198 0.1009749 ]
test_loss at epoch37: 0.013269488740226498
test_mses at epoch37: [0.98778363 0.21267639 0.68291036 0.47595909 0.39794758 0.2199593
 0.34607085 0.50582888 0.27898417 0.02170165]
test_maes at epoch37: [0.75033537 0.22091183 0.34565518 0.34482581 0.520935   0.3378794
 0.30244212 0.49191977 0.24146623 0.074685  ]

train_loss at epoch38: 0.002025659236571063
train_mses at epoch38: [0.2809948  0.05669788 0.08919587 0.07554859 0.03547031 0.02904561
 0.04179681 0.05938597 0.05237195 0.02275222]
train_maes at epoch38: [0.39537539 0.1430466  0.18337991 0.16417867 0.134738   0.12528711
 0.12023095 0.15651453 0.14646701 0.09674703]
test_loss at epoch38: 0.012516966980436573
test_mses at epoch38: [0.98352555 0.22029361 0.67107861 0.3785153  0.30214351 0.24303563
 0.31987367 0.54046224 0.26299648 0.02326515]
test_maes at epoch38: [0.74770438 0.21726183 0.34752733 0.28631908 0.41652119 0.36313129
 0.29941806 0.55570871 0.23529069 0.08766939]

train_loss at epoch39: 0.0019465813656216083
train_mses at epoch39: [0.24230681 0.04659759 0.06800184 0.05484497 0.03777314 0.03380839
 0.05946027 0.06961889 0.05977457 0.01369272]
train_maes at epoch39: [0.39292203 0.15796887 0.18680782 0.15873365 0.13287386 0.13920755
 0.16795091 0.19084754 0.15691867 0.09435702]
test_loss at epoch39: 0.014588962754477625
test_mses at epoch39: [1.30230013 0.28240727 0.86149686 0.56382518 0.30376201 0.22682735
 0.38200089 0.55161646 0.24009212 0.02389006]
test_maes at epoch39: [0.81237716 0.21650091 0.40998584 0.33029508 0.37814902 0.34541128
 0.3002755  0.53638984 0.23391823 0.08627427]

train_loss at epoch40: 0.0025168500597710195
train_mses at epoch40: [0.41959539 0.03580828 0.09606781 0.15041902 0.04628641 0.03173605
 0.06490215 0.05659738 0.06565701 0.01502699]
train_maes at epoch40: [0.44828032 0.13085568 0.19529692 0.19918415 0.13212887 0.13076766
 0.14688445 0.14482818 0.16078372 0.07866874]
test_loss at epoch40: 0.01444788973616517
test_mses at epoch40: [1.22337412 0.26807694 0.78942532 0.61190286 0.30106829 0.23361868
 0.38628843 0.5114217  0.28309774 0.0234582 ]
test_maes at epoch40: [0.79053716 0.218975   0.37781203 0.34921251 0.38062369 0.35197038
 0.29711255 0.50217127 0.23508813 0.08560697]

train_loss at epoch41: 0.0019767645584500356
train_mses at epoch41: [0.30982593 0.0572076  0.07803606 0.08565904 0.06180865 0.04671209
 0.02914151 0.02978366 0.03253873 0.01802639]
train_maes at epoch41: [0.42809804 0.15551919 0.17165585 0.16488606 0.15740344 0.1584273
 0.1154706  0.12628627 0.13477579 0.09369224]
test_loss at epoch41: 0.012408672791460285
test_mses at epoch41: [0.9939382  0.20724658 0.64333578 0.47419248 0.34029305 0.21741046
 0.31043024 0.46433789 0.25852678 0.02518881]
test_maes at epoch41: [0.74527188 0.23598623 0.34152792 0.31809425 0.4717154  0.34192489
 0.29863739 0.50751142 0.24572637 0.09754278]

train_loss at epoch42: 0.0015948482019745786
train_mses at epoch42: [0.27001722 0.04561748 0.05427147 0.04045054 0.04415768 0.03356546
 0.02842971 0.04810079 0.03846915 0.01616445]
train_maes at epoch42: [0.40223314 0.1543704  0.15280967 0.13826996 0.15103101 0.13579074
 0.11296184 0.15348862 0.13945822 0.09368717]
test_loss at epoch42: 0.012079594737809637
test_mses at epoch42: [0.97814391 0.23031755 0.73167479 0.36353783 0.3083023  0.20231131
 0.30887594 0.44556276 0.24849695 0.02365672]
test_maes at epoch42: [0.73991776 0.21609545 0.38580116 0.27219164 0.4445281  0.32410291
 0.31023475 0.48673936 0.23130467 0.09011845]

train_loss at epoch43: 0.00165180048054975
train_mses at epoch43: [0.24044128 0.03426396 0.07524327 0.04571705 0.02092139 0.03744774
 0.0326332  0.07005206 0.03462744 0.01436021]
train_maes at epoch43: [0.35012648 0.12040546 0.16735987 0.13858589 0.11417525 0.12976934
 0.11783765 0.17000869 0.14140665 0.08469394]
test_loss at epoch43: 0.012401964029540186
test_mses at epoch43: [1.01806067 0.25783677 0.8008228  0.32827794 0.26083382 0.21746077
 0.32840788 0.4529564  0.26961827 0.02152788]
test_maes at epoch43: [0.7455014  0.22346654 0.39825791 0.26080628 0.38617488 0.33397047
 0.31089543 0.49807711 0.23723973 0.08036437]

train_loss at epoch44: 0.0017002547240775564
train_mses at epoch44: [0.19894    0.05247048 0.04822431 0.07441756 0.03461581 0.04889167
 0.04032917 0.03271769 0.04299382 0.0129837 ]
train_maes at epoch44: [0.3300646  0.15379971 0.14769969 0.17706876 0.12708652 0.14223467
 0.13005694 0.13103147 0.14082376 0.08872062]
test_loss at epoch44: 0.012129543268162271
test_mses at epoch44: [1.11849078 0.20432378 0.71391127 0.33025336 0.25553098 0.28805617
 0.34446631 0.42550647 0.27627706 0.02054617]
test_maes at epoch44: [0.79144831 0.22010434 0.35862733 0.26840811 0.38154186 0.39224495
 0.31066373 0.47443677 0.24042044 0.07683947]

train_loss at epoch45: 0.0018929837109602015
train_mses at epoch45: [0.33729599 0.04152497 0.10061821 0.04526501 0.03281807 0.04837486
 0.05248463 0.04374529 0.03529398 0.02089962]
train_maes at epoch45: [0.42729644 0.14329355 0.20259796 0.14908285 0.13255758 0.15195029
 0.15218478 0.13989674 0.14317949 0.10787411]
test_loss at epoch45: 0.014008528188518856
test_mses at epoch45: [1.18069591 0.18052892 0.84105693 0.60009556 0.28712259 0.21151491
 0.41441036 0.47358524 0.27211268 0.02005137]
test_maes at epoch45: [0.78488652 0.21459134 0.36582258 0.38013158 0.40212198 0.31906849
 0.31199218 0.40537849 0.23746863 0.06299984]

train_loss at epoch46: 0.0022063969593981037
train_mses at epoch46: [0.27199447 0.05210575 0.08734731 0.09530466 0.05048892 0.05229631
 0.03146    0.09376855 0.02903094 0.01199698]
train_maes at epoch46: [0.3752789  0.14097972 0.17563087 0.17303211 0.11826123 0.14266219
 0.1083765  0.1786544  0.12049373 0.08106398]
test_loss at epoch46: 0.011960398405790329
test_mses at epoch46: [1.10802743 0.1759903  0.685016   0.4122183  0.30753542 0.2014029
 0.34427815 0.42697713 0.239475   0.02166591]
test_maes at epoch46: [0.78465651 0.23370038 0.34467064 0.32949133 0.4424843  0.31222474
 0.31512162 0.45475913 0.22613282 0.07932694]

train_loss at epoch47: 0.001721764428784018
train_mses at epoch47: [0.29187474 0.04177106 0.05695746 0.04816621 0.04275345 0.04127637
 0.03287444 0.05139499 0.04978889 0.0218917 ]
train_maes at epoch47: [0.38600999 0.14237699 0.14800437 0.14307815 0.14226995 0.1384205
 0.12440266 0.1463751  0.16307355 0.11197456]
test_loss at epoch47: 0.012553017910407938
test_mses at epoch47: [1.0641602  0.19385902 0.72950164 0.37440931 0.32398664 0.22707328
 0.36334639 0.47679138 0.25260723 0.02329129]
test_maes at epoch47: [0.75857645 0.21125765 0.35778851 0.29071601 0.44919223 0.33878953
 0.31785477 0.52835138 0.22742924 0.08831356]

train_loss at epoch48: 0.0015270322075356607
train_mses at epoch48: [0.29620468 0.02857157 0.05421443 0.04101612 0.0460715  0.05628338
 0.03473491 0.02752935 0.02998949 0.01567415]
train_maes at epoch48: [0.41104624 0.12274025 0.15528938 0.13645602 0.14216584 0.16545148
 0.1211031  0.11637488 0.11475498 0.0918461 ]
test_loss at epoch48: 0.013891100235607313
test_mses at epoch48: [1.16502708 0.22619472 0.81942076 0.40159126 0.33337194 0.24790564
 0.45016503 0.48655806 0.28551588 0.02247768]
test_maes at epoch48: [0.77981936 0.20613279 0.38688354 0.27966445 0.43981718 0.36047501
 0.32895258 0.52272944 0.24166859 0.0816367 ]

train_loss at epoch49: 0.0015538147772135942
train_mses at epoch49: [0.30073461 0.04077467 0.02964565 0.0587846  0.05047151 0.04185682
 0.03750213 0.03411699 0.03374308 0.01236736]
train_maes at epoch49: [0.38677967 0.12382893 0.12050688 0.148186   0.1372427  0.15076698
 0.13275935 0.12418638 0.13480063 0.08641865]
test_loss at epoch49: 0.012906298689220263
test_mses at epoch49: [1.0575366  0.19765012 0.68839148 0.31777091 0.37485791 0.28793372
 0.39550937 0.52694468 0.243638   0.02153798]
test_maes at epoch49: [0.78274074 0.2139667  0.35310888 0.2636788  0.5041586  0.40210633
 0.33007004 0.59666446 0.23210499 0.08501928]

